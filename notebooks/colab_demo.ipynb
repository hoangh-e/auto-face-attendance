{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéØ AI Attendance System - Enhanced Colab Demo\n",
        "\n",
        "**SCRFD Detection + ArcFace Recognition + SQLite Database**\n",
        "\n",
        "Pipeline V1: `hoangh-e/auto-face-attendance/tree/pipeline-v1.0/`\n",
        "\n",
        "---\n",
        "\n",
        "## üåü Features Overview\n",
        "\n",
        "‚úÖ **Quick Setup**: One-cell installation with progress tracking  \n",
        "‚úÖ **Employee Management**: Batch registration with ZIP file upload  \n",
        "‚úÖ **Advanced Video Processing**: Multi-format support with real-time metrics  \n",
        "‚úÖ **Performance Dashboard**: Live charts and comprehensive analytics  \n",
        "‚úÖ **Export Functionality**: JSON, CSV, Excel with attendance reports  \n",
        "\n",
        "**üöÄ Ready for immediate testing and evaluation!**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Cell 1: ‚ö° Quick Setup & Installation\n",
        "\n",
        "**Automated installation with GPU detection and environment optimization**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI Attendance System - Enhanced Colab Demo\n",
        "# Quick Setup & Installation\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "from pathlib import Path\n",
        "import platform\n",
        "\n",
        "print(\"üéØ AI ATTENDANCE SYSTEM - ENHANCED COLAB DEMO\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Environment Detection\n",
        "print(\"üîç ENVIRONMENT DETECTION:\")\n",
        "print(f\"‚îú‚îÄ Platform: {platform.system()} {platform.release()}\")\n",
        "print(f\"‚îú‚îÄ Python: {sys.version.split()[0]}\")\n",
        "print(f\"‚îú‚îÄ Working Directory: {os.getcwd()}\")\n",
        "\n",
        "# GPU Detection\n",
        "try:\n",
        "    import torch\n",
        "    gpu_available = torch.cuda.is_available()\n",
        "    if gpu_available:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"üöÄ GPU: ‚úÖ {gpu_name} ({gpu_memory:.1f}GB)\")\n",
        "    else:\n",
        "        print(f\"üíª GPU: ‚ùå CPU mode (slower but functional)\")\n",
        "except ImportError:\n",
        "    print(f\"üì¶ PyTorch: Installing...\")\n",
        "    gpu_available = False\n",
        "\n",
        "# Install packages with progress tracking\n",
        "print(f\"\\nüì¶ INSTALLING PACKAGES:\")\n",
        "\n",
        "packages = [\n",
        "    'torch>=1.9.0',\n",
        "    'torchvision>=0.10.0', \n",
        "    'insightface>=0.7.3',\n",
        "    'opencv-python>=4.5.0',\n",
        "    'scikit-learn>=1.0.0',\n",
        "    'matplotlib>=3.5.0',\n",
        "    'pandas>=1.3.0',\n",
        "    'tqdm>=4.62.0',\n",
        "    'pillow>=8.3.0',\n",
        "    'numpy>=1.21.0'\n",
        "]\n",
        "\n",
        "for i, package in enumerate(packages, 1):\n",
        "    try:\n",
        "        print(f\"‚îú‚îÄ [{i}/{len(packages)}] Installing {package.split('>=')[0]}...\")\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, '-m', 'pip', 'install', package, '--quiet'],\n",
        "            capture_output=True, text=True, timeout=120\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(f\"    ‚úÖ Installed successfully\")\n",
        "        else:\n",
        "            print(f\"    ‚ö†Ô∏è Already installed or skipped\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(f\"    ‚ö†Ô∏è Installation timeout, continuing...\")\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ùå Error: {e}\")\n",
        "\n",
        "# Create directories\n",
        "work_dir = Path('/content') if 'google.colab' in sys.modules else Path.cwd()\n",
        "data_dir = work_dir / 'attendance_data'\n",
        "employees_dir = data_dir / 'employees'\n",
        "videos_dir = data_dir / 'videos'\n",
        "exports_dir = data_dir / 'exports'\n",
        "\n",
        "for directory in [data_dir, employees_dir, videos_dir, exports_dir]:\n",
        "    directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\nüìÅ DIRECTORY STRUCTURE:\")\n",
        "print(f\"‚îú‚îÄ Data: {data_dir}\")\n",
        "print(f\"‚îú‚îÄ Employees: {employees_dir}\")\n",
        "print(f\"‚îú‚îÄ Videos: {videos_dir}\")\n",
        "print(f\"‚îî‚îÄ Exports: {exports_dir}\")\n",
        "\n",
        "print(f\"\\nüéâ SETUP COMPLETED!\")\n",
        "print(f\"Ready for employee registration and video processing!\")\n",
        "print(\"=\" * 55)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Cell 2: ü§ñ AI Models & Database Initialization\n",
        "\n",
        "**Initialize SCRFD + ArcFace models with SQLite database**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI Models & Database Initialization\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import sqlite3\n",
        "import time\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import base64\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# AI and ML imports\n",
        "import insightface\n",
        "from insightface.app import FaceAnalysis\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"ü§ñ INITIALIZING AI MODELS & DATABASE\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ColabAISystem:\n",
        "    \"\"\"Enhanced AI system for Colab demo\"\"\"\n",
        "    \n",
        "    def __init__(self, model_pack='buffalo_l'):\n",
        "        self.model_pack = model_pack\n",
        "        self.app = None\n",
        "        self.performance_stats = {\n",
        "            'total_inferences': 0,\n",
        "            'avg_latency_ms': 0.0,\n",
        "            'total_time': 0.0,\n",
        "            'gpu_available': False\n",
        "        }\n",
        "        self._init_models()\n",
        "    \n",
        "    def _init_models(self):\n",
        "        \"\"\"Initialize InsightFace models\"\"\"\n",
        "        try:\n",
        "            import torch\n",
        "            if torch.cuda.is_available():\n",
        "                providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "                self.performance_stats['gpu_available'] = True\n",
        "                print(f\"üöÄ GPU Mode: {torch.cuda.get_device_name(0)}\")\n",
        "            else:\n",
        "                providers = ['CPUExecutionProvider']\n",
        "                print(\"üíª CPU Mode: Slower but functional\")\n",
        "            \n",
        "            print(f\"üì¶ Loading {self.model_pack} model pack...\")\n",
        "            self.app = FaceAnalysis(name=self.model_pack, providers=providers)\n",
        "            \n",
        "            # Prepare with optimal settings\n",
        "            ctx_id = 0 if torch.cuda.is_available() else -1\n",
        "            self.app.prepare(ctx_id=ctx_id, det_size=(640, 640), det_thresh=0.5)\n",
        "            \n",
        "            print(f\"‚úÖ {self.model_pack} loaded successfully!\")\n",
        "            \n",
        "            # Performance test\n",
        "            self._test_performance()\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Model loading failed: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def _test_performance(self):\n",
        "        \"\"\"Test model performance\"\"\"\n",
        "        print(\"üß™ Testing model performance...\")\n",
        "        \n",
        "        # Create test image\n",
        "        test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
        "        \n",
        "        # Run multiple inferences\n",
        "        times = []\n",
        "        for _ in range(5):\n",
        "            start = time.time()\n",
        "            self.app.get(test_image)\n",
        "            times.append((time.time() - start) * 1000)\n",
        "        \n",
        "        avg_time = np.mean(times)\n",
        "        print(f\"  ‚îú‚îÄ Average inference time: {avg_time:.1f}ms\")\n",
        "        \n",
        "        if avg_time < 100:\n",
        "            print(f\"  ‚îî‚îÄ Performance: üåü EXCELLENT (real-time capable)\")\n",
        "        elif avg_time < 300:\n",
        "            print(f\"  ‚îî‚îÄ Performance: üëç GOOD\")\n",
        "        else:\n",
        "            print(f\"  ‚îî‚îÄ Performance: ‚ö†Ô∏è SLOW (consider lighter model)\")\n",
        "    \n",
        "    def detect_and_recognize(self, image):\n",
        "        \"\"\"Process image and return face data\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            faces = self.app.get(image)\n",
        "            \n",
        "            results = []\n",
        "            for face in faces:\n",
        "                result = {\n",
        "                    'bbox': face.bbox,\n",
        "                    'det_score': float(face.det_score),\n",
        "                    'landmarks': getattr(face, 'kps', None),\n",
        "                    'embedding': face.embedding,\n",
        "                    'age': getattr(face, 'age', None),\n",
        "                    'gender': getattr(face, 'gender', None)\n",
        "                }\n",
        "                results.append(result)\n",
        "            \n",
        "            # Update performance stats\n",
        "            processing_time = (time.time() - start_time) * 1000\n",
        "            self.performance_stats['total_inferences'] += 1\n",
        "            self.performance_stats['total_time'] += processing_time\n",
        "            self.performance_stats['avg_latency_ms'] = (\n",
        "                self.performance_stats['total_time'] / \n",
        "                self.performance_stats['total_inferences']\n",
        "            )\n",
        "            \n",
        "            return results\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Processing error: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def batch_process(self, images):\n",
        "        \"\"\"Process multiple images efficiently\"\"\"\n",
        "        results = []\n",
        "        for img in tqdm(images, desc=\"Processing images\"):\n",
        "            result = self.detect_and_recognize(img)\n",
        "            results.append(result)\n",
        "        return results\n",
        "\n",
        "class ColabDatabase:\n",
        "    \"\"\"Enhanced SQLite database for Colab demo\"\"\"\n",
        "    \n",
        "    def __init__(self, db_path=\"colab_attendance.db\"):\n",
        "        self.db_path = data_dir / db_path\n",
        "        self.conn = sqlite3.connect(str(self.db_path), check_same_thread=False)\n",
        "        self.conn.row_factory = sqlite3.Row\n",
        "        self._create_tables()\n",
        "        logger.info(f\"Database initialized: {self.db_path}\")\n",
        "    \n",
        "    def _create_tables(self):\n",
        "        \"\"\"Create database tables\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        \n",
        "        # Employees table\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS employees (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            employee_code TEXT UNIQUE NOT NULL,\n",
        "            name TEXT NOT NULL,\n",
        "            email TEXT UNIQUE NOT NULL,\n",
        "            department TEXT,\n",
        "            position TEXT,\n",
        "            face_embeddings TEXT,\n",
        "            face_count INTEGER DEFAULT 0,\n",
        "            avg_quality REAL DEFAULT 0.0,\n",
        "            is_active BOOLEAN DEFAULT 1,\n",
        "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "        )\n",
        "        \"\"\")\n",
        "        \n",
        "        # Attendance logs\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS attendance_logs (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            employee_id INTEGER,\n",
        "            event_type TEXT NOT NULL,\n",
        "            timestamp TIMESTAMP NOT NULL,\n",
        "            confidence REAL NOT NULL,\n",
        "            image_data TEXT,\n",
        "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "            FOREIGN KEY (employee_id) REFERENCES employees (id)\n",
        "        )\n",
        "        \"\"\")\n",
        "        \n",
        "        # Processing stats\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS processing_stats (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "            total_frames INTEGER,\n",
        "            faces_detected INTEGER,\n",
        "            employees_recognized INTEGER,\n",
        "            avg_processing_time REAL,\n",
        "            session_id TEXT\n",
        "        )\n",
        "        \"\"\")\n",
        "        \n",
        "        self.conn.commit()\n",
        "    \n",
        "    def register_employee(self, employee_data, face_embeddings):\n",
        "        \"\"\"Register employee with face embeddings\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        \n",
        "        try:\n",
        "            # Calculate average embedding\n",
        "            if face_embeddings:\n",
        "                avg_embedding = np.mean(face_embeddings, axis=0)\n",
        "                avg_quality = np.mean([np.linalg.norm(emb) for emb in face_embeddings])\n",
        "            else:\n",
        "                return None\n",
        "            \n",
        "            cursor.execute(\"\"\"\n",
        "            INSERT INTO employees \n",
        "            (employee_code, name, email, department, position, face_embeddings, \n",
        "             face_count, avg_quality)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            \"\"\", (\n",
        "                employee_data['employee_code'],\n",
        "                employee_data['name'], \n",
        "                employee_data['email'],\n",
        "                employee_data.get('department', ''),\n",
        "                employee_data.get('position', ''),\n",
        "                json.dumps(avg_embedding.tolist()),\n",
        "                len(face_embeddings),\n",
        "                avg_quality\n",
        "            ))\n",
        "            \n",
        "            employee_id = cursor.lastrowid\n",
        "            self.conn.commit()\n",
        "            \n",
        "            logger.info(f\"Employee registered: {employee_data['name']} (ID: {employee_id})\")\n",
        "            return employee_id\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.conn.rollback()\n",
        "            logger.error(f\"Registration error: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def find_employee_by_embedding(self, query_embedding, threshold=0.65):\n",
        "        \"\"\"Find employee by face embedding\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        \n",
        "        cursor.execute(\"\"\"\n",
        "        SELECT id, employee_code, name, email, face_embeddings\n",
        "        FROM employees WHERE is_active = 1 AND face_embeddings IS NOT NULL\n",
        "        \"\"\")\n",
        "        \n",
        "        best_match = None\n",
        "        best_similarity = 0.0\n",
        "        \n",
        "        for row in cursor.fetchall():\n",
        "            try:\n",
        "                stored_embedding = np.array(json.loads(row['face_embeddings']))\n",
        "                similarity = cosine_similarity(\n",
        "                    query_embedding.reshape(1, -1),\n",
        "                    stored_embedding.reshape(1, -1)\n",
        "                )[0][0]\n",
        "                \n",
        "                if similarity > best_similarity and similarity > threshold:\n",
        "                    best_similarity = similarity\n",
        "                    best_match = {\n",
        "                        'id': row['id'],\n",
        "                        'employee_code': row['employee_code'],\n",
        "                        'name': row['name'],\n",
        "                        'email': row['email'],\n",
        "                        'similarity': similarity\n",
        "                    }\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        return best_match\n",
        "    \n",
        "    def record_attendance(self, employee_id, event_type, confidence, image_data=None):\n",
        "        \"\"\"Record attendance event\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        \n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute(\"\"\"\n",
        "        INSERT INTO attendance_logs (employee_id, event_type, timestamp, confidence, image_data)\n",
        "        VALUES (?, ?, ?, ?, ?)\n",
        "        \"\"\", (employee_id, event_type, timestamp, confidence, image_data))\n",
        "        \n",
        "        self.conn.commit()\n",
        "        return cursor.lastrowid\n",
        "    \n",
        "    def get_statistics(self):\n",
        "        \"\"\"Get database statistics\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        \n",
        "        stats = {}\n",
        "        \n",
        "        cursor.execute(\"SELECT COUNT(*) FROM employees WHERE is_active = 1\")\n",
        "        stats['total_employees'] = cursor.fetchone()[0]\n",
        "        \n",
        "        cursor.execute(\"SELECT COUNT(*) FROM attendance_logs\")\n",
        "        stats['total_logs'] = cursor.fetchone()[0]\n",
        "        \n",
        "        cursor.execute(\"\"\"\n",
        "        SELECT COUNT(*) FROM attendance_logs \n",
        "        WHERE DATE(timestamp) = DATE('now')\n",
        "        \"\"\")\n",
        "        stats['today_logs'] = cursor.fetchone()[0]\n",
        "        \n",
        "        return stats\n",
        "\n",
        "# Initialize systems\n",
        "print(\"üîÑ Initializing AI system...\")\n",
        "ai_system = ColabAISystem()\n",
        "\n",
        "print(\"\\nüîÑ Initializing database...\")\n",
        "db = ColabDatabase()\n",
        "\n",
        "print(f\"\\nüìä INITIALIZATION COMPLETED!\")\n",
        "print(f\"‚îú‚îÄ Model: {ai_system.model_pack}\")\n",
        "print(f\"‚îú‚îÄ GPU Available: {ai_system.performance_stats['gpu_available']}\")\n",
        "print(f\"‚îî‚îÄ Database: {db.db_path}\")\n",
        "\n",
        "# Show current statistics\n",
        "stats = db.get_statistics()\n",
        "print(f\"\\nüìà Database Statistics:\")\n",
        "print(f\"‚îú‚îÄ Employees: {stats['total_employees']}\")\n",
        "print(f\"‚îú‚îÄ Total Logs: {stats['total_logs']}\")\n",
        "print(f\"‚îî‚îÄ Today's Logs: {stats['today_logs']}\")\n",
        "\n",
        "print(\"=\" * 45)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Cell 3: üë• Employee Management System\n",
        "\n",
        "**Batch employee registration with photo upload and quality validation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Employee Management System\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "\n",
        "print(\"üë• EMPLOYEE MANAGEMENT SYSTEM\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "class EmployeeManager:\n",
        "    \"\"\"Enhanced employee management for Colab\"\"\"\n",
        "    \n",
        "    def __init__(self, ai_system, database):\n",
        "        self.ai_system = ai_system\n",
        "        self.db = database\n",
        "        self.registered_employees = []\n",
        "    \n",
        "    def upload_employee_photos(self):\n",
        "        \"\"\"Upload employee photos via file picker\"\"\"\n",
        "        print(\"üì§ Employee Photo Upload\")\n",
        "        print(\"Supported formats: ZIP archives, individual images\")\n",
        "        print(\"Folder structure: employee_name/photo1.jpg, photo2.jpg, ...\")\n",
        "        \n",
        "        uploaded = files.upload()\n",
        "        \n",
        "        for filename, content in uploaded.items():\n",
        "            print(f\"\\nüìÅ Processing: {filename}\")\n",
        "            \n",
        "            if filename.endswith('.zip'):\n",
        "                self._process_zip_file(filename, content)\n",
        "            elif filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                self._process_single_image(filename, content)\n",
        "            else:\n",
        "                print(f\"  ‚ö†Ô∏è Unsupported format: {filename}\")\n",
        "    \n",
        "    def _process_zip_file(self, filename, content):\n",
        "        \"\"\"Process ZIP file with employee folders\"\"\"\n",
        "        zip_path = employees_dir / filename\n",
        "        \n",
        "        # Save ZIP file\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            f.write(content)\n",
        "        \n",
        "        # Extract ZIP\n",
        "        extract_dir = employees_dir / 'extracted'\n",
        "        extract_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_dir)\n",
        "            \n",
        "            print(f\"  ‚úÖ Extracted to: {extract_dir}\")\n",
        "            \n",
        "            # Process extracted folders\n",
        "            self._scan_employee_folders(extract_dir)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå ZIP extraction error: {e}\")\n",
        "        finally:\n",
        "            # Cleanup\n",
        "            if zip_path.exists():\n",
        "                zip_path.unlink()\n",
        "    \n",
        "    def _process_single_image(self, filename, content):\n",
        "        \"\"\"Process single image file\"\"\"\n",
        "        # Extract employee name from filename\n",
        "        employee_name = filename.split('.')[0].replace('_', ' ').title()\n",
        "        \n",
        "        # Save image\n",
        "        img_path = employees_dir / filename\n",
        "        with open(img_path, 'wb') as f:\n",
        "            f.write(content)\n",
        "        \n",
        "        # Process image\n",
        "        try:\n",
        "            image = cv2.imread(str(img_path))\n",
        "            if image is not None:\n",
        "                self._register_single_employee(employee_name, [image])\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error processing {filename}: {e}\")\n",
        "    \n",
        "    def _scan_employee_folders(self, base_dir):\n",
        "        \"\"\"Scan and process employee folders\"\"\"\n",
        "        employee_folders = [f for f in base_dir.iterdir() \n",
        "                           if f.is_dir() and not f.name.startswith('.')]\n",
        "        \n",
        "        if not employee_folders:\n",
        "            print(f\"  ‚ö†Ô∏è No employee folders found\")\n",
        "            return\n",
        "        \n",
        "        print(f\"  üìÅ Found {len(employee_folders)} employee folders\")\n",
        "        \n",
        "        for folder in tqdm(employee_folders, desc=\"Processing employees\"):\n",
        "            employee_name = folder.name.replace('_', ' ').title()\n",
        "            \n",
        "            # Find image files\n",
        "            image_files = []\n",
        "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
        "                image_files.extend(folder.glob(ext))\n",
        "            \n",
        "            if not image_files:\n",
        "                print(f\"  ‚ö†Ô∏è No images in {folder.name}\")\n",
        "                continue\n",
        "            \n",
        "            # Load images\n",
        "            images = []\n",
        "            for img_file in image_files:\n",
        "                try:\n",
        "                    img = cv2.imread(str(img_file))\n",
        "                    if img is not None:\n",
        "                        images.append(img)\n",
        "                except:\n",
        "                    continue\n",
        "            \n",
        "            if images:\n",
        "                self._register_single_employee(employee_name, images)\n",
        "    \n",
        "    def _register_single_employee(self, employee_name, images):\n",
        "        \"\"\"Register single employee with images\"\"\"\n",
        "        print(f\"\\\\nüë§ Processing: {employee_name}\")\n",
        "        \n",
        "        # Extract face embeddings\n",
        "        face_embeddings = []\n",
        "        quality_scores = []\n",
        "        processed_count = 0\n",
        "        \n",
        "        for i, image in enumerate(images):\n",
        "            try:\n",
        "                faces = self.ai_system.detect_and_recognize(image)\n",
        "                \n",
        "                if len(faces) == 1:  # Exactly one face\n",
        "                    face_data = faces[0]\n",
        "                    \n",
        "                    if face_data['det_score'] > 0.7:  # Good quality\n",
        "                        face_embeddings.append(face_data['embedding'])\n",
        "                        quality_scores.append(face_data['det_score'])\n",
        "                        processed_count += 1\n",
        "                        print(f\"  ‚úÖ Image {i+1}: confidence {face_data['det_score']:.3f}\")\n",
        "                    else:\n",
        "                        print(f\"  ‚ö†Ô∏è Image {i+1}: low quality ({face_data['det_score']:.3f})\")\n",
        "                elif len(faces) == 0:\n",
        "                    print(f\"  ‚ùå Image {i+1}: no face detected\")\n",
        "                else:\n",
        "                    print(f\"  ‚ö†Ô∏è Image {i+1}: multiple faces ({len(faces)})\") \n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Image {i+1}: error - {e}\")\n",
        "        \n",
        "        # Register employee if sufficient faces\n",
        "        if len(face_embeddings) >= 1:\n",
        "            # Create employee data\n",
        "            employee_code = employee_name.upper().replace(' ', '_')\n",
        "            employee_data = {\n",
        "                'employee_code': employee_code,\n",
        "                'name': employee_name,\n",
        "                'email': f\"{employee_code.lower()}@company.com\",\n",
        "                'department': 'Demo Department',\n",
        "                'position': 'Employee'\n",
        "            }\n",
        "            \n",
        "            # Register in database\n",
        "            employee_id = self.db.register_employee(employee_data, face_embeddings)\n",
        "            \n",
        "            if employee_id:\n",
        "                avg_quality = np.mean(quality_scores)\n",
        "                self.registered_employees.append({\n",
        "                    'id': employee_id,\n",
        "                    'name': employee_name,\n",
        "                    'face_count': len(face_embeddings),\n",
        "                    'avg_quality': avg_quality\n",
        "                })\n",
        "                \n",
        "                print(f\"  üéâ Registered: {len(face_embeddings)} faces, avg quality: {avg_quality:.3f}\")\n",
        "            else:\n",
        "                print(f\"  ‚ùå Database registration failed\")\n",
        "        else:\n",
        "            print(f\"  ‚ùå Insufficient quality faces (need at least 1)\")\n",
        "    \n",
        "    def manual_employee_entry(self):\n",
        "        \"\"\"Manual employee entry with form\"\"\"\n",
        "        print(\"‚úèÔ∏è Manual Employee Entry\")\n",
        "        \n",
        "        # Create form widgets\n",
        "        name_widget = widgets.Text(description='Name:', placeholder='John Doe')\n",
        "        email_widget = widgets.Text(description='Email:', placeholder='john.doe@company.com')\n",
        "        dept_widget = widgets.Text(description='Department:', placeholder='Engineering')\n",
        "        position_widget = widgets.Text(description='Position:', placeholder='Developer')\n",
        "        \n",
        "        submit_button = widgets.Button(description='Register Employee')\n",
        "        output = widgets.Output()\n",
        "        \n",
        "        def on_submit(b):\n",
        "            with output:\n",
        "                clear_output()\n",
        "                \n",
        "                if not name_widget.value:\n",
        "                    print(\"‚ùå Name is required\")\n",
        "                    return\n",
        "                \n",
        "                employee_data = {\n",
        "                    'employee_code': name_widget.value.upper().replace(' ', '_'),\n",
        "                    'name': name_widget.value,\n",
        "                    'email': email_widget.value or f\"{name_widget.value.lower().replace(' ', '.')}@company.com\",\n",
        "                    'department': dept_widget.value or 'General',\n",
        "                    'position': position_widget.value or 'Employee'\n",
        "                }\n",
        "                \n",
        "                # Register without face embeddings (manual entry)\n",
        "                employee_id = self.db.register_employee(employee_data, [])\n",
        "                \n",
        "                if employee_id:\n",
        "                    print(f\"‚úÖ Employee registered: {employee_data['name']} (ID: {employee_id})\")\n",
        "                    print(\"‚ö†Ô∏è Note: No face embeddings - photos needed for recognition\")\n",
        "                else:\n",
        "                    print(\"‚ùå Registration failed\")\n",
        "        \n",
        "        submit_button.on_click(on_submit)\n",
        "        \n",
        "        # Display form\n",
        "        display(widgets.VBox([\n",
        "            name_widget, email_widget, dept_widget, position_widget,\n",
        "            submit_button, output\n",
        "        ]))\n",
        "    \n",
        "    def show_registered_employees(self):\n",
        "        \"\"\"Display registered employees table\"\"\"\n",
        "        if not self.registered_employees:\n",
        "            print(\"üìù No employees registered yet\")\n",
        "            return\n",
        "        \n",
        "        print(f\"üìã REGISTERED EMPLOYEES ({len(self.registered_employees)})\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame(self.registered_employees)\n",
        "        df.index = df.index + 1  # Start from 1\n",
        "        \n",
        "        # Display table\n",
        "        display(HTML(df.to_html()))\n",
        "        \n",
        "        # Show summary statistics\n",
        "        total_faces = df['face_count'].sum()\n",
        "        avg_quality = df['avg_quality'].mean()\n",
        "        \n",
        "        print(f\"\\\\nüìä Summary:\")\n",
        "        print(f\"‚îú‚îÄ Total Employees: {len(self.registered_employees)}\")\n",
        "        print(f\"‚îú‚îÄ Total Face Images: {total_faces}\")\n",
        "        print(f\"‚îî‚îÄ Average Quality: {avg_quality:.3f}\")\n",
        "    \n",
        "    def export_employee_data(self):\n",
        "        \"\"\"Export employee data to various formats\"\"\"\n",
        "        if not self.registered_employees:\n",
        "            print(\"‚ùå No data to export\")\n",
        "            return\n",
        "        \n",
        "        print(\"üì§ Exporting employee data...\")\n",
        "        \n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame(self.registered_employees)\n",
        "        \n",
        "        # Export to CSV\n",
        "        csv_path = exports_dir / 'employees.csv'\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        \n",
        "        # Export to Excel\n",
        "        excel_path = exports_dir / 'employees.xlsx'\n",
        "        df.to_excel(excel_path, index=False)\n",
        "        \n",
        "        # Export to JSON\n",
        "        json_path = exports_dir / 'employees.json'\n",
        "        df.to_json(json_path, orient='records', indent=2)\n",
        "        \n",
        "        print(f\"‚úÖ Exported to:\")\n",
        "        print(f\"‚îú‚îÄ CSV: {csv_path}\")\n",
        "        print(f\"‚îú‚îÄ Excel: {excel_path}\")\n",
        "        print(f\"‚îî‚îÄ JSON: {json_path}\")\n",
        "        \n",
        "        # Download files (Colab specific)\n",
        "        try:\n",
        "            files.download(str(csv_path))\n",
        "            files.download(str(excel_path))\n",
        "            files.download(str(json_path))\n",
        "            print(\"üì• Files downloaded to your computer\")\n",
        "        except:\n",
        "            print(\"üí° Files saved to exports/ directory\")\n",
        "\n",
        "# Initialize employee manager\n",
        "employee_manager = EmployeeManager(ai_system, db)\n",
        "\n",
        "print(\"üéØ Employee Management Options:\")\n",
        "print(\"1. Upload photos (ZIP or individual files)\")\n",
        "print(\"2. Manual entry (without photos)\")\n",
        "print(\"3. View registered employees\")\n",
        "print(\"4. Export employee data\")\n",
        "print(\"\\\\nRun the appropriate methods below:\")\n",
        "print(\"=\" * 35)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Cell 4: üé• Advanced Video Processing\n",
        "\n",
        "**Multi-format video processing with real-time analytics and attendance tracking**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Video Processing\n",
        "\n",
        "import cv2\n",
        "import tempfile\n",
        "import threading\n",
        "from collections import defaultdict, deque\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üé• ADVANCED VIDEO PROCESSING\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "class VideoProcessor:\n",
        "    \"\"\"Enhanced video processor for Colab\"\"\"\n",
        "    \n",
        "    def __init__(self, ai_system, database):\n",
        "        self.ai_system = ai_system\n",
        "        self.db = database\n",
        "        self.processing_stats = {\n",
        "            'total_frames': 0,\n",
        "            'faces_detected': 0,\n",
        "            'employees_recognized': 0,\n",
        "            'processing_times': deque(maxlen=100),\n",
        "            'recognition_history': [],\n",
        "            'session_start': time.time()\n",
        "        }\n",
        "        self.attendance_events = []\n",
        "        self.frame_buffer = deque(maxlen=30)  # Keep last 30 frames\n",
        "    \n",
        "    def upload_and_process_video(self):\n",
        "        \"\"\"Upload and process video file\"\"\"\n",
        "        print(\"üì§ Video Upload\")\n",
        "        print(\"Supported formats: MP4, AVI, MOV, MKV\")\n",
        "        print(\"Max file size: 200MB (Colab limitation)\")\n",
        "        \n",
        "        uploaded = files.upload()\n",
        "        \n",
        "        for filename, content in uploaded.items():\n",
        "            print(f\"\\\\nüé¨ Processing: {filename}\")\n",
        "            \n",
        "            # Save uploaded file\n",
        "            temp_path = data_dir / filename\n",
        "            with open(temp_path, 'wb') as f:\n",
        "                f.write(content)\n",
        "            \n",
        "            # Process video\n",
        "            self.process_video_file(temp_path)\n",
        "            \n",
        "            # Cleanup\n",
        "            if temp_path.exists():\n",
        "                temp_path.unlink()\n",
        "    \n",
        "    def process_video_file(self, video_path):\n",
        "        \"\"\"Process video file with real-time analytics\"\"\"\n",
        "        print(f\"üìä Video Analysis: {video_path.name}\")\n",
        "        \n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "        \n",
        "        if not cap.isOpened():\n",
        "            print(\"‚ùå Cannot open video file\")\n",
        "            return\n",
        "        \n",
        "        # Video properties\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        duration = total_frames / fps if fps > 0 else 0\n",
        "        \n",
        "        print(f\"‚îú‚îÄ Total Frames: {total_frames}\")\n",
        "        print(f\"‚îú‚îÄ FPS: {fps:.1f}\")\n",
        "        print(f\"‚îî‚îÄ Duration: {duration:.1f} seconds\")\n",
        "        \n",
        "        # Process frames with progress\n",
        "        frame_count = 0\n",
        "        recognition_results = []\n",
        "        \n",
        "        with tqdm(total=total_frames, desc=\"Processing frames\") as pbar:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                \n",
        "                frame_count += 1\n",
        "                \n",
        "                # Process every nth frame (for performance)\n",
        "                if frame_count % max(1, int(fps // 5)) == 0:  # 5 FPS processing\n",
        "                    results = self._process_single_frame(frame, frame_count)\n",
        "                    recognition_results.extend(results)\n",
        "                \n",
        "                pbar.update(1)\n",
        "        \n",
        "        cap.release()\n",
        "        \n",
        "        # Generate analytics\n",
        "        self._generate_video_analytics(recognition_results, total_frames, duration)\n",
        "    \n",
        "    def _process_single_frame(self, frame, frame_number):\n",
        "        \"\"\"Process single frame and return results\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Detect faces\n",
        "        faces = self.ai_system.detect_and_recognize(frame)\n",
        "        \n",
        "        processing_time = (time.time() - start_time) * 1000\n",
        "        self.processing_stats['processing_times'].append(processing_time)\n",
        "        self.processing_stats['total_frames'] += 1\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        if faces:\n",
        "            self.processing_stats['faces_detected'] += 1\n",
        "            \n",
        "            for face in faces:\n",
        "                # Try to recognize employee\n",
        "                employee = self.db.find_employee_by_embedding(\n",
        "                    face['embedding'], threshold=0.6\n",
        "                )\n",
        "                \n",
        "                if employee:\n",
        "                    self.processing_stats['employees_recognized'] += 1\n",
        "                    \n",
        "                    # Check for duplicate recent recognition (cooldown)\n",
        "                    current_time = time.time()\n",
        "                    recent_recognition = any(\n",
        "                        r['employee_id'] == employee['id'] and \n",
        "                        (current_time - r['timestamp']) < 30  # 30 second cooldown\n",
        "                        for r in self.attendance_events[-10:]  # Check last 10 events\n",
        "                    )\n",
        "                    \n",
        "                    if not recent_recognition:\n",
        "                        # Record attendance\n",
        "                        attendance_id = self.db.record_attendance(\n",
        "                            employee['id'], 'video_checkin', employee['similarity']\n",
        "                        )\n",
        "                        \n",
        "                        event = {\n",
        "                            'attendance_id': attendance_id,\n",
        "                            'employee_id': employee['id'],\n",
        "                            'employee_name': employee['name'],\n",
        "                            'timestamp': current_time,\n",
        "                            'frame_number': frame_number,\n",
        "                            'confidence': employee['similarity'],\n",
        "                            'bbox': face['bbox']\n",
        "                        }\n",
        "                        \n",
        "                        self.attendance_events.append(event)\n",
        "                        results.append(event)\n",
        "                        \n",
        "                        print(f\"‚úÖ Frame {frame_number}: {employee['name']} ({employee['similarity']:.3f})\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _generate_video_analytics(self, recognition_results, total_frames, duration):\n",
        "        \"\"\"Generate comprehensive video analytics\"\"\"\n",
        "        print(f\"\\\\nüìä VIDEO PROCESSING ANALYTICS\")\n",
        "        print(\"=\" * 40)\n",
        "        \n",
        "        # Basic statistics\n",
        "        unique_employees = len(set(r['employee_id'] for r in recognition_results))\n",
        "        avg_processing_time = np.mean(self.processing_stats['processing_times'])\n",
        "        \n",
        "        print(f\"üìà Processing Statistics:\")\n",
        "        print(f\"‚îú‚îÄ Total Frames: {total_frames:,}\")\n",
        "        print(f\"‚îú‚îÄ Processed Frames: {self.processing_stats['total_frames']:,}\")\n",
        "        print(f\"‚îú‚îÄ Faces Detected: {self.processing_stats['faces_detected']:,}\")\n",
        "        print(f\"‚îú‚îÄ Employees Recognized: {self.processing_stats['employees_recognized']:,}\")\n",
        "        print(f\"‚îú‚îÄ Unique Employees: {unique_employees}\")\n",
        "        print(f\"‚îú‚îÄ Avg Processing Time: {avg_processing_time:.1f}ms\")\n",
        "        print(f\"‚îî‚îÄ Effective FPS: {1000/avg_processing_time:.1f}\")\n",
        "        \n",
        "        # Generate attendance summary\n",
        "        if recognition_results:\n",
        "            self._show_attendance_summary(recognition_results)\n",
        "            self._plot_recognition_timeline(recognition_results, duration)\n",
        "        else:\n",
        "            print(\"\\\\n‚ö†Ô∏è No employees recognized in video\")\n",
        "    \n",
        "    def _show_attendance_summary(self, results):\n",
        "        \"\"\"Show attendance summary table\"\"\"\n",
        "        print(f\"\\\\nüë• ATTENDANCE SUMMARY\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        # Group by employee\n",
        "        employee_summary = defaultdict(list)\n",
        "        for result in results:\n",
        "            employee_summary[result['employee_name']].append(result)\n",
        "        \n",
        "        summary_data = []\n",
        "        for employee_name, events in employee_summary.items():\n",
        "            first_seen = min(e['frame_number'] for e in events)\n",
        "            last_seen = max(e['frame_number'] for e in events)\n",
        "            avg_confidence = np.mean([e['confidence'] for e in events])\n",
        "            total_detections = len(events)\n",
        "            \n",
        "            summary_data.append({\n",
        "                'Employee': employee_name,\n",
        "                'First Seen (Frame)': first_seen,\n",
        "                'Last Seen (Frame)': last_seen,\n",
        "                'Total Detections': total_detections,\n",
        "                'Avg Confidence': f\"{avg_confidence:.3f}\"\n",
        "            })\n",
        "        \n",
        "        # Display as DataFrame\n",
        "        df = pd.DataFrame(summary_data)\n",
        "        display(HTML(df.to_html(index=False)))\n",
        "    \n",
        "    def _plot_recognition_timeline(self, results, duration):\n",
        "        \"\"\"Plot recognition timeline\"\"\"\n",
        "        print(f\"\\\\nüìà RECOGNITION TIMELINE\")\n",
        "        \n",
        "        # Create timeline plot\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=1,\n",
        "            subplot_titles=('Recognition Timeline', 'Confidence Distribution'),\n",
        "            vertical_spacing=0.12\n",
        "        )\n",
        "        \n",
        "        # Timeline plot\n",
        "        for employee_name in set(r['employee_name'] for r in results):\n",
        "            employee_events = [r for r in results if r['employee_name'] == employee_name]\n",
        "            frame_numbers = [e['frame_number'] for e in employee_events]\n",
        "            confidences = [e['confidence'] for e in employee_events]\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=frame_numbers,\n",
        "                    y=confidences,\n",
        "                    mode='markers+lines',\n",
        "                    name=employee_name,\n",
        "                    line=dict(width=2),\n",
        "                    marker=dict(size=8)\n",
        "                ),\n",
        "                row=1, col=1\n",
        "            )\n",
        "        \n",
        "        # Confidence distribution\n",
        "        all_confidences = [r['confidence'] for r in results]\n",
        "        fig.add_trace(\n",
        "            go.Histogram(\n",
        "                x=all_confidences,\n",
        "                nbinsx=20,\n",
        "                name='Confidence Distribution',\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            height=600,\n",
        "            title_text=\"Video Processing Analytics\",\n",
        "            showlegend=True\n",
        "        )\n",
        "        \n",
        "        fig.update_xaxes(title_text=\"Frame Number\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Confidence\", row=1, col=1)\n",
        "        fig.update_xaxes(title_text=\"Confidence Score\", row=2, col=1)\n",
        "        fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
        "        \n",
        "        fig.show()\n",
        "    \n",
        "    def process_webcam_demo(self, duration_seconds=30):\n",
        "        \"\"\"Demo webcam processing (simulated in Colab)\"\"\"\n",
        "        print(f\"üéÆ WEBCAM DEMO SIMULATION ({duration_seconds}s)\")\n",
        "        print(\"Note: Simulated since Colab doesn't support direct webcam access\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        # Simulate webcam frames\n",
        "        import random\n",
        "        \n",
        "        frame_count = 0\n",
        "        fps = 10  # Simulated FPS\n",
        "        total_frames = duration_seconds * fps\n",
        "        \n",
        "        print(\"Simulating real-time processing...\")\n",
        "        \n",
        "        with tqdm(total=total_frames, desc=\"Processing webcam\") as pbar:\n",
        "            for i in range(total_frames):\n",
        "                frame_count += 1\n",
        "                \n",
        "                # Simulate processing delay\n",
        "                time.sleep(0.1)  # 100ms simulated processing\n",
        "                \n",
        "                # Randomly simulate face detection\n",
        "                if random.random() < 0.3:  # 30% chance of face detection\n",
        "                    print(f\"\\\\rFrame {frame_count}: Face detected\", end=\"\")\n",
        "                \n",
        "                pbar.update(1)\n",
        "        \n",
        "        print(f\"\\\\n‚úÖ Webcam demo completed: {frame_count} frames processed\")\n",
        "\n",
        "# Initialize video processor\n",
        "video_processor = VideoProcessor(ai_system, db)\n",
        "\n",
        "print(\"üéØ Video Processing Options:\")\n",
        "print(\"1. Upload and process video file\")\n",
        "print(\"2. Webcam demo simulation\")\n",
        "print(\"3. Batch video processing\")\n",
        "print(\"\\\\nRun the appropriate methods below:\")\n",
        "print(\"=\" * 35)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Cell 5: üìä Performance Dashboard & Analytics\n",
        "\n",
        "**Real-time performance monitoring with interactive charts and system metrics**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance Dashboard & Analytics\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import psutil\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(\"üìä PERFORMANCE DASHBOARD & ANALYTICS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "class PerformanceDashboard:\n",
        "    \"\"\"Real-time performance monitoring dashboard\"\"\"\n",
        "    \n",
        "    def __init__(self, ai_system, database, video_processor):\n",
        "        self.ai_system = ai_system\n",
        "        self.db = database\n",
        "        self.video_processor = video_processor\n",
        "        self.system_metrics = []\n",
        "        self.start_time = time.time()\n",
        "    \n",
        "    def show_real_time_dashboard(self):\n",
        "        \"\"\"Display comprehensive real-time dashboard\"\"\"\n",
        "        print(\"üöÄ REAL-TIME PERFORMANCE DASHBOARD\")\n",
        "        print(\"=\" * 45)\n",
        "        \n",
        "        # Collect current metrics\n",
        "        current_metrics = self._collect_system_metrics()\n",
        "        \n",
        "        # Create dashboard plots\n",
        "        self._create_performance_plots()\n",
        "        self._show_system_status()\n",
        "        self._show_database_analytics()\n",
        "        self._show_ai_model_performance()\n",
        "    \n",
        "    def _collect_system_metrics(self):\n",
        "        \"\"\"Collect current system metrics\"\"\"\n",
        "        import torch\n",
        "        \n",
        "        metrics = {\n",
        "            'timestamp': datetime.now(),\n",
        "            'cpu_usage': psutil.cpu_percent(),\n",
        "            'memory_usage': psutil.virtual_memory().percent,\n",
        "            'memory_available': psutil.virtual_memory().available / (1024**3),  # GB\n",
        "            'gpu_available': torch.cuda.is_available(),\n",
        "            'processing_fps': 0,\n",
        "            'total_inferences': self.ai_system.performance_stats.get('total_inferences', 0),\n",
        "            'avg_latency': self.ai_system.performance_stats.get('avg_latency_ms', 0)\n",
        "        }\n",
        "        \n",
        "        # Calculate processing FPS\n",
        "        if metrics['avg_latency'] > 0:\n",
        "            metrics['processing_fps'] = 1000 / metrics['avg_latency']\n",
        "        \n",
        "        # GPU metrics if available\n",
        "        if torch.cuda.is_available():\n",
        "            metrics['gpu_memory_used'] = torch.cuda.memory_allocated() / (1024**3)  # GB\n",
        "            metrics['gpu_memory_total'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "            metrics['gpu_utilization'] = torch.cuda.utilization() if hasattr(torch.cuda, 'utilization') else 0\n",
        "        \n",
        "        self.system_metrics.append(metrics)\n",
        "        return metrics\n",
        "    \n",
        "    def _create_performance_plots(self):\n",
        "        \"\"\"Create comprehensive performance visualization\"\"\"\n",
        "        \n",
        "        # Create subplots\n",
        "        fig = make_subplots(\n",
        "            rows=3, cols=2,\n",
        "            subplot_titles=(\n",
        "                'System CPU & Memory Usage', 'AI Model Performance',\n",
        "                'Processing Timeline', 'Database Statistics',\n",
        "                'Recognition Accuracy', 'System Temperature'\n",
        "            ),\n",
        "            specs=[[{\"secondary_y\": True}, {\"secondary_y\": True}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]],\n",
        "            vertical_spacing=0.08\n",
        "        )\n",
        "        \n",
        "        # 1. System Resource Usage\n",
        "        if len(self.system_metrics) > 1:\n",
        "            timestamps = [m['timestamp'] for m in self.system_metrics[-20:]]  # Last 20 points\n",
        "            cpu_usage = [m['cpu_usage'] for m in self.system_metrics[-20:]]\n",
        "            memory_usage = [m['memory_usage'] for m in self.system_metrics[-20:]]\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=timestamps, y=cpu_usage, name='CPU %', line=dict(color='blue')),\n",
        "                row=1, col=1\n",
        "            )\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=timestamps, y=memory_usage, name='Memory %', line=dict(color='red')),\n",
        "                row=1, col=1\n",
        "            )\n",
        "        \n",
        "        # 2. AI Model Performance\n",
        "        latest_metrics = self.system_metrics[-1] if self.system_metrics else {}\n",
        "        ai_metrics = ['Total Inferences', 'Avg Latency (ms)', 'Processing FPS']\n",
        "        ai_values = [\n",
        "            latest_metrics.get('total_inferences', 0),\n",
        "            latest_metrics.get('avg_latency', 0),\n",
        "            latest_metrics.get('processing_fps', 0)\n",
        "        ]\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(x=ai_metrics, y=ai_values, name='AI Performance', marker_color='green'),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # 3. Processing Timeline (if video processing occurred)\n",
        "        if hasattr(self.video_processor, 'processing_stats') and self.video_processor.processing_stats['processing_times']:\n",
        "            processing_times = list(self.video_processor.processing_stats['processing_times'])\n",
        "            frame_numbers = list(range(len(processing_times)))\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=frame_numbers, y=processing_times, name='Frame Processing Time', \n",
        "                          line=dict(color='orange')),\n",
        "                row=2, col=1\n",
        "            )\n",
        "        \n",
        "        # 4. Database Statistics\n",
        "        db_stats = self.db.get_statistics()\n",
        "        db_labels = list(db_stats.keys())\n",
        "        db_values = list(db_stats.values())\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(x=db_labels, y=db_values, name='Database Stats', marker_color='purple'),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        # 5. Recognition Accuracy (if attendance events exist)\n",
        "        if hasattr(self.video_processor, 'attendance_events') and self.video_processor.attendance_events:\n",
        "            confidences = [event['confidence'] for event in self.video_processor.attendance_events]\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Histogram(x=confidences, nbinsx=15, name='Recognition Confidence', \n",
        "                           marker_color='cyan'),\n",
        "                row=3, col=1\n",
        "            )\n",
        "        \n",
        "        # 6. System Temperature (simulated)\n",
        "        import random\n",
        "        temp_data = [random.uniform(45, 75) for _ in range(10)]  # Simulated temperatures\n",
        "        temp_labels = [f'Sensor {i+1}' for i in range(10)]\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=temp_labels, y=temp_data, mode='markers+lines', \n",
        "                      name='System Temperature', line=dict(color='red')),\n",
        "            row=3, col=2\n",
        "        )\n",
        "        \n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            height=900,\n",
        "            title_text=\"üöÄ AI Attendance System - Performance Dashboard\",\n",
        "            showlegend=True\n",
        "        )\n",
        "        \n",
        "        # Update axes labels\n",
        "        fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Usage %\", row=1, col=1)\n",
        "        fig.update_xaxes(title_text=\"Metrics\", row=1, col=2)\n",
        "        fig.update_yaxes(title_text=\"Values\", row=1, col=2)\n",
        "        fig.update_xaxes(title_text=\"Frame Number\", row=2, col=1)\n",
        "        fig.update_yaxes(title_text=\"Processing Time (ms)\", row=2, col=1)\n",
        "        fig.update_xaxes(title_text=\"Database Tables\", row=2, col=2)\n",
        "        fig.update_yaxes(title_text=\"Count\", row=2, col=2)\n",
        "        fig.update_xaxes(title_text=\"Confidence Score\", row=3, col=1)\n",
        "        fig.update_yaxes(title_text=\"Frequency\", row=3, col=1)\n",
        "        fig.update_xaxes(title_text=\"Sensors\", row=3, col=2)\n",
        "        fig.update_yaxes(title_text=\"Temperature (¬∞C)\", row=3, col=2)\n",
        "        \n",
        "        fig.show()\n",
        "    \n",
        "    def _show_system_status(self):\n",
        "        \"\"\"Show current system status\"\"\"\n",
        "        latest = self.system_metrics[-1] if self.system_metrics else {}\n",
        "        \n",
        "        print(f\"\\\\nüñ•Ô∏è SYSTEM STATUS\")\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"‚îú‚îÄ CPU Usage: {latest.get('cpu_usage', 0):.1f}%\")\n",
        "        print(f\"‚îú‚îÄ Memory Usage: {latest.get('memory_usage', 0):.1f}%\")\n",
        "        print(f\"‚îú‚îÄ Memory Available: {latest.get('memory_available', 0):.2f} GB\")\n",
        "        print(f\"‚îú‚îÄ GPU Available: {latest.get('gpu_available', False)}\")\n",
        "        \n",
        "        if latest.get('gpu_available'):\n",
        "            print(f\"‚îú‚îÄ GPU Memory Used: {latest.get('gpu_memory_used', 0):.2f} GB\")\n",
        "            print(f\"‚îî‚îÄ GPU Memory Total: {latest.get('gpu_memory_total', 0):.2f} GB\")\n",
        "        else:\n",
        "            print(f\"‚îî‚îÄ GPU: Not Available\")\n",
        "    \n",
        "    def _show_database_analytics(self):\n",
        "        \"\"\"Show detailed database analytics\"\"\"\n",
        "        print(f\"\\\\nüóÑÔ∏è DATABASE ANALYTICS\")\n",
        "        print(\"-\" * 25)\n",
        "        \n",
        "        # Get enhanced statistics\n",
        "        cursor = self.db.conn.cursor()\n",
        "        \n",
        "        # Employee statistics\n",
        "        cursor.execute(\"\"\"\n",
        "        SELECT \n",
        "            COUNT(*) as total_employees,\n",
        "            AVG(face_count) as avg_faces_per_employee,\n",
        "            AVG(avg_quality) as avg_face_quality\n",
        "        FROM employees WHERE is_active = 1\n",
        "        \"\"\")\n",
        "        emp_stats = cursor.fetchone()\n",
        "        \n",
        "        # Attendance statistics\n",
        "        cursor.execute(\"\"\"\n",
        "        SELECT \n",
        "            COUNT(*) as total_logs,\n",
        "            COUNT(DISTINCT employee_id) as unique_employees_today,\n",
        "            AVG(confidence) as avg_confidence\n",
        "        FROM attendance_logs \n",
        "        WHERE DATE(timestamp) = DATE('now')\n",
        "        \"\"\")\n",
        "        att_stats = cursor.fetchone()\n",
        "        \n",
        "        print(f\"üìä Employee Data:\")\n",
        "        print(f\"‚îú‚îÄ Total Employees: {emp_stats[0] if emp_stats[0] else 0}\")\n",
        "        print(f\"‚îú‚îÄ Avg Faces/Employee: {emp_stats[1]:.1f if emp_stats[1] else 0}\")\n",
        "        print(f\"‚îî‚îÄ Avg Face Quality: {emp_stats[2]:.3f if emp_stats[2] else 0}\")\n",
        "        \n",
        "        print(f\"\\\\nüìù Attendance Logs:\")\n",
        "        print(f\"‚îú‚îÄ Today's Logs: {att_stats[0] if att_stats[0] else 0}\")\n",
        "        print(f\"‚îú‚îÄ Unique Employees: {att_stats[1] if att_stats[1] else 0}\")\n",
        "        print(f\"‚îî‚îÄ Avg Confidence: {att_stats[2]:.3f if att_stats[2] else 0}\")\n",
        "    \n",
        "    def _show_ai_model_performance(self):\n",
        "        \"\"\"Show AI model performance metrics\"\"\"\n",
        "        stats = self.ai_system.performance_stats\n",
        "        \n",
        "        print(f\"\\\\nü§ñ AI MODEL PERFORMANCE\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"‚îú‚îÄ Total Inferences: {stats.get('total_inferences', 0):,}\")\n",
        "        print(f\"‚îú‚îÄ Average Latency: {stats.get('avg_latency_ms', 0):.1f} ms\")\n",
        "        print(f\"‚îú‚îÄ Processing FPS: {1000/stats.get('avg_latency_ms', 1):.1f}\")\n",
        "        print(f\"‚îú‚îÄ GPU Acceleration: {stats.get('gpu_available', False)}\")\n",
        "        \n",
        "        # Performance rating\n",
        "        avg_latency = stats.get('avg_latency_ms', float('inf'))\n",
        "        if avg_latency < 50:\n",
        "            rating = \"üåü EXCELLENT\"\n",
        "        elif avg_latency < 100:\n",
        "            rating = \"üëç GOOD\"\n",
        "        elif avg_latency < 200:\n",
        "            rating = \"‚ö†Ô∏è MODERATE\"\n",
        "        else:\n",
        "            rating = \"üêå SLOW\"\n",
        "        \n",
        "        print(f\"‚îî‚îÄ Performance: {rating}\")\n",
        "    \n",
        "    def export_performance_report(self):\n",
        "        \"\"\"Export comprehensive performance report\"\"\"\n",
        "        print(\"üì§ Exporting Performance Report...\")\n",
        "        \n",
        "        # Create comprehensive report\n",
        "        report_data = {\n",
        "            'system_info': {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'session_duration': time.time() - self.start_time,\n",
        "                'python_version': sys.version,\n",
        "                'platform': platform.platform()\n",
        "            },\n",
        "            'ai_performance': self.ai_system.performance_stats,\n",
        "            'database_stats': self.db.get_statistics(),\n",
        "            'system_metrics': self.system_metrics[-10:] if self.system_metrics else [],  # Last 10 readings\n",
        "            'video_processing': {\n",
        "                'total_frames': getattr(self.video_processor.processing_stats, 'total_frames', 0),\n",
        "                'faces_detected': getattr(self.video_processor.processing_stats, 'faces_detected', 0),\n",
        "                'employees_recognized': getattr(self.video_processor.processing_stats, 'employees_recognized', 0)\n",
        "            } if hasattr(self.video_processor, 'processing_stats') else {}\n",
        "        }\n",
        "        \n",
        "        # Export to JSON\n",
        "        report_path = exports_dir / f'performance_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(report_data, f, indent=2, default=str)\n",
        "        \n",
        "        print(f\"‚úÖ Performance report exported to: {report_path}\")\n",
        "        \n",
        "        # Download in Colab\n",
        "        try:\n",
        "            files.download(str(report_path))\n",
        "        except:\n",
        "            print(\"üí° Report saved to exports/ directory\")\n",
        "\n",
        "# Initialize dashboard\n",
        "dashboard = PerformanceDashboard(ai_system, db, video_processor)\n",
        "\n",
        "print(\"üéØ Dashboard Options:\")\n",
        "print(\"1. Show real-time dashboard\")\n",
        "print(\"2. Export performance report\")\n",
        "print(\"3. System health check\")\n",
        "print(\"\\\\nRun dashboard.show_real_time_dashboard() for live metrics\")\n",
        "print(\"=\" * 40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Cell 6: üìã Export & Reporting System\n",
        "\n",
        "**Comprehensive data export with multiple formats and attendance reports**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"header_title\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"# üéØ AI Attendance System - Enhanced Colab Demo\\n\",\n",
        "    \"\\n\",\n",
        "    \"**Pipeline V1: SCRFD Detection + ArcFace Recognition + SQLite Database**\\n\",\n",
        "    \"\\n\",\n",
        "    \"Repository: `hoangh-e/auto-face-attendance/tree/pipeline-v1.0/`\\n\",\n",
        "    \"\\n\",\n",
        "    \"---\\n\",\n",
        "    \"\\n\",\n",
        "    \"## üìã Features Overview\\n\",\n",
        "    \"\\n\",\n",
        "    \"‚úÖ **Quick Setup**: One-cell installation with progress tracking  \\n\",\n",
        "    \"‚úÖ **Employee Management**: Batch registration with folder upload  \\n\",\n",
        "    \"‚úÖ **Advanced Video Processing**: Multi-format support with real-time metrics  \\n\",\n",
        "    \"‚úÖ **Performance Dashboard**: Live charts and statistics  \\n\",\n",
        "    \"‚úÖ **Export Functionality**: Multiple format support (JSON, CSV, Excel)  \\n\",\n",
        "    \"‚úÖ **Error Recovery**: Comprehensive error handling with user guidance  \\n\",\n",
        "    \"\\n\",\n",
        "    \"---\"\n",
        "   ]\n",
        "  }\n",
        " ]\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
