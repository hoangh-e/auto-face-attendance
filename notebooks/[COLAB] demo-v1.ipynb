{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGJXj08k9XPL",
        "outputId": "ca6930ed-ff8f-47af-8e7c-7d0413251ffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ CELL 1: SYSTEM SETUP & INSTALLATION\n",
            "=============================================\n",
            "üîç ENVIRONMENT DETECTION:\n",
            "‚îú‚îÄ Platform: Linux 6.1.123+\n",
            "‚îú‚îÄ Python: 3.11.13\n",
            "‚îú‚îÄ Working Directory: /content\n",
            "‚îî‚îÄ Colab Environment: True\n",
            "üöÄ GPU: ‚úÖ Tesla T4 (15.8GB)\n",
            "   ‚îî‚îÄ Will be used for HIGH-QUALITY face recognition\n",
            "\n",
            "üì¶ INSTALLING PACKAGES:\n",
            "‚îú‚îÄ [1/13] Installing torch...\n",
            "    ‚úÖ Installed successfully\n",
            "‚îú‚îÄ [2/13] Installing torchvision...\n",
            "    ‚úÖ Installed successfully\n",
            "‚îú‚îÄ [3/13] Installing insightface...\n",
            "    ‚úÖ Installed successfully\n",
            "‚îú‚îÄ [4/13] Installing opencv-python...\n",
            "    ‚úÖ Installed successfully\n",
            "‚îú‚îÄ [5/13] Installing scikit-learn...\n",
            "    ‚úÖ Installed successfully\n",
            "‚îú‚îÄ [6/13] Installing matplotlib...\n",
            "    ‚úÖ Installed successfully\n",
            "‚îú‚îÄ [7/13] Installing pandas...\n",
            "    ‚úÖ Installed successfully\n",
            "‚îú‚îÄ [8/13] Installing tqdm...\n",
            "    ‚úÖ Installed successfully\n",
            "‚îú‚îÄ [9/13] Installing pillow...\n",
            "    ‚úÖ Installed successfully\n",
            "‚îú‚îÄ [10/13] Installing numpy...\n",
            "    ‚úÖ Installed successfully\n",
            "‚îú‚îÄ [11/13] Installing plotly...\n",
            "    ‚úÖ Installed successfully\n",
            "‚îú‚îÄ [12/13] Installing ipywidgets...\n",
            "    ‚úÖ Installed successfully\n",
            "‚îú‚îÄ [13/13] Installing psutil...\n",
            "    ‚úÖ Installed successfully\n",
            "\n",
            "üìÅ PIPELINE V1 DIRECTORY STRUCTURE:\n",
            "‚îú‚îÄ Data Root: /content/attendance_data\n",
            "‚îú‚îÄ Employees: /content/attendance_data/employees\n",
            "‚îú‚îÄ Videos: /content/attendance_data/videos\n",
            "‚îú‚îÄ Snapshots: /content/attendance_data/snapshots\n",
            "‚îú‚îÄ Exports: /content/attendance_data/exports\n",
            "‚îî‚îÄ Database: /content/attendance_data/database\n",
            "\n",
            "üéâ SYSTEM SETUP COMPLETED!\n",
            "Ready for Pipeline V1 dual-stream processing!\n",
            "=============================================\n",
            "‚úÖ Cell 1 completed. Import this file to use setup_system() function.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CELL 1: SYSTEM SETUP & INSTALLATION\n",
        "- Environment detection (GPU/CPU)\n",
        "- Package installation with progress tracking\n",
        "- Directory structure creation\n",
        "- System optimization for dual-stream processing\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "import platform\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, clear_output, HTML\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "    print(\"‚ö†Ô∏è Not running in Colab environment\")\n",
        "\n",
        "def setup_system():\n",
        "    \"\"\"Setup system environment and install required packages\"\"\"\n",
        "    print(\"üì¶ CELL 1: SYSTEM SETUP & INSTALLATION\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    # Environment Detection\n",
        "    print(\"üîç ENVIRONMENT DETECTION:\")\n",
        "    print(f\"‚îú‚îÄ Platform: {platform.system()} {platform.release()}\")\n",
        "    print(f\"‚îú‚îÄ Python: {sys.version.split()[0]}\")\n",
        "    print(f\"‚îú‚îÄ Working Directory: {os.getcwd()}\")\n",
        "    print(f\"‚îî‚îÄ Colab Environment: {COLAB_ENV}\")\n",
        "\n",
        "    # GPU Detection\n",
        "    gpu_available = False\n",
        "    try:\n",
        "        import torch\n",
        "        gpu_available = torch.cuda.is_available()\n",
        "        if gpu_available:\n",
        "            gpu_name = torch.cuda.get_device_name(0)\n",
        "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "            print(f\"üöÄ GPU: ‚úÖ {gpu_name} ({gpu_memory:.1f}GB)\")\n",
        "            print(\"   ‚îî‚îÄ Will be used for HIGH-QUALITY face recognition\")\n",
        "        else:\n",
        "            print(f\"üíª GPU: ‚ùå CPU mode\")\n",
        "            print(\"   ‚îî‚îÄ CPU will handle both person detection and face recognition\")\n",
        "    except ImportError:\n",
        "        print(f\"üì¶ PyTorch: Installing...\")\n",
        "\n",
        "    # Install packages with progress tracking\n",
        "    print(f\"\\nüì¶ INSTALLING PACKAGES:\")\n",
        "\n",
        "    packages = [\n",
        "        'torch>=1.9.0',\n",
        "        'torchvision>=0.10.0',\n",
        "        'insightface>=0.7.3',\n",
        "        'opencv-python>=4.5.0',\n",
        "        'scikit-learn>=1.0.0',\n",
        "        'matplotlib>=3.5.0',\n",
        "        'pandas>=1.3.0',\n",
        "        'tqdm>=4.62.0',\n",
        "        'pillow>=8.3.0',\n",
        "        'numpy>=1.21.0',\n",
        "        'plotly>=5.0.0',\n",
        "        'ipywidgets>=7.6.0' if COLAB_ENV else 'ipywidgets>=7.6.0',\n",
        "        'psutil>=5.8.0'\n",
        "    ]\n",
        "\n",
        "    for i, package in enumerate(packages, 1):\n",
        "        try:\n",
        "            print(f\"‚îú‚îÄ [{i}/{len(packages)}] Installing {package.split('>=')[0]}...\")\n",
        "            result = subprocess.run(\n",
        "                [sys.executable, '-m', 'pip', 'install', package, '--quiet'],\n",
        "                capture_output=True, text=True, timeout=120\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(f\"    ‚úÖ Installed successfully\")\n",
        "            else:\n",
        "                print(f\"    ‚ö†Ô∏è Already installed or skipped\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"    ‚ö†Ô∏è Installation timeout, continuing...\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ùå Error: {e}\")\n",
        "\n",
        "    # Create directory structure for Pipeline V1\n",
        "    if COLAB_ENV:\n",
        "        work_dir = Path('/content')\n",
        "    else:\n",
        "        work_dir = Path.cwd()\n",
        "\n",
        "    # Pipeline V1 directory structure\n",
        "    data_dir = work_dir / 'attendance_data'\n",
        "    employees_dir = data_dir / 'employees'          # Employee photos\n",
        "    videos_dir = data_dir / 'videos'                # Uploaded videos\n",
        "    snapshots_dir = data_dir / 'snapshots'          # High-quality snapshots\n",
        "    exports_dir = data_dir / 'exports'              # Reports and exports\n",
        "    db_dir = data_dir / 'database'                  # SQLite database\n",
        "\n",
        "    directories = [data_dir, employees_dir, videos_dir, snapshots_dir, exports_dir, db_dir]\n",
        "\n",
        "    for directory in directories:\n",
        "        directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"\\nüìÅ PIPELINE V1 DIRECTORY STRUCTURE:\")\n",
        "    print(f\"‚îú‚îÄ Data Root: {data_dir}\")\n",
        "    print(f\"‚îú‚îÄ Employees: {employees_dir}\")\n",
        "    print(f\"‚îú‚îÄ Videos: {videos_dir}\")\n",
        "    print(f\"‚îú‚îÄ Snapshots: {snapshots_dir}\")\n",
        "    print(f\"‚îú‚îÄ Exports: {exports_dir}\")\n",
        "    print(f\"‚îî‚îÄ Database: {db_dir}\")\n",
        "\n",
        "    print(f\"\\nüéâ SYSTEM SETUP COMPLETED!\")\n",
        "    print(f\"Ready for Pipeline V1 dual-stream processing!\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    return {\n",
        "        'gpu_available': gpu_available,\n",
        "        'work_dir': work_dir,\n",
        "        'data_dir': data_dir,\n",
        "        'employees_dir': employees_dir,\n",
        "        'videos_dir': videos_dir,\n",
        "        'snapshots_dir': snapshots_dir,\n",
        "        'exports_dir': exports_dir,\n",
        "        'db_dir': db_dir\n",
        "    }\n",
        "\n",
        "# Run setup when imported\n",
        "if __name__ == \"__main__\":\n",
        "    config = setup_system()\n",
        "    print(\"‚úÖ Cell 1 completed. Import this file to use setup_system() function.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mei16hzfRj2M",
        "outputId": "5012214a-d336-48d4-b63b-0e9fec915f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: onnxruntime-gpu in /usr/local/lib/python3.11/dist-packages (1.22.0)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxruntime\n",
            "Successfully installed onnxruntime-1.22.1\n"
          ]
        }
      ],
      "source": [
        "!pip install onnxruntime-gpu onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8JOP4YHQRMK",
        "outputId": "21e221ce-ba81-4756-8b5d-62fbe97dbd53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ CELL 2: DUAL-STREAM AI MODELS INITIALIZATION\n",
            "==================================================\n",
            "üöÄ GPU Mode: Tesla T4\n",
            "‚îú‚îÄ CPU Stream: Person detection (low quality)\n",
            "‚îî‚îÄ GPU Stream: Face recognition (high quality)\n",
            "\n",
            "üì¶ Loading CPU Stream - buffalo_l...\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "set det-size: (320, 320)\n",
            "‚úÖ CPU Stream initialized (Person Detection)\n",
            "\n",
            "üì¶ Loading GPU Stream - buffalo_l...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "set det-size: (640, 640)\n",
            "‚úÖ GPU Stream initialized (Face Recognition)\n",
            "\n",
            "üß™ DUAL-STREAM PERFORMANCE TEST:\n",
            "-----------------------------------\n",
            "‚îú‚îÄ Average Total Latency: 163.6ms\n",
            "‚îú‚îÄ Best Total Latency: 121.4ms\n",
            "‚îú‚îÄ CPU Detection Avg: 160.4ms\n",
            "‚îú‚îÄ GPU Recognition Avg: 0.0ms\n",
            "‚îú‚îÄ GPU Available: True\n",
            "‚îî‚îÄ Model Pack: buffalo_l\n",
            "üëç Performance: GOOD\n",
            "‚úÖ Cell 2 completed. AI System ready for dual-stream processing.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CELL 2: DUAL-STREAM AI SYSTEM\n",
        "- Initialize SCRFD + ArcFace models theo Pipeline V1\n",
        "- Setup dual-stream processing:\n",
        "  + CPU stream for person detection (low quality)\n",
        "  + GPU stream for face recognition (high quality snapshots)\n",
        "- Performance testing and optimization\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Optional, Tuple, Any\n",
        "\n",
        "# AI and ML imports\n",
        "import insightface\n",
        "from insightface.app import FaceAnalysis\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DualStreamAISystem:\n",
        "    \"\"\"\n",
        "    Dual-Stream AI System theo Pipeline V1\n",
        "    - CPU Stream: Person detection on low-quality frames\n",
        "    - GPU Stream: Face recognition on high-quality snapshots\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_pack='buffalo_l'):\n",
        "        self.model_pack = model_pack\n",
        "        self.gpu_available = False\n",
        "\n",
        "        # Dual stream models\n",
        "        self.cpu_app = None      # CPU for person detection\n",
        "        self.gpu_app = None      # GPU for face recognition\n",
        "\n",
        "        # Performance stats\n",
        "        self.performance_stats = {\n",
        "            'total_inferences': 0,\n",
        "            'cpu_detections': 0,\n",
        "            'gpu_recognitions': 0,\n",
        "            'avg_cpu_latency_ms': 0.0,\n",
        "            'avg_gpu_latency_ms': 0.0,\n",
        "            'total_cpu_time': 0.0,\n",
        "            'total_gpu_time': 0.0,\n",
        "            'gpu_available': False\n",
        "        }\n",
        "\n",
        "        print(\"ü§ñ CELL 2: DUAL-STREAM AI MODELS INITIALIZATION\")\n",
        "        print(\"=\" * 50)\n",
        "        self._init_dual_stream_models()\n",
        "        self._test_performance()\n",
        "\n",
        "    def _init_dual_stream_models(self):\n",
        "        \"\"\"Initialize dual-stream models according to Pipeline V1\"\"\"\n",
        "        try:\n",
        "            import torch\n",
        "            self.gpu_available = torch.cuda.is_available()\n",
        "            self.performance_stats['gpu_available'] = self.gpu_available\n",
        "\n",
        "            if self.gpu_available:\n",
        "                print(f\"üöÄ GPU Mode: {torch.cuda.get_device_name(0)}\")\n",
        "                print(\"‚îú‚îÄ CPU Stream: Person detection (low quality)\")\n",
        "                print(\"‚îî‚îÄ GPU Stream: Face recognition (high quality)\")\n",
        "\n",
        "                # Setup providers for dual stream\n",
        "                cpu_providers = ['CPUExecutionProvider']\n",
        "                gpu_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "\n",
        "                # Initialize CPU stream for person detection\n",
        "                print(f\"\\nüì¶ Loading CPU Stream - {self.model_pack}...\")\n",
        "                self.cpu_app = FaceAnalysis(name=self.model_pack, providers=cpu_providers)\n",
        "                self.cpu_app.prepare(ctx_id=-1, det_size=(320, 320), det_thresh=0.3)  # Lower quality for CPU\n",
        "                print(\"‚úÖ CPU Stream initialized (Person Detection)\")\n",
        "\n",
        "                # Initialize GPU stream for face recognition\n",
        "                print(f\"\\nüì¶ Loading GPU Stream - {self.model_pack}...\")\n",
        "                self.gpu_app = FaceAnalysis(name=self.model_pack, providers=gpu_providers)\n",
        "                self.gpu_app.prepare(ctx_id=0, det_size=(640, 640), det_thresh=0.5)   # Higher quality for GPU\n",
        "                print(\"‚úÖ GPU Stream initialized (Face Recognition)\")\n",
        "\n",
        "            else:\n",
        "                print(\"üíª CPU Only Mode: Single stream processing\")\n",
        "                print(\"‚îî‚îÄ CPU will handle both detection and recognition\")\n",
        "\n",
        "                # Single CPU stream\n",
        "                cpu_providers = ['CPUExecutionProvider']\n",
        "                self.cpu_app = FaceAnalysis(name=self.model_pack, providers=cpu_providers)\n",
        "                self.cpu_app.prepare(ctx_id=-1, det_size=(640, 640), det_thresh=0.5)\n",
        "                print(\"‚úÖ CPU Stream initialized (Detection + Recognition)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Model loading failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def detect_person_cpu(self, low_quality_frame: np.ndarray) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        CPU Stream: Person detection on low-quality frames\n",
        "        Used for initial person detection in zones (Pipeline V1)\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # CPU detection with lower threshold for person detection\n",
        "            faces = self.cpu_app.get(low_quality_frame)\n",
        "\n",
        "            # Update CPU performance stats\n",
        "            cpu_time = (time.time() - start_time) * 1000\n",
        "            self.performance_stats['cpu_detections'] += 1\n",
        "            self.performance_stats['total_cpu_time'] += cpu_time\n",
        "            if self.performance_stats['cpu_detections'] > 0:\n",
        "                self.performance_stats['avg_cpu_latency_ms'] = (\n",
        "                    self.performance_stats['total_cpu_time'] /\n",
        "                    self.performance_stats['cpu_detections']\n",
        "                )\n",
        "\n",
        "            # Format results for person detection\n",
        "            results = []\n",
        "            for face in faces:\n",
        "                result = {\n",
        "                    'bbox': face.bbox,\n",
        "                    'det_score': float(face.det_score),\n",
        "                    'landmarks': getattr(face, 'kps', None),\n",
        "                    'stream_type': 'cpu_detection',\n",
        "                    'processing_time_ms': cpu_time\n",
        "                }\n",
        "                results.append(result)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"CPU detection error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def recognize_face_gpu(self, high_quality_snapshot: np.ndarray) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        GPU Stream: Face recognition on high-quality snapshots\n",
        "        Used for actual employee recognition (Pipeline V1)\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Use GPU stream if available, otherwise fallback to CPU\n",
        "            app = self.gpu_app if self.gpu_available else self.cpu_app\n",
        "            faces = app.get(high_quality_snapshot)\n",
        "\n",
        "            # Update GPU performance stats\n",
        "            gpu_time = (time.time() - start_time) * 1000\n",
        "            self.performance_stats['gpu_recognitions'] += 1\n",
        "            self.performance_stats['total_gpu_time'] += gpu_time\n",
        "            if self.performance_stats['gpu_recognitions'] > 0:\n",
        "                self.performance_stats['avg_gpu_latency_ms'] = (\n",
        "                    self.performance_stats['total_gpu_time'] /\n",
        "                    self.performance_stats['gpu_recognitions']\n",
        "                )\n",
        "\n",
        "            # Format results with embeddings for recognition\n",
        "            results = []\n",
        "            for face in faces:\n",
        "                result = {\n",
        "                    'bbox': face.bbox,\n",
        "                    'det_score': float(face.det_score),\n",
        "                    'landmarks': getattr(face, 'kps', None),\n",
        "                    'embedding': face.embedding,  # 512-dim ArcFace embedding\n",
        "                    'age': getattr(face, 'age', None),\n",
        "                    'gender': getattr(face, 'gender', None),\n",
        "                    'stream_type': 'gpu_recognition',\n",
        "                    'processing_time_ms': gpu_time,\n",
        "                    'embedding_norm': float(np.linalg.norm(face.embedding))\n",
        "                }\n",
        "                results.append(result)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"GPU recognition error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def process_dual_stream(self, low_quality_frame: np.ndarray,\n",
        "                           high_quality_frame: np.ndarray = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Process dual stream according to Pipeline V1:\n",
        "        1. CPU detects person in low-quality frame\n",
        "        2. If person detected, GPU processes high-quality frame for recognition\n",
        "        \"\"\"\n",
        "        result = {\n",
        "            'person_detected': False,\n",
        "            'faces_recognized': [],\n",
        "            'cpu_detections': [],\n",
        "            'processing_summary': {\n",
        "                'cpu_time_ms': 0.0,\n",
        "                'gpu_time_ms': 0.0,\n",
        "                'total_time_ms': 0.0\n",
        "            }\n",
        "        }\n",
        "\n",
        "        total_start = time.time()\n",
        "\n",
        "        # Step 1: CPU person detection on low-quality frame\n",
        "        cpu_detections = self.detect_person_cpu(low_quality_frame)\n",
        "        result['cpu_detections'] = cpu_detections\n",
        "\n",
        "        if cpu_detections:\n",
        "            result['person_detected'] = True\n",
        "\n",
        "            # Step 2: GPU face recognition on high-quality frame (if provided)\n",
        "            if high_quality_frame is not None:\n",
        "                gpu_recognitions = self.recognize_face_gpu(high_quality_frame)\n",
        "                result['faces_recognized'] = gpu_recognitions\n",
        "            else:\n",
        "                # Fallback: use low-quality frame for recognition\n",
        "                gpu_recognitions = self.recognize_face_gpu(low_quality_frame)\n",
        "                result['faces_recognized'] = gpu_recognitions\n",
        "\n",
        "        # Update performance summary\n",
        "        total_time = (time.time() - total_start) * 1000\n",
        "        result['processing_summary']['total_time_ms'] = total_time\n",
        "        result['processing_summary']['cpu_time_ms'] = sum(d.get('processing_time_ms', 0) for d in cpu_detections)\n",
        "        result['processing_summary']['gpu_time_ms'] = sum(f.get('processing_time_ms', 0) for f in result['faces_recognized'])\n",
        "\n",
        "        self.performance_stats['total_inferences'] += 1\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _test_performance(self):\n",
        "        \"\"\"Test dual-stream performance\"\"\"\n",
        "        print(\"\\nüß™ DUAL-STREAM PERFORMANCE TEST:\")\n",
        "        print(\"-\" * 35)\n",
        "\n",
        "        try:\n",
        "            # Create test images\n",
        "            low_quality_test = np.random.randint(0, 255, (240, 320, 3), dtype=np.uint8)   # Sub-stream\n",
        "            high_quality_test = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)  # Main-stream\n",
        "\n",
        "            # Warm up\n",
        "            for _ in range(3):\n",
        "                self.process_dual_stream(low_quality_test, high_quality_test)\n",
        "\n",
        "            # Performance test\n",
        "            times = []\n",
        "            for i in range(5):\n",
        "                start_time = time.time()\n",
        "                result = self.process_dual_stream(low_quality_test, high_quality_test)\n",
        "                end_time = time.time()\n",
        "                times.append((end_time - start_time) * 1000)\n",
        "\n",
        "            avg_latency = np.mean(times)\n",
        "            min_latency = np.min(times)\n",
        "\n",
        "            print(f\"‚îú‚îÄ Average Total Latency: {avg_latency:.1f}ms\")\n",
        "            print(f\"‚îú‚îÄ Best Total Latency: {min_latency:.1f}ms\")\n",
        "            print(f\"‚îú‚îÄ CPU Detection Avg: {self.performance_stats['avg_cpu_latency_ms']:.1f}ms\")\n",
        "            print(f\"‚îú‚îÄ GPU Recognition Avg: {self.performance_stats['avg_gpu_latency_ms']:.1f}ms\")\n",
        "            print(f\"‚îú‚îÄ GPU Available: {self.gpu_available}\")\n",
        "            print(f\"‚îî‚îÄ Model Pack: {self.model_pack}\")\n",
        "\n",
        "            # Performance rating\n",
        "            if avg_latency < 100:\n",
        "                print(\"üåü Performance: EXCELLENT (real-time capable)\")\n",
        "            elif avg_latency < 300:\n",
        "                print(\"üëç Performance: GOOD\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Performance: Needs optimization\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"‚ö†Ô∏è Performance test error: {e}\")\n",
        "\n",
        "    def get_performance_stats(self) -> Dict:\n",
        "        \"\"\"Get comprehensive performance statistics\"\"\"\n",
        "        stats = self.performance_stats.copy()\n",
        "\n",
        "        # Add system information\n",
        "        if self.gpu_available:\n",
        "            import torch\n",
        "            stats['gpu_name'] = torch.cuda.get_device_name(0)\n",
        "            stats['gpu_memory_allocated_mb'] = torch.cuda.memory_allocated() / 1024 / 1024\n",
        "\n",
        "        stats['model_pack'] = self.model_pack\n",
        "        stats['dual_stream_enabled'] = self.gpu_available\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        try:\n",
        "            if hasattr(self, 'cpu_app'):\n",
        "                del self.cpu_app\n",
        "            if hasattr(self, 'gpu_app'):\n",
        "                del self.gpu_app\n",
        "\n",
        "            # Clear GPU cache if available\n",
        "            if self.gpu_available:\n",
        "                import torch\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            logger.info(\"‚úÖ AI Models cleanup completed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"‚ö†Ô∏è Cleanup warning: {e}\")\n",
        "\n",
        "# Initialize AI system when imported\n",
        "if __name__ == \"__main__\":\n",
        "    ai_system = DualStreamAISystem()\n",
        "    print(\"‚úÖ Cell 2 completed. AI System ready for dual-stream processing.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFjeonMcQUfA",
        "outputId": "afc1690a-c2c0-4fe4-a122-478f4d6ed83b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üóÑÔ∏è CELL 3: SQLITE DATABASE INITIALIZATION\n",
            "=============================================\n",
            "üìä Database: attendance_pipeline_v1.db\n",
            "‚îú‚îÄ Vector Similarity: Cosine distance simulation\n",
            "‚îú‚îÄ Employee Management: Face embeddings storage\n",
            "‚îú‚îÄ Attendance Logging: Comprehensive metadata\n",
            "‚îî‚îÄ Migration Ready: PostgreSQL + pgvector compatible\n",
            "üîß Database connection optimized\n",
            "üóÑÔ∏è Pipeline V1 tables created\n",
            "üìá Database indexes created\n",
            "‚ö° Performance optimization applied\n",
            "‚öôÔ∏è Pipeline V1 configuration initialized\n",
            "‚úÖ Database initialization completed\n",
            "‚úÖ Cell 3 completed. Database ready for Pipeline V1 operations.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CELL 3: SQLITE DATABASE WITH VECTOR SIMILARITY SEARCH\n",
        "- SQLite database design theo Pipeline V1\n",
        "- Vector similarity search using cosine distance\n",
        "- Employee registration with face embeddings\n",
        "- Attendance logging with comprehensive metadata\n",
        "- Compatible v·ªõi PostgreSQL migration path\n",
        "\"\"\"\n",
        "\n",
        "import sqlite3\n",
        "import json\n",
        "import numpy as np\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional, Tuple, Any\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AttendanceDatabaseSQLite:\n",
        "    \"\"\"\n",
        "    SQLite database with vector operations simulation\n",
        "    Compatible with PostgreSQL + pgvector migration (Pipeline V1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, db_path: str = \"attendance_pipeline_v1.db\", enable_wal: bool = True):\n",
        "        \"\"\"Initialize SQLite database with Pipeline V1 schema\"\"\"\n",
        "        print(\"\\nüóÑÔ∏è CELL 3: SQLITE DATABASE INITIALIZATION\")\n",
        "        print(\"=\" * 45)\n",
        "\n",
        "        self.db_path = db_path if isinstance(db_path, Path) else Path(db_path)\n",
        "        self.enable_wal = enable_wal\n",
        "\n",
        "        print(f\"üìä Database: {self.db_path}\")\n",
        "        print(\"‚îú‚îÄ Vector Similarity: Cosine distance simulation\")\n",
        "        print(\"‚îú‚îÄ Employee Management: Face embeddings storage\")\n",
        "        print(\"‚îú‚îÄ Attendance Logging: Comprehensive metadata\")\n",
        "        print(\"‚îî‚îÄ Migration Ready: PostgreSQL + pgvector compatible\")\n",
        "\n",
        "        # Create database directory if needed\n",
        "        self.db_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Initialize connection\n",
        "        self._init_connection()\n",
        "\n",
        "        # Setup schema\n",
        "        self._create_tables()\n",
        "        self._create_indexes()\n",
        "        self._setup_performance_optimization()\n",
        "        self._initialize_system_config()\n",
        "\n",
        "        print(\"‚úÖ Database initialization completed\")\n",
        "\n",
        "    def _init_connection(self):\n",
        "        \"\"\"Initialize database connection with optimizations\"\"\"\n",
        "        self.conn = sqlite3.connect(\n",
        "            str(self.db_path),\n",
        "            check_same_thread=False,\n",
        "            timeout=30.0\n",
        "        )\n",
        "\n",
        "        # Enable row factory for dict-like access\n",
        "        self.conn.row_factory = sqlite3.Row\n",
        "\n",
        "        # Enable WAL mode for better concurrency\n",
        "        if self.enable_wal:\n",
        "            self.conn.execute(\"PRAGMA journal_mode=WAL\")\n",
        "\n",
        "        # Performance optimizations\n",
        "        self.conn.execute(\"PRAGMA synchronous=NORMAL\")\n",
        "        self.conn.execute(\"PRAGMA cache_size=10000\")\n",
        "        self.conn.execute(\"PRAGMA temp_store=MEMORY\")\n",
        "        self.conn.execute(\"PRAGMA mmap_size=268435456\")  # 256MB\n",
        "\n",
        "        print(\"üîß Database connection optimized\")\n",
        "\n",
        "    def _create_tables(self):\n",
        "        \"\"\"Create Pipeline V1 compatible schema\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        # Employees table with face embeddings (JSON format for SQLite)\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS employees (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            employee_code TEXT UNIQUE NOT NULL,\n",
        "            name TEXT NOT NULL,\n",
        "            email TEXT UNIQUE NOT NULL,\n",
        "            department TEXT,\n",
        "            position TEXT,\n",
        "            face_embeddings TEXT,  -- JSON array of 512-dim vectors\n",
        "            embedding_count INTEGER DEFAULT 0,\n",
        "            registration_quality REAL DEFAULT 0.0,\n",
        "            is_active BOOLEAN DEFAULT 1,\n",
        "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "        )\n",
        "        \"\"\")\n",
        "\n",
        "        # Attendance logs with dual-stream metadata\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS attendance_logs (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            employee_id INTEGER,\n",
        "            event_type TEXT NOT NULL CHECK (event_type IN ('check_in', 'check_out', 'video_detection')),\n",
        "            timestamp TIMESTAMP NOT NULL,\n",
        "            confidence REAL NOT NULL,\n",
        "\n",
        "            -- Dual-stream processing metadata\n",
        "            detection_stream TEXT,  -- 'cpu' or 'gpu'\n",
        "            recognition_stream TEXT,  -- 'cpu' or 'gpu'\n",
        "            cpu_processing_time_ms REAL,\n",
        "            gpu_processing_time_ms REAL,\n",
        "\n",
        "            -- Video processing metadata\n",
        "            video_source TEXT,\n",
        "            frame_number INTEGER,\n",
        "            snapshot_path TEXT,\n",
        "\n",
        "            -- Face detection metadata\n",
        "            face_bbox TEXT,  -- JSON [x1, y1, x2, y2]\n",
        "            face_quality REAL,\n",
        "            face_landmarks TEXT,  -- JSON landmarks\n",
        "\n",
        "            -- Business logic metadata\n",
        "            zone_detected TEXT,\n",
        "            cooldown_applied BOOLEAN DEFAULT 0,\n",
        "            work_hours_valid BOOLEAN DEFAULT 1,\n",
        "\n",
        "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "            FOREIGN KEY (employee_id) REFERENCES employees (id)\n",
        "        )\n",
        "        \"\"\")\n",
        "\n",
        "        # Individual face registrations for quality tracking\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS face_registrations (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            employee_id INTEGER,\n",
        "            image_path TEXT NOT NULL,\n",
        "            embedding TEXT NOT NULL,  -- JSON 512-dim vector\n",
        "            quality_score REAL,\n",
        "            detection_confidence REAL,\n",
        "            landmarks TEXT,  -- JSON landmarks\n",
        "            image_metadata TEXT,  -- JSON metadata (size, format, etc.)\n",
        "            stream_type TEXT,  -- 'cpu' or 'gpu'\n",
        "            processing_time_ms REAL,\n",
        "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "            FOREIGN KEY (employee_id) REFERENCES employees (id)\n",
        "        )\n",
        "        \"\"\")\n",
        "\n",
        "        # System configuration for Pipeline V1 business rules\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS system_config (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            config_key TEXT UNIQUE NOT NULL,\n",
        "            config_value TEXT NOT NULL,\n",
        "            description TEXT,\n",
        "            config_type TEXT DEFAULT 'string',\n",
        "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "        )\n",
        "        \"\"\")\n",
        "\n",
        "        # Performance metrics for dual-stream monitoring\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS performance_metrics (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            metric_type TEXT NOT NULL,\n",
        "            metric_value REAL NOT NULL,\n",
        "            stream_type TEXT,  -- 'cpu', 'gpu', 'dual'\n",
        "            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "            metadata TEXT  -- JSON additional context\n",
        "        )\n",
        "        \"\"\")\n",
        "\n",
        "        self.conn.commit()\n",
        "        print(\"üóÑÔ∏è Pipeline V1 tables created\")\n",
        "\n",
        "    def _create_indexes(self):\n",
        "        \"\"\"Create indexes for performance\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        indexes = [\n",
        "            # Attendance performance indexes\n",
        "            \"CREATE INDEX IF NOT EXISTS idx_attendance_employee_date ON attendance_logs(employee_id, DATE(timestamp))\",\n",
        "            \"CREATE INDEX IF NOT EXISTS idx_attendance_timestamp ON attendance_logs(timestamp)\",\n",
        "            \"CREATE INDEX IF NOT EXISTS idx_attendance_event_type ON attendance_logs(event_type)\",\n",
        "\n",
        "            # Employee indexes\n",
        "            \"CREATE INDEX IF NOT EXISTS idx_employees_active ON employees(is_active)\",\n",
        "            \"CREATE INDEX IF NOT EXISTS idx_employees_code ON employees(employee_code)\",\n",
        "\n",
        "            # Face registration indexes\n",
        "            \"CREATE INDEX IF NOT EXISTS idx_face_registrations_employee ON face_registrations(employee_id)\",\n",
        "            \"CREATE INDEX IF NOT EXISTS idx_face_registrations_quality ON face_registrations(quality_score)\",\n",
        "\n",
        "            # System config indexes\n",
        "            \"CREATE INDEX IF NOT EXISTS idx_system_config_key ON system_config(config_key)\",\n",
        "\n",
        "            # Performance metrics indexes\n",
        "            \"CREATE INDEX IF NOT EXISTS idx_performance_metrics_type ON performance_metrics(metric_type, timestamp)\",\n",
        "            \"CREATE INDEX IF NOT EXISTS idx_performance_metrics_stream ON performance_metrics(stream_type, timestamp)\"\n",
        "        ]\n",
        "\n",
        "        for index_sql in indexes:\n",
        "            cursor.execute(index_sql)\n",
        "\n",
        "        self.conn.commit()\n",
        "        print(\"üìá Database indexes created\")\n",
        "\n",
        "    def _setup_performance_optimization(self):\n",
        "        \"\"\"Setup additional performance optimizations\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        # Analyze tables for query optimization\n",
        "        cursor.execute(\"ANALYZE\")\n",
        "\n",
        "        # Enable automatic index recommendations\n",
        "        cursor.execute(\"PRAGMA optimize\")\n",
        "\n",
        "        self.conn.commit()\n",
        "        print(\"‚ö° Performance optimization applied\")\n",
        "\n",
        "    def _initialize_system_config(self):\n",
        "        \"\"\"Initialize Pipeline V1 default configuration\"\"\"\n",
        "        default_configs = [\n",
        "            # Recognition thresholds\n",
        "            (\"recognition_threshold\", \"0.65\", \"Minimum similarity threshold for face recognition\", \"float\"),\n",
        "            (\"detection_threshold_cpu\", \"0.3\", \"CPU detection confidence threshold\", \"float\"),\n",
        "            (\"detection_threshold_gpu\", \"0.5\", \"GPU detection confidence threshold\", \"float\"),\n",
        "\n",
        "            # Business logic\n",
        "            (\"cooldown_minutes\", \"30\", \"Minimum minutes between attendance records\", \"int\"),\n",
        "            (\"work_hours_start\", \"07:00\", \"Work day start time\", \"time\"),\n",
        "            (\"work_hours_end\", \"19:00\", \"Work day end time\", \"time\"),\n",
        "\n",
        "            # Dual-stream processing\n",
        "            (\"enable_dual_stream\", \"true\", \"Enable dual-stream processing\", \"bool\"),\n",
        "            (\"cpu_frame_resolution\", \"320x240\", \"CPU processing frame resolution\", \"string\"),\n",
        "            (\"gpu_frame_resolution\", \"640x480\", \"GPU processing frame resolution\", \"string\"),\n",
        "\n",
        "            # Quality settings\n",
        "            (\"face_quality_threshold\", \"0.3\", \"Minimum face quality score\", \"float\"),\n",
        "            (\"max_face_registrations\", \"10\", \"Maximum face images per employee\", \"int\"),\n",
        "\n",
        "            # Performance settings\n",
        "            (\"max_processing_fps\", \"5\", \"Maximum video processing FPS\", \"int\"),\n",
        "            (\"snapshot_quality\", \"95\", \"JPEG quality for snapshots\", \"int\"),\n",
        "\n",
        "            # Data retention\n",
        "            (\"backup_retention_days\", \"30\", \"Days to retain backup files\", \"int\"),\n",
        "            (\"performance_metrics_retention_days\", \"7\", \"Days to retain performance metrics\", \"int\")\n",
        "        ]\n",
        "\n",
        "        cursor = self.conn.cursor()\n",
        "        for key, value, description, config_type in default_configs:\n",
        "            cursor.execute(\"\"\"\n",
        "            INSERT OR IGNORE INTO system_config (config_key, config_value, description, config_type)\n",
        "            VALUES (?, ?, ?, ?)\n",
        "            \"\"\", (key, value, description, config_type))\n",
        "\n",
        "        self.conn.commit()\n",
        "        print(\"‚öôÔ∏è Pipeline V1 configuration initialized\")\n",
        "\n",
        "    def register_employee(self, employee_data: Dict, face_embeddings: List[np.ndarray],\n",
        "                         registration_metadata: List[Dict] = None) -> Optional[int]:\n",
        "        \"\"\"\n",
        "        Employee registration with multiple face embeddings\n",
        "        Compatible with Pipeline V1 dual-stream processing\n",
        "        \"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        try:\n",
        "            print(f\"üìù Registering employee: {employee_data['name']}\")\n",
        "\n",
        "            # Begin transaction\n",
        "            cursor.execute(\"BEGIN TRANSACTION\")\n",
        "\n",
        "            # Insert employee record\n",
        "            cursor.execute(\"\"\"\n",
        "            INSERT INTO employees (employee_code, name, email, department, position, embedding_count)\n",
        "            VALUES (?, ?, ?, ?, ?, ?)\n",
        "            \"\"\", (\n",
        "                employee_data['employee_code'],\n",
        "                employee_data['name'],\n",
        "                employee_data['email'],\n",
        "                employee_data.get('department', ''),\n",
        "                employee_data.get('position', ''),\n",
        "                len(face_embeddings)\n",
        "            ))\n",
        "\n",
        "            employee_id = cursor.lastrowid\n",
        "\n",
        "            # Store individual face registrations with metadata\n",
        "            registration_qualities = []\n",
        "\n",
        "            for i, embedding in enumerate(face_embeddings):\n",
        "                # Get metadata for this registration\n",
        "                metadata = registration_metadata[i] if registration_metadata and i < len(registration_metadata) else {}\n",
        "\n",
        "                # Calculate quality score\n",
        "                quality_score = self._calculate_embedding_quality(embedding, metadata)\n",
        "                registration_qualities.append(quality_score)\n",
        "\n",
        "                cursor.execute(\"\"\"\n",
        "                INSERT INTO face_registrations\n",
        "                (employee_id, image_path, embedding, quality_score, detection_confidence,\n",
        "                 landmarks, image_metadata, stream_type, processing_time_ms)\n",
        "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                \"\"\", (\n",
        "                    employee_id,\n",
        "                    f\"registration_{employee_id}_{i}.jpg\",\n",
        "                    json.dumps(embedding.tolist()),\n",
        "                    quality_score,\n",
        "                    metadata.get('detection_confidence', 0.0),\n",
        "                    json.dumps(metadata.get('landmarks', []).tolist() if hasattr(metadata.get('landmarks', []), 'tolist') else metadata.get('landmarks', [])),\n",
        "                    json.dumps(metadata.get('image_metadata', {})),\n",
        "                    metadata.get('stream_type', 'gpu'),\n",
        "                    metadata.get('processing_time_ms', 0.0)\n",
        "                ))\n",
        "\n",
        "            # Calculate average embedding and overall quality\n",
        "            if face_embeddings:\n",
        "                avg_embedding = np.mean(face_embeddings, axis=0)\n",
        "                avg_quality = np.mean(registration_qualities)\n",
        "\n",
        "                # Update employee with averaged embedding\n",
        "                cursor.execute(\"\"\"\n",
        "                UPDATE employees\n",
        "                SET face_embeddings = ?, registration_quality = ?, updated_at = CURRENT_TIMESTAMP\n",
        "                WHERE id = ?\n",
        "                \"\"\", (json.dumps(avg_embedding.tolist()), avg_quality, employee_id))\n",
        "\n",
        "            # Commit transaction\n",
        "            cursor.execute(\"COMMIT\")\n",
        "\n",
        "            print(f\"‚úÖ Employee registered successfully: ID {employee_id}\")\n",
        "            print(f\"   ‚îú‚îÄ Face embeddings: {len(face_embeddings)}\")\n",
        "            print(f\"   ‚îî‚îÄ Average quality: {avg_quality:.3f}\")\n",
        "\n",
        "            # Record performance metric\n",
        "            self._record_performance_metric(\"employee_registration\", 1.0, \"dual\", {\"employee_id\": employee_id})\n",
        "\n",
        "            return employee_id\n",
        "\n",
        "        except Exception as e:\n",
        "            cursor.execute(\"ROLLBACK\")\n",
        "            logger.error(f\"‚ùå Employee registration failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def find_employee_by_embedding(self, embedding: np.ndarray, threshold: float = 0.65) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Vector similarity search using cosine distance\n",
        "        Optimized for Pipeline V1 performance requirements\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            cursor = self.conn.cursor()\n",
        "\n",
        "            # Get all active employees with embeddings\n",
        "            cursor.execute(\"\"\"\n",
        "            SELECT id, employee_code, name, email, department, face_embeddings, registration_quality\n",
        "            FROM employees\n",
        "            WHERE is_active = 1 AND face_embeddings IS NOT NULL\n",
        "            \"\"\")\n",
        "\n",
        "            employees = cursor.fetchall()\n",
        "\n",
        "            if not employees:\n",
        "                return None\n",
        "\n",
        "            best_match = None\n",
        "            best_similarity = 0.0\n",
        "\n",
        "            # Calculate similarities with vectorized operations\n",
        "            query_embedding = embedding.reshape(1, -1)\n",
        "\n",
        "            for emp in employees:\n",
        "                try:\n",
        "                    stored_embedding = np.array(json.loads(emp['face_embeddings']))\n",
        "                    stored_embedding = stored_embedding.reshape(1, -1)\n",
        "\n",
        "                    # Calculate cosine similarity\n",
        "                    similarity = cosine_similarity(query_embedding, stored_embedding)[0][0]\n",
        "\n",
        "                    if similarity > best_similarity and similarity > threshold:\n",
        "                        best_similarity = similarity\n",
        "                        best_match = {\n",
        "                            'id': emp['id'],\n",
        "                            'employee_code': emp['employee_code'],\n",
        "                            'name': emp['name'],\n",
        "                            'email': emp['email'],\n",
        "                            'department': emp['department'],\n",
        "                            'similarity': similarity,\n",
        "                            'registration_quality': emp['registration_quality']\n",
        "                        }\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error processing employee {emp['id']}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Record performance metric\n",
        "            search_time = (time.time() - start_time) * 1000\n",
        "            self._record_performance_metric(\"embedding_search_ms\", search_time, \"gpu\", {\n",
        "                \"employees_searched\": len(employees),\n",
        "                \"match_found\": best_match is not None,\n",
        "                \"threshold\": threshold\n",
        "            })\n",
        "\n",
        "            return best_match\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Employee search error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def record_attendance(self, employee_id: int, event_type: str, confidence: float,\n",
        "                         timestamp: str = None, dual_stream_metadata: Dict = None, **kwargs) -> Optional[int]:\n",
        "        \"\"\"\n",
        "        Record attendance with comprehensive dual-stream metadata\n",
        "        Pipeline V1 compatible with business logic\n",
        "        \"\"\"\n",
        "        if timestamp is None:\n",
        "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        try:\n",
        "            cursor = self.conn.cursor()\n",
        "\n",
        "            # Extract dual-stream metadata\n",
        "            metadata = dual_stream_metadata or {}\n",
        "\n",
        "            # Prepare attendance record\n",
        "            cursor.execute(\"\"\"\n",
        "            INSERT INTO attendance_logs\n",
        "            (employee_id, event_type, timestamp, confidence,\n",
        "             detection_stream, recognition_stream, cpu_processing_time_ms, gpu_processing_time_ms,\n",
        "             video_source, frame_number, snapshot_path,\n",
        "             face_bbox, face_quality, face_landmarks,\n",
        "             zone_detected, cooldown_applied, work_hours_valid)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            \"\"\", (\n",
        "                employee_id,\n",
        "                event_type,\n",
        "                timestamp,\n",
        "                confidence,\n",
        "                metadata.get('detection_stream', 'gpu'),\n",
        "                metadata.get('recognition_stream', 'gpu'),\n",
        "                metadata.get('cpu_processing_time_ms', 0.0),\n",
        "                metadata.get('gpu_processing_time_ms', 0.0),\n",
        "                kwargs.get('video_source', ''),\n",
        "                kwargs.get('frame_number', 0),\n",
        "                kwargs.get('snapshot_path', ''),\n",
        "                json.dumps(kwargs.get('face_bbox', []).tolist() if hasattr(kwargs.get('face_bbox', []), 'tolist') else kwargs.get('face_bbox', [])),\n",
        "                kwargs.get('face_quality', 0.0),\n",
        "                json.dumps(kwargs.get('face_landmarks', []).tolist() if hasattr(kwargs.get('face_landmarks', []), 'tolist') else kwargs.get('face_landmarks', [])),\n",
        "                kwargs.get('zone_detected', 'main_entrance'),\n",
        "                kwargs.get('cooldown_applied', False),\n",
        "                kwargs.get('work_hours_valid', True)\n",
        "            ))\n",
        "\n",
        "            attendance_id = cursor.lastrowid\n",
        "            self.conn.commit()\n",
        "\n",
        "            logger.info(f\"üìã Attendance recorded: Employee {employee_id}, {event_type}, ID {attendance_id}\")\n",
        "\n",
        "            # Record performance metric\n",
        "            self._record_performance_metric(\"attendance_recorded\", 1.0, \"dual\", {\n",
        "                \"employee_id\": employee_id,\n",
        "                \"event_type\": event_type,\n",
        "                \"confidence\": confidence\n",
        "            })\n",
        "\n",
        "            return attendance_id\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Attendance recording error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _calculate_embedding_quality(self, embedding: np.ndarray, metadata: Dict = None) -> float:\n",
        "        \"\"\"Calculate quality score for face embedding\"\"\"\n",
        "        try:\n",
        "            # Base quality metrics\n",
        "            norm = np.linalg.norm(embedding)\n",
        "            variance = np.var(embedding)\n",
        "\n",
        "            # Normalize metrics\n",
        "            norm_score = min(norm / 1.0, 1.0)  # Good embeddings have norm around 1\n",
        "            variance_score = min(variance * 10, 1.0)  # Higher variance usually better\n",
        "\n",
        "            base_quality = (norm_score + variance_score) / 2\n",
        "\n",
        "            # Factor in detection confidence if available\n",
        "            if metadata and 'detection_confidence' in metadata:\n",
        "                det_confidence = metadata['detection_confidence']\n",
        "                quality = (base_quality + det_confidence) / 2\n",
        "            else:\n",
        "                quality = base_quality\n",
        "\n",
        "            return min(max(quality, 0.0), 1.0)\n",
        "\n",
        "        except:\n",
        "            return 0.5  # Default quality\n",
        "\n",
        "    def _record_performance_metric(self, metric_type: str, metric_value: float,\n",
        "                                  stream_type: str = None, metadata: Dict = None):\n",
        "        \"\"\"Record performance metric for monitoring\"\"\"\n",
        "        try:\n",
        "            cursor = self.conn.cursor()\n",
        "            cursor.execute(\"\"\"\n",
        "            INSERT INTO performance_metrics (metric_type, metric_value, stream_type, metadata)\n",
        "            VALUES (?, ?, ?, ?)\n",
        "            \"\"\", (metric_type, metric_value, stream_type, json.dumps(metadata) if metadata else None))\n",
        "            self.conn.commit()\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to record performance metric: {e}\")\n",
        "\n",
        "    def get_statistics(self) -> Dict:\n",
        "        \"\"\"Get comprehensive database statistics\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        stats = {}\n",
        "\n",
        "        # Basic counts\n",
        "        cursor.execute(\"SELECT COUNT(*) FROM employees WHERE is_active = 1\")\n",
        "        stats['total_active_employees'] = cursor.fetchone()[0]\n",
        "\n",
        "        cursor.execute(\"SELECT COUNT(*) FROM employees WHERE is_active = 1 AND face_embeddings IS NOT NULL\")\n",
        "        stats['employees_with_faces'] = cursor.fetchone()[0]\n",
        "\n",
        "        cursor.execute(\"SELECT COUNT(*) FROM attendance_logs\")\n",
        "        stats['total_attendance_logs'] = cursor.fetchone()[0]\n",
        "\n",
        "        cursor.execute(\"SELECT COUNT(*) FROM face_registrations\")\n",
        "        stats['total_face_registrations'] = cursor.fetchone()[0]\n",
        "\n",
        "        # Today's statistics\n",
        "        cursor.execute(\"SELECT COUNT(*) FROM attendance_logs WHERE DATE(timestamp) = DATE('now')\")\n",
        "        stats['todays_attendance_count'] = cursor.fetchone()[0]\n",
        "\n",
        "        # Average quality scores\n",
        "        cursor.execute(\"SELECT AVG(registration_quality) FROM employees WHERE registration_quality > 0\")\n",
        "        avg_quality = cursor.fetchone()[0]\n",
        "        stats['avg_registration_quality'] = avg_quality if avg_quality else 0.0\n",
        "\n",
        "        # Dual-stream performance\n",
        "        cursor.execute(\"SELECT COUNT(*) FROM attendance_logs WHERE detection_stream = 'cpu'\")\n",
        "        stats['cpu_detections'] = cursor.fetchone()[0]\n",
        "\n",
        "        cursor.execute(\"SELECT COUNT(*) FROM attendance_logs WHERE recognition_stream = 'gpu'\")\n",
        "        stats['gpu_recognitions'] = cursor.fetchone()[0]\n",
        "\n",
        "        # Recent activity\n",
        "        cursor.execute(\"\"\"\n",
        "        SELECT COUNT(*) FROM attendance_logs\n",
        "        WHERE timestamp >= datetime('now', '-24 hours')\n",
        "        \"\"\")\n",
        "        stats['activity_last_24h'] = cursor.fetchone()[0]\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def get_attendance_logs(self, limit: int = 100) -> List[Dict]:\n",
        "        \"\"\"Get recent attendance logs with employee information\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute(\"\"\"\n",
        "        SELECT al.*, e.name, e.employee_code, e.department\n",
        "        FROM attendance_logs al\n",
        "        JOIN employees e ON al.employee_id = e.id\n",
        "        ORDER BY al.timestamp DESC\n",
        "        LIMIT ?\n",
        "        \"\"\", (limit,))\n",
        "\n",
        "        return [dict(row) for row in cursor.fetchall()]\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close database connection\"\"\"\n",
        "        try:\n",
        "            if hasattr(self, 'conn'):\n",
        "                self.conn.close()\n",
        "            logger.info(\"‚úÖ Database connection closed\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"‚ö†Ô∏è Database close warning: {e}\")\n",
        "\n",
        "# Initialize database when imported\n",
        "if __name__ == \"__main__\":\n",
        "    database = AttendanceDatabaseSQLite()\n",
        "    print(\"‚úÖ Cell 3 completed. Database ready for Pipeline V1 operations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTdlEfCpQYTx",
        "outputId": "1a0784ac-ab0f-443a-871e-951f8afb3506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cell 4 completed. Employee Management System ready.\n",
            "üí° Use with ai_system, database, and config from previous cells.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CELL 4: EMPLOYEE MANAGEMENT SYSTEM\n",
        "- Photo upload and validation\n",
        "- Face detection and embedding extraction\n",
        "- Quality assessment and filtering\n",
        "- Batch registration with progress tracking\n",
        "- Integration with dual-stream AI system\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from typing import List, Dict, Optional\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    from IPython.display import display, HTML\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class EmployeeManager:\n",
        "    \"\"\"\n",
        "    Employee Management System for Pipeline V1\n",
        "    Handles photo upload, face detection, and registration\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ai_system, database, config: Dict):\n",
        "        self.ai_system = ai_system\n",
        "        self.db = database\n",
        "        self.config = config\n",
        "        self.registered_employees = []\n",
        "\n",
        "        print(\"\\nüë• CELL 4: EMPLOYEE MANAGEMENT SYSTEM\")\n",
        "        print(\"=\" * 45)\n",
        "        print(\"‚îú‚îÄ Photo Upload: ZIP files or individual images\")\n",
        "        print(\"‚îú‚îÄ Face Detection: Dual-stream processing\")\n",
        "        print(\"‚îú‚îÄ Quality Assessment: Multi-factor validation\")\n",
        "        print(\"‚îî‚îÄ Batch Registration: Progress tracking\")\n",
        "\n",
        "    def upload_employee_photos(self):\n",
        "        \"\"\"Upload employee photos via Colab file picker\"\"\"\n",
        "        if not COLAB_ENV:\n",
        "            print(\"‚ùå This function requires Google Colab environment\")\n",
        "            return\n",
        "\n",
        "        print(\"üì§ EMPLOYEE PHOTO UPLOAD\")\n",
        "        print(\"=\" * 30)\n",
        "        print(\"üìã Supported formats:\")\n",
        "        print(\"‚îú‚îÄ ZIP archives: employee_name/photo1.jpg, photo2.jpg, ...\")\n",
        "        print(\"‚îú‚îÄ Individual images: employee_name.jpg\")\n",
        "        print(\"‚îî‚îÄ Image formats: JPG, JPEG, PNG\")\n",
        "\n",
        "        try:\n",
        "            uploaded = files.upload()\n",
        "\n",
        "            for filename, content in uploaded.items():\n",
        "                print(f\"\\nüìÅ Processing: {filename}\")\n",
        "\n",
        "                if filename.endswith('.zip'):\n",
        "                    self._process_zip_file(filename, content)\n",
        "                elif filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    self._process_single_image(filename, content)\n",
        "                else:\n",
        "                    print(f\"  ‚ö†Ô∏è Unsupported format: {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Upload error: {e}\")\n",
        "\n",
        "    def _process_zip_file(self, filename: str, content: bytes):\n",
        "        \"\"\"Process ZIP file with employee folders\"\"\"\n",
        "        zip_path = self.config['employees_dir'] / filename\n",
        "\n",
        "        # Save ZIP file\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        # Extract ZIP\n",
        "        extract_dir = self.config['employees_dir'] / 'extracted'\n",
        "        extract_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_dir)\n",
        "\n",
        "            print(f\"  ‚úÖ Extracted to: {extract_dir}\")\n",
        "\n",
        "            # Process extracted folders\n",
        "            self._scan_employee_folders(extract_dir)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå ZIP extraction error: {e}\")\n",
        "        finally:\n",
        "            # Cleanup\n",
        "            if zip_path.exists():\n",
        "                zip_path.unlink()\n",
        "\n",
        "    def _process_single_image(self, filename: str, content: bytes):\n",
        "        \"\"\"Process single image file\"\"\"\n",
        "        # Extract employee name from filename\n",
        "        employee_name = filename.split('.')[0].replace('_', ' ').title()\n",
        "\n",
        "        # Save image\n",
        "        img_path = self.config['employees_dir'] / filename\n",
        "        with open(img_path, 'wb') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        # Process image\n",
        "        try:\n",
        "            image = cv2.imread(str(img_path))\n",
        "            if image is not None:\n",
        "                self._register_single_employee(employee_name, [image], [str(img_path)])\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error processing {filename}: {e}\")\n",
        "\n",
        "    def _scan_employee_folders(self, base_dir: Path):\n",
        "        \"\"\"Scan and process employee folders\"\"\"\n",
        "        employee_folders = [f for f in base_dir.iterdir()\n",
        "                           if f.is_dir() and not f.name.startswith('.')]\n",
        "\n",
        "        if not employee_folders:\n",
        "            print(f\"  ‚ö†Ô∏è No employee folders found\")\n",
        "            return\n",
        "\n",
        "        print(f\"  üìÅ Found {len(employee_folders)} employee folders\")\n",
        "\n",
        "        for folder in tqdm(employee_folders, desc=\"Processing employees\"):\n",
        "            employee_name = folder.name.replace('_', ' ').title()\n",
        "\n",
        "            # Find image files\n",
        "            image_files = []\n",
        "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
        "                image_files.extend(folder.glob(ext))\n",
        "\n",
        "            if not image_files:\n",
        "                print(f\"  ‚ö†Ô∏è No images in {folder.name}\")\n",
        "                continue\n",
        "\n",
        "            # Load images\n",
        "            images = []\n",
        "            image_paths = []\n",
        "            for img_file in image_files:\n",
        "                try:\n",
        "                    img = cv2.imread(str(img_file))\n",
        "                    if img is not None:\n",
        "                        images.append(img)\n",
        "                        image_paths.append(str(img_file))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if images:\n",
        "                self._register_single_employee(employee_name, images, image_paths)\n",
        "\n",
        "    def _register_single_employee(self, employee_name: str, images: List[np.ndarray], image_paths: List[str]):\n",
        "        \"\"\"Register single employee with dual-stream processing\"\"\"\n",
        "        print(f\"\\nüë§ Processing: {employee_name}\")\n",
        "        print(\"-\" * (len(employee_name) + 15))\n",
        "\n",
        "        # Extract face embeddings using dual-stream\n",
        "        face_embeddings = []\n",
        "        registration_metadata = []\n",
        "        processed_count = 0\n",
        "\n",
        "        for i, (image, img_path) in enumerate(zip(images, image_paths)):\n",
        "            try:\n",
        "                print(f\"  üì∏ Image {i+1}/{len(images)}: {Path(img_path).name}\")\n",
        "\n",
        "                # Create low and high quality versions for dual-stream\n",
        "                height, width = image.shape[:2]\n",
        "\n",
        "                # Low quality for CPU detection\n",
        "                low_quality = cv2.resize(image, (320, 240))\n",
        "\n",
        "                # High quality for GPU recognition\n",
        "                if width > 640 or height > 480:\n",
        "                    # Resize maintaining aspect ratio\n",
        "                    scale = min(640/width, 480/height)\n",
        "                    new_width = int(width * scale)\n",
        "                    new_height = int(height * scale)\n",
        "                    high_quality = cv2.resize(image, (new_width, new_height))\n",
        "                else:\n",
        "                    high_quality = image.copy()\n",
        "\n",
        "                # Process with dual-stream\n",
        "                result = self.ai_system.process_dual_stream(low_quality, high_quality)\n",
        "\n",
        "                if result['person_detected'] and result['faces_recognized']:\n",
        "                    face_data = result['faces_recognized'][0]  # Take best face\n",
        "\n",
        "                    # Quality validation\n",
        "                    if face_data['det_score'] > 0.7:  # Good quality threshold\n",
        "                        face_embeddings.append(face_data['embedding'])\n",
        "\n",
        "                        # Collect metadata\n",
        "                        metadata = {\n",
        "                            'detection_confidence': face_data['det_score'],\n",
        "                            'landmarks': face_data.get('landmarks'),\n",
        "                            'stream_type': face_data.get('stream_type', 'gpu'),\n",
        "                            'processing_time_ms': face_data.get('processing_time_ms', 0.0),\n",
        "                            'image_metadata': {\n",
        "                                'original_size': (width, height),\n",
        "                                'processed_size': high_quality.shape[:2],\n",
        "                                'file_path': img_path\n",
        "                            }\n",
        "                        }\n",
        "                        registration_metadata.append(metadata)\n",
        "                        processed_count += 1\n",
        "\n",
        "                        print(f\"    ‚úÖ Confidence: {face_data['det_score']:.3f}, Stream: {face_data.get('stream_type', 'gpu')}\")\n",
        "                    else:\n",
        "                        print(f\"    ‚ö†Ô∏è Low quality: {face_data['det_score']:.3f}\")\n",
        "                elif result['person_detected']:\n",
        "                    print(f\"    ‚ùå Person detected but no face recognized\")\n",
        "                else:\n",
        "                    print(f\"    ‚ùå No person detected\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ùå Processing error: {e}\")\n",
        "\n",
        "        # Register employee if sufficient faces\n",
        "        if len(face_embeddings) >= 1:\n",
        "            # Create employee data\n",
        "            employee_code = employee_name.upper().replace(' ', '_')\n",
        "            employee_data = {\n",
        "                'employee_code': employee_code,\n",
        "                'name': employee_name,\n",
        "                'email': f\"{employee_code.lower()}@company.com\",\n",
        "                'department': 'Demo Department',\n",
        "                'position': 'Employee'\n",
        "            }\n",
        "\n",
        "            # Register in database\n",
        "            employee_id = self.db.register_employee(\n",
        "                employee_data,\n",
        "                face_embeddings,\n",
        "                registration_metadata\n",
        "            )\n",
        "\n",
        "            if employee_id:\n",
        "                avg_quality = np.mean([m.get('detection_confidence', 0.0) for m in registration_metadata])\n",
        "                self.registered_employees.append({\n",
        "                    'id': employee_id,\n",
        "                    'name': employee_name,\n",
        "                    'face_count': len(face_embeddings),\n",
        "                    'avg_quality': avg_quality,\n",
        "                    'processing_method': 'dual_stream'\n",
        "                })\n",
        "\n",
        "                print(f\"  üéâ REGISTERED SUCCESSFULLY!\")\n",
        "                print(f\"    ‚îú‚îÄ Employee ID: {employee_id}\")\n",
        "                print(f\"    ‚îú‚îÄ Face embeddings: {len(face_embeddings)}\")\n",
        "                print(f\"    ‚îú‚îÄ Average quality: {avg_quality:.3f}\")\n",
        "                print(f\"    ‚îî‚îÄ Processing: Dual-stream\")\n",
        "            else:\n",
        "                print(f\"  ‚ùå Database registration failed\")\n",
        "        else:\n",
        "            print(f\"  ‚ùå Insufficient quality faces (need at least 1)\")\n",
        "            print(f\"    ‚îî‚îÄ Try uploading clearer, well-lit photos\")\n",
        "\n",
        "    def show_registered_employees(self):\n",
        "        \"\"\"Display registered employees with comprehensive information\"\"\"\n",
        "        print(\"\\nüìã REGISTERED EMPLOYEES\")\n",
        "        print(\"=\" * 30)\n",
        "\n",
        "        # Get all employees from database\n",
        "        cursor = self.db.conn.cursor()\n",
        "        cursor.execute(\"\"\"\n",
        "        SELECT id, employee_code, name, email, department, position,\n",
        "               embedding_count, registration_quality, created_at\n",
        "        FROM employees WHERE is_active = 1\n",
        "        ORDER BY created_at DESC\n",
        "        \"\"\")\n",
        "\n",
        "        employees = [dict(row) for row in cursor.fetchall()]\n",
        "\n",
        "        if not employees:\n",
        "            print(\"üìù No employees registered yet\")\n",
        "            print(\"üí° Use employee_manager.upload_employee_photos() to register employees\")\n",
        "            return\n",
        "\n",
        "        # Display in table format\n",
        "        print(f\"Total Employees: {len(employees)}\")\n",
        "        print()\n",
        "\n",
        "        # Create DataFrame for better display\n",
        "        df_data = []\n",
        "        for emp in employees:\n",
        "            df_data.append({\n",
        "                'ID': emp['id'],\n",
        "                'Name': emp['name'],\n",
        "                'Code': emp['employee_code'],\n",
        "                'Department': emp['department'],\n",
        "                'Faces': emp['embedding_count'],\n",
        "                'Quality': f\"{emp['registration_quality']:.3f}\",\n",
        "                'Registered': emp['created_at'][:19]  # Remove microseconds\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(df_data)\n",
        "\n",
        "        if COLAB_ENV:\n",
        "            display(HTML(df.to_html(index=False)))\n",
        "        else:\n",
        "            print(df.to_string(index=False))\n",
        "\n",
        "        # Show summary statistics\n",
        "        total_faces = sum(emp['embedding_count'] for emp in employees)\n",
        "        avg_quality = np.mean([emp['registration_quality'] for emp in employees if emp['registration_quality'] > 0])\n",
        "\n",
        "        print(f\"\\nüìä SUMMARY:\")\n",
        "        print(f\"‚îú‚îÄ Total Employees: {len(employees)}\")\n",
        "        print(f\"‚îú‚îÄ Total Face Images: {total_faces}\")\n",
        "        print(f\"‚îú‚îÄ Average Quality: {avg_quality:.3f}\")\n",
        "        print(f\"‚îî‚îÄ Registration Method: Dual-stream processing\")\n",
        "\n",
        "    def get_employee_statistics(self) -> Dict:\n",
        "        \"\"\"Get detailed employee statistics\"\"\"\n",
        "        return self.db.get_statistics()\n",
        "\n",
        "    def create_sample_employee_data(self):\n",
        "        \"\"\"Create sample employee data for testing\"\"\"\n",
        "        print(\"\\nüß™ CREATING SAMPLE EMPLOYEE DATA\")\n",
        "        print(\"=\" * 35)\n",
        "\n",
        "        sample_employees = [\n",
        "            {\n",
        "                'employee_code': 'EMP001',\n",
        "                'name': 'John Smith',\n",
        "                'email': 'john.smith@company.com',\n",
        "                'department': 'Engineering',\n",
        "                'position': 'Senior Developer'\n",
        "            },\n",
        "            {\n",
        "                'employee_code': 'EMP002',\n",
        "                'name': 'Sarah Johnson',\n",
        "                'email': 'sarah.johnson@company.com',\n",
        "                'department': 'Marketing',\n",
        "                'position': 'Marketing Manager'\n",
        "            },\n",
        "            {\n",
        "                'employee_code': 'EMP003',\n",
        "                'name': 'Mike Wilson',\n",
        "                'email': 'mike.wilson@company.com',\n",
        "                'department': 'Sales',\n",
        "                'position': 'Sales Representative'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        import random\n",
        "\n",
        "        for emp_data in sample_employees:\n",
        "            # Generate random face embeddings (simulating real photos)\n",
        "            embeddings = []\n",
        "            metadata = []\n",
        "\n",
        "            for i in range(random.randint(3, 5)):  # 3-5 face images per employee\n",
        "                # Generate normalized random embedding\n",
        "                embedding = np.random.normal(0, 1, 512)\n",
        "                embedding = embedding / np.linalg.norm(embedding)\n",
        "                embeddings.append(embedding)\n",
        "\n",
        "                # Create realistic metadata\n",
        "                meta = {\n",
        "                    'detection_confidence': random.uniform(0.75, 0.95),\n",
        "                    'landmarks': np.random.rand(5, 2).tolist(),\n",
        "                    'stream_type': 'gpu',\n",
        "                    'processing_time_ms': random.uniform(50, 150),\n",
        "                    'image_metadata': {\n",
        "                        'original_size': (1024, 768),\n",
        "                        'processed_size': (640, 480),\n",
        "                        'file_path': f\"sample_{emp_data['employee_code']}_{i}.jpg\"\n",
        "                    }\n",
        "                }\n",
        "                metadata.append(meta)\n",
        "\n",
        "            # Register employee\n",
        "            employee_id = self.db.register_employee(emp_data, embeddings, metadata)\n",
        "            if employee_id:\n",
        "                print(f\"‚úÖ {emp_data['name']}: ID {employee_id}, {len(embeddings)} embeddings\")\n",
        "\n",
        "        print(f\"\\nüéâ Sample employee data created!\")\n",
        "\n",
        "# Initialize when imported\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"‚úÖ Cell 4 completed. Employee Management System ready.\")\n",
        "    print(\"üí° Use with ai_system, database, and config from previous cells.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kScK7odRQeRb",
        "outputId": "018824ee-b981-4a94-fec7-0bb6c519a434"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cell 5 completed. Video Processing System ready.\n",
            "üí° Use with ai_system, database, and config from previous cells.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CELL 5: VIDEO PROCESSING SYSTEM\n",
        "- Upload and process video files with dual-stream processing\n",
        "- Frame-by-frame attendance tracking with business logic\n",
        "- Real-time analytics and progress tracking\n",
        "- Interactive video upload with progress visualization\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional, Tuple, Iterator\n",
        "from pathlib import Path\n",
        "import json\n",
        "from collections import defaultdict, deque\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class VideoProcessor:\n",
        "    \"\"\"\n",
        "    Video Processing System for Pipeline V1\n",
        "    Handles video upload, dual-stream processing, and attendance tracking\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ai_system, database, config: Dict):\n",
        "        self.ai_system = ai_system\n",
        "        self.db = database\n",
        "        self.config = config\n",
        "\n",
        "        # Business logic configuration\n",
        "        self.business_config = {\n",
        "            'cooldown_minutes': 30,\n",
        "            'work_hours_start': '07:00',\n",
        "            'work_hours_end': '19:00',\n",
        "            'recognition_threshold': 0.65,\n",
        "            'processing_fps': 5,  # Process 5 frames per second\n",
        "            'min_confidence': 0.7\n",
        "        }\n",
        "\n",
        "        # Processing statistics\n",
        "        self.processing_stats = {\n",
        "            'total_frames': 0,\n",
        "            'frames_processed': 0,\n",
        "            'persons_detected': 0,\n",
        "            'faces_recognized': 0,\n",
        "            'attendance_recorded': 0,\n",
        "            'processing_times': deque(maxlen=100),\n",
        "            'recognition_history': [],\n",
        "            'session_start': time.time()\n",
        "        }\n",
        "\n",
        "        # Attendance events for this session\n",
        "        self.attendance_events = []\n",
        "\n",
        "        # Employee cooldown tracking (employee_id -> last_seen_time)\n",
        "        self.employee_cooldowns = {}\n",
        "\n",
        "        print(\"\\nüé• CELL 5: VIDEO PROCESSING SYSTEM\")\n",
        "        print(\"=\" * 40)\n",
        "        print(\"‚îú‚îÄ Dual-Stream Processing: CPU detection + GPU recognition\")\n",
        "        print(\"‚îú‚îÄ Business Logic: 30min cooldown, work hours validation\")\n",
        "        print(\"‚îú‚îÄ Real-time Analytics: Frame-by-frame tracking\")\n",
        "        print(\"‚îî‚îÄ Interactive Upload: Progress visualization\")\n",
        "\n",
        "    def upload_and_process_video(self):\n",
        "        \"\"\"Upload and process video file with Colab integration\"\"\"\n",
        "        if not COLAB_ENV:\n",
        "            print(\"‚ùå This function requires Google Colab environment\")\n",
        "            return\n",
        "\n",
        "        print(\"üì§ VIDEO UPLOAD & PROCESSING\")\n",
        "        print(\"=\" * 35)\n",
        "        print(\"üìã Supported formats: MP4, AVI, MOV, MKV\")\n",
        "        print(\"üí° Recommended: 30fps, 1080p or lower for optimal processing\")\n",
        "\n",
        "        try:\n",
        "            uploaded = files.upload()\n",
        "\n",
        "            for filename, content in uploaded.items():\n",
        "                print(f\"\\nüé¨ Processing video: {filename}\")\n",
        "\n",
        "                # Save uploaded file\n",
        "                video_path = self.config['videos_dir'] / filename\n",
        "                with open(video_path, 'wb') as f:\n",
        "                    f.write(content)\n",
        "\n",
        "                # Process video\n",
        "                self.process_video_file(video_path, filename)\n",
        "\n",
        "                # Cleanup\n",
        "                if video_path.exists():\n",
        "                    video_path.unlink()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Video upload error: {e}\")\n",
        "\n",
        "    def process_video_file(self, video_path: Path, video_name: str):\n",
        "        \"\"\"Process video file with dual-stream Pipeline V1\"\"\"\n",
        "        print(f\"\\nüìä VIDEO ANALYSIS: {video_name}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Reset processing stats for this video\n",
        "        self._reset_processing_stats()\n",
        "\n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            print(\"‚ùå Cannot open video file\")\n",
        "            return\n",
        "\n",
        "        # Video properties\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        duration = total_frames / fps if fps > 0 else 0\n",
        "\n",
        "        print(f\"üìπ Video Properties:\")\n",
        "        print(f\"‚îú‚îÄ Total Frames: {total_frames:,}\")\n",
        "        print(f\"‚îú‚îÄ FPS: {fps:.1f}\")\n",
        "        print(f\"‚îú‚îÄ Duration: {duration:.1f} seconds\")\n",
        "        print(f\"‚îî‚îÄ Processing Rate: {self.business_config['processing_fps']} FPS\")\n",
        "\n",
        "        # Calculate processing interval\n",
        "        frame_interval = max(1, int(fps // self.business_config['processing_fps']))\n",
        "        frames_to_process = total_frames // frame_interval\n",
        "\n",
        "        print(f\"\\nüîÑ Starting dual-stream processing...\")\n",
        "        print(f\"‚îú‚îÄ Frame interval: Every {frame_interval} frames\")\n",
        "        print(f\"‚îî‚îÄ Estimated frames to process: {frames_to_process:,}\")\n",
        "\n",
        "        # Process frames with progress tracking\n",
        "        frame_count = 0\n",
        "        processed_count = 0\n",
        "\n",
        "        with tqdm(total=frames_to_process, desc=\"Processing video\") as pbar:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                frame_count += 1\n",
        "                self.processing_stats['total_frames'] = frame_count\n",
        "\n",
        "                # Process only at specified intervals\n",
        "                if frame_count % frame_interval == 0:\n",
        "                    processed_count += 1\n",
        "\n",
        "                    # Calculate timestamp in video\n",
        "                    video_timestamp = frame_count / fps\n",
        "\n",
        "                    # Process frame with Pipeline V1\n",
        "                    results = self._process_frame_pipeline_v1(\n",
        "                        frame, frame_count, video_name, video_timestamp\n",
        "                    )\n",
        "\n",
        "                    # Update progress\n",
        "                    pbar.update(1)\n",
        "                    pbar.set_postfix({\n",
        "                        'Detected': self.processing_stats['persons_detected'],\n",
        "                        'Recognized': self.processing_stats['faces_recognized'],\n",
        "                        'Attendance': self.processing_stats['attendance_recorded']\n",
        "                    })\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Generate comprehensive analytics\n",
        "        self._generate_video_analytics(video_name, duration, fps)\n",
        "\n",
        "    def _process_frame_pipeline_v1(self, frame: np.ndarray, frame_number: int,\n",
        "                                  video_name: str, video_timestamp: float) -> Dict:\n",
        "        \"\"\"Process single frame using Pipeline V1 dual-stream architecture\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        result = {\n",
        "            'frame_number': frame_number,\n",
        "            'video_timestamp': video_timestamp,\n",
        "            'person_detected': False,\n",
        "            'faces_recognized': [],\n",
        "            'attendance_events': [],\n",
        "            'processing_time_ms': 0.0\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Step 1: Create dual-stream frames\n",
        "            height, width = frame.shape[:2]\n",
        "\n",
        "            # Low quality for CPU person detection (Pipeline V1)\n",
        "            low_quality_frame = cv2.resize(frame, (320, 240))\n",
        "\n",
        "            # High quality for GPU face recognition\n",
        "            if width > 640 or height > 480:\n",
        "                scale = min(640/width, 480/height)\n",
        "                new_width = int(width * scale)\n",
        "                new_height = int(height * scale)\n",
        "                high_quality_frame = cv2.resize(frame, (new_width, new_height))\n",
        "            else:\n",
        "                high_quality_frame = frame.copy()\n",
        "\n",
        "            # Step 2: Dual-stream processing\n",
        "            dual_stream_result = self.ai_system.process_dual_stream(\n",
        "                low_quality_frame, high_quality_frame\n",
        "            )\n",
        "\n",
        "            result['person_detected'] = dual_stream_result['person_detected']\n",
        "\n",
        "            if dual_stream_result['person_detected']:\n",
        "                self.processing_stats['persons_detected'] += 1\n",
        "\n",
        "                # Step 3: Process recognized faces\n",
        "                for face_data in dual_stream_result['faces_recognized']:\n",
        "                    # Try to recognize employee\n",
        "                    employee = self.db.find_employee_by_embedding(\n",
        "                        face_data['embedding'],\n",
        "                        self.business_config['recognition_threshold']\n",
        "                    )\n",
        "\n",
        "                    if employee and face_data['det_score'] >= self.business_config['min_confidence']:\n",
        "                        self.processing_stats['faces_recognized'] += 1\n",
        "\n",
        "                        # Step 4: Apply business logic\n",
        "                        attendance_decision = self._apply_business_logic(\n",
        "                            employee['id'], frame_number, video_name, video_timestamp\n",
        "                        )\n",
        "\n",
        "                        face_result = {\n",
        "                            'employee_id': employee['id'],\n",
        "                            'employee_name': employee['name'],\n",
        "                            'employee_code': employee['employee_code'],\n",
        "                            'similarity': employee['similarity'],\n",
        "                            'detection_confidence': face_data['det_score'],\n",
        "                            'business_decision': attendance_decision\n",
        "                        }\n",
        "                        result['faces_recognized'].append(face_result)\n",
        "\n",
        "                        # Step 5: Record attendance if approved\n",
        "                        if attendance_decision['should_record']:\n",
        "                            attendance_event = self._record_video_attendance(\n",
        "                                employee, face_data, frame_number, video_name,\n",
        "                                video_timestamp, attendance_decision\n",
        "                            )\n",
        "                            if attendance_event:\n",
        "                                result['attendance_events'].append(attendance_event)\n",
        "                                self.attendance_events.append(attendance_event)\n",
        "                                self.processing_stats['attendance_recorded'] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Frame processing error: {e}\")\n",
        "\n",
        "        # Update processing statistics\n",
        "        processing_time = (time.time() - start_time) * 1000\n",
        "        result['processing_time_ms'] = processing_time\n",
        "        self.processing_stats['processing_times'].append(processing_time)\n",
        "        self.processing_stats['frames_processed'] += 1\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _apply_business_logic(self, employee_id: int, frame_number: int,\n",
        "                            video_name: str, video_timestamp: float) -> Dict:\n",
        "        \"\"\"Apply Pipeline V1 business logic for attendance decisions\"\"\"\n",
        "\n",
        "        decision = {\n",
        "            'should_record': False,\n",
        "            'reason': '',\n",
        "            'event_type': None,\n",
        "            'checks': {\n",
        "                'work_hours': False,\n",
        "                'cooldown': False,\n",
        "                'existing_records': False\n",
        "            }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Get current time (simulated from video timestamp)\n",
        "            # In real implementation, this would be actual current time\n",
        "            current_time = datetime.now()\n",
        "\n",
        "            # 1. Work hours check\n",
        "            work_start = datetime.strptime(self.business_config['work_hours_start'], '%H:%M').time()\n",
        "            work_end = datetime.strptime(self.business_config['work_hours_end'], '%H:%M').time()\n",
        "            current_time_only = current_time.time()\n",
        "\n",
        "            in_work_hours = work_start <= current_time_only <= work_end\n",
        "            decision['checks']['work_hours'] = in_work_hours\n",
        "\n",
        "            if not in_work_hours:\n",
        "                decision['reason'] = f'Outside work hours ({self.business_config[\"work_hours_start\"]}-{self.business_config[\"work_hours_end\"]})'\n",
        "                return decision\n",
        "\n",
        "            # 2. Cooldown check (using session-based tracking for video processing)\n",
        "            current_timestamp = time.time()\n",
        "            last_seen = self.employee_cooldowns.get(employee_id, 0)\n",
        "            time_since_last = (current_timestamp - last_seen) / 60  # minutes\n",
        "\n",
        "            cooldown_ok = time_since_last >= self.business_config['cooldown_minutes']\n",
        "            decision['checks']['cooldown'] = cooldown_ok\n",
        "\n",
        "            if not cooldown_ok:\n",
        "                remaining_minutes = self.business_config['cooldown_minutes'] - time_since_last\n",
        "                decision['reason'] = f'Cooldown period: {remaining_minutes:.1f} minutes remaining'\n",
        "                return decision\n",
        "\n",
        "            # 3. Check existing records for event type determination\n",
        "            today_records = self.db.get_today_records(employee_id)\n",
        "            decision['checks']['existing_records'] = True\n",
        "\n",
        "            # Determine event type\n",
        "            if not today_records:\n",
        "                event_type = 'check_in'\n",
        "            else:\n",
        "                last_event = today_records[-1]['event_type']\n",
        "                event_type = 'check_out' if last_event == 'check_in' else 'check_in'\n",
        "\n",
        "            # All checks passed\n",
        "            decision['should_record'] = True\n",
        "            decision['event_type'] = event_type\n",
        "            decision['reason'] = f'Approved for {event_type}'\n",
        "\n",
        "            # Update cooldown tracking\n",
        "            self.employee_cooldowns[employee_id] = current_timestamp\n",
        "\n",
        "        except Exception as e:\n",
        "            decision['reason'] = f'Business logic error: {str(e)}'\n",
        "            logger.error(f\"Business logic error: {e}\")\n",
        "\n",
        "        return decision\n",
        "\n",
        "    def _record_video_attendance(self, employee: Dict, face_data: Dict,\n",
        "                               frame_number: int, video_name: str,\n",
        "                               video_timestamp: float, business_decision: Dict) -> Optional[Dict]:\n",
        "        \"\"\"Record attendance event with comprehensive metadata\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Prepare dual-stream metadata\n",
        "            dual_stream_metadata = {\n",
        "                'detection_stream': 'cpu',\n",
        "                'recognition_stream': face_data.get('stream_type', 'gpu'),\n",
        "                'cpu_processing_time_ms': 0.0,  # From dual-stream result\n",
        "                'gpu_processing_time_ms': face_data.get('processing_time_ms', 0.0)\n",
        "            }\n",
        "\n",
        "            # Record attendance in database\n",
        "            attendance_id = self.db.record_attendance(\n",
        "                employee_id=employee['id'],\n",
        "                event_type=business_decision['event_type'],\n",
        "                confidence=employee['similarity'],\n",
        "                dual_stream_metadata=dual_stream_metadata,\n",
        "                video_source=video_name,\n",
        "                frame_number=frame_number,\n",
        "                face_bbox=face_data.get('bbox', []),\n",
        "                face_quality=face_data.get('det_score', 0.0),\n",
        "                face_landmarks=face_data.get('landmarks', []),\n",
        "                zone_detected='video_processing',\n",
        "                cooldown_applied=False,\n",
        "                work_hours_valid=business_decision['checks']['work_hours']\n",
        "            )\n",
        "\n",
        "            if attendance_id:\n",
        "                attendance_event = {\n",
        "                    'attendance_id': attendance_id,\n",
        "                    'employee_id': employee['id'],\n",
        "                    'employee_name': employee['name'],\n",
        "                    'employee_code': employee['employee_code'],\n",
        "                    'event_type': business_decision['event_type'],\n",
        "                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'confidence': employee['similarity'],\n",
        "                    'frame_number': frame_number,\n",
        "                    'video_timestamp': video_timestamp,\n",
        "                    'video_source': video_name\n",
        "                }\n",
        "\n",
        "                print(f\"‚úÖ Frame {frame_number}: {employee['name']} - {business_decision['event_type']} ({employee['similarity']:.3f})\")\n",
        "\n",
        "                return attendance_event\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to record attendance: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _generate_video_analytics(self, video_name: str, duration: float, fps: float):\n",
        "        \"\"\"Generate comprehensive video processing analytics\"\"\"\n",
        "        print(f\"\\nüìä VIDEO PROCESSING ANALYTICS\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        # Processing statistics\n",
        "        avg_processing_time = np.mean(self.processing_stats['processing_times']) if self.processing_stats['processing_times'] else 0\n",
        "        total_processing_time = sum(self.processing_stats['processing_times']) / 1000  # seconds\n",
        "\n",
        "        print(f\"üìà Processing Statistics:\")\n",
        "        print(f\"‚îú‚îÄ Total Frames: {self.processing_stats['total_frames']:,}\")\n",
        "        print(f\"‚îú‚îÄ Frames Processed: {self.processing_stats['frames_processed']:,}\")\n",
        "        print(f\"‚îú‚îÄ Persons Detected: {self.processing_stats['persons_detected']:,}\")\n",
        "        print(f\"‚îú‚îÄ Faces Recognized: {self.processing_stats['faces_recognized']:,}\")\n",
        "        print(f\"‚îú‚îÄ Attendance Recorded: {self.processing_stats['attendance_recorded']:,}\")\n",
        "        print(f\"‚îú‚îÄ Avg Processing Time: {avg_processing_time:.1f}ms/frame\")\n",
        "        print(f\"‚îú‚îÄ Total Processing Time: {total_processing_time:.1f}s\")\n",
        "        print(f\"‚îî‚îÄ Processing Efficiency: {self.processing_stats['frames_processed']/self.processing_stats['total_frames']*100:.1f}%\")\n",
        "\n",
        "        # Attendance summary\n",
        "        if self.attendance_events:\n",
        "            self._show_attendance_summary()\n",
        "            if COLAB_ENV:\n",
        "                self._plot_video_analytics(video_name, duration)\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è No attendance events recorded\")\n",
        "            print(\"üí° Possible reasons:\")\n",
        "            print(\"   ‚îú‚îÄ No employees registered\")\n",
        "            print(\"   ‚îú‚îÄ Low video quality\")\n",
        "            print(\"   ‚îú‚îÄ Recognition threshold too high\")\n",
        "            print(\"   ‚îî‚îÄ Business logic restrictions\")\n",
        "\n",
        "    def _show_attendance_summary(self):\n",
        "        \"\"\"Show attendance summary table\"\"\"\n",
        "        print(f\"\\nüë• ATTENDANCE SUMMARY\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Group by employee\n",
        "        employee_summary = defaultdict(list)\n",
        "        for event in self.attendance_events:\n",
        "            employee_summary[event['employee_name']].append(event)\n",
        "\n",
        "        summary_data = []\n",
        "        for employee_name, events in employee_summary.items():\n",
        "            first_event = min(events, key=lambda x: x['frame_number'])\n",
        "            last_event = max(events, key=lambda x: x['frame_number'])\n",
        "            avg_confidence = np.mean([e['confidence'] for e in events])\n",
        "\n",
        "            summary_data.append({\n",
        "                'Employee': employee_name,\n",
        "                'First Detection': f\"Frame {first_event['frame_number']} ({first_event['event_type']})\",\n",
        "                'Last Detection': f\"Frame {last_event['frame_number']} ({last_event['event_type']})\",\n",
        "                'Total Events': len(events),\n",
        "                'Avg Confidence': f\"{avg_confidence:.3f}\"\n",
        "            })\n",
        "\n",
        "        # Display as DataFrame\n",
        "        df = pd.DataFrame(summary_data)\n",
        "        if COLAB_ENV:\n",
        "            display(HTML(df.to_html(index=False)))\n",
        "        else:\n",
        "            print(df.to_string(index=False))\n",
        "\n",
        "    def _plot_video_analytics(self, video_name: str, duration: float):\n",
        "        \"\"\"Plot interactive video processing analytics\"\"\"\n",
        "        if not COLAB_ENV:\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüìà INTERACTIVE ANALYTICS\")\n",
        "\n",
        "        # Create timeline plot\n",
        "        fig = make_subplots(\n",
        "            rows=3, cols=1,\n",
        "            subplot_titles=(\n",
        "                'Attendance Timeline',\n",
        "                'Processing Performance',\n",
        "                'Recognition Confidence Distribution'\n",
        "            ),\n",
        "            vertical_spacing=0.1\n",
        "        )\n",
        "\n",
        "        # 1. Attendance Timeline\n",
        "        if self.attendance_events:\n",
        "            for employee_name in set(e['employee_name'] for e in self.attendance_events):\n",
        "                employee_events = [e for e in self.attendance_events if e['employee_name'] == employee_name]\n",
        "                frame_numbers = [e['frame_number'] for e in employee_events]\n",
        "                video_timestamps = [e['video_timestamp'] for e in employee_events]\n",
        "                confidences = [e['confidence'] for e in employee_events]\n",
        "\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=video_timestamps,\n",
        "                        y=confidences,\n",
        "                        mode='markers+lines',\n",
        "                        name=employee_name,\n",
        "                        text=[f\"Frame {f}\" for f in frame_numbers],\n",
        "                        hovertemplate='<b>%{fullData.name}</b><br>Time: %{x:.1f}s<br>Confidence: %{y:.3f}<br>%{text}<extra></extra>'\n",
        "                    ),\n",
        "                    row=1, col=1\n",
        "                )\n",
        "\n",
        "        # 2. Processing Performance\n",
        "        if self.processing_stats['processing_times']:\n",
        "            frame_indices = list(range(len(self.processing_stats['processing_times'])))\n",
        "            processing_times = list(self.processing_stats['processing_times'])\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=frame_indices,\n",
        "                    y=processing_times,\n",
        "                    mode='lines',\n",
        "                    name='Processing Time',\n",
        "                    line=dict(color='orange'),\n",
        "                    showlegend=False\n",
        "                ),\n",
        "                row=2, col=1\n",
        "            )\n",
        "\n",
        "        # 3. Confidence Distribution\n",
        "        if self.attendance_events:\n",
        "            confidences = [e['confidence'] for e in self.attendance_events]\n",
        "            fig.add_trace(\n",
        "                go.Histogram(\n",
        "                    x=confidences,\n",
        "                    nbinsx=20,\n",
        "                    name='Confidence Distribution',\n",
        "                    marker_color='lightblue',\n",
        "                    showlegend=False\n",
        "                ),\n",
        "                row=3, col=1\n",
        "            )\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            height=800,\n",
        "            title_text=f\"Video Processing Analytics: {video_name}\",\n",
        "            showlegend=True\n",
        "        )\n",
        "\n",
        "        fig.update_xaxes(title_text=\"Video Time (seconds)\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Recognition Confidence\", row=1, col=1)\n",
        "        fig.update_xaxes(title_text=\"Processed Frame Index\", row=2, col=1)\n",
        "        fig.update_yaxes(title_text=\"Processing Time (ms)\", row=2, col=1)\n",
        "        fig.update_xaxes(title_text=\"Confidence Score\", row=3, col=1)\n",
        "        fig.update_yaxes(title_text=\"Frequency\", row=3, col=1)\n",
        "\n",
        "        fig.show()\n",
        "\n",
        "    def _reset_processing_stats(self):\n",
        "        \"\"\"Reset processing statistics for new video\"\"\"\n",
        "        self.processing_stats = {\n",
        "            'total_frames': 0,\n",
        "            'frames_processed': 0,\n",
        "            'persons_detected': 0,\n",
        "            'faces_recognized': 0,\n",
        "            'attendance_recorded': 0,\n",
        "            'processing_times': deque(maxlen=100),\n",
        "            'recognition_history': [],\n",
        "            'session_start': time.time()\n",
        "        }\n",
        "        self.attendance_events = []\n",
        "        self.employee_cooldowns = {}\n",
        "\n",
        "    def get_processing_summary(self) -> Dict:\n",
        "        \"\"\"Get comprehensive processing summary\"\"\"\n",
        "        return {\n",
        "            'processing_stats': dict(self.processing_stats),\n",
        "            'attendance_events': self.attendance_events,\n",
        "            'business_config': self.business_config,\n",
        "            'employee_cooldowns': self.employee_cooldowns\n",
        "        }\n",
        "\n",
        "# Initialize when imported\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"‚úÖ Cell 5 completed. Video Processing System ready.\")\n",
        "    print(\"üí° Use with ai_system, database, and config from previous cells.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNlbp1tkQgU-",
        "outputId": "6cbb2eb2-5eb4-4c4b-cbbb-e2e8a2b8f2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cell 6 completed. Analytics Dashboard & Reporting ready.\n",
            "üí° Use with ai_system, database, video_processor, and config from previous cells.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CELL 6: ANALYTICS DASHBOARD & REPORTING\n",
        "- Real-time system performance monitoring\n",
        "- Comprehensive attendance analytics\n",
        "- Interactive dashboards with Plotly\n",
        "- Multi-format export functionality (CSV, Excel, JSON)\n",
        "- System health monitoring and optimization\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Optional\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    import plotly.graph_objects as go\n",
        "    import plotly.express as px\n",
        "    from plotly.subplots import make_subplots\n",
        "    import ipywidgets as widgets\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AnalyticsDashboard:\n",
        "    \"\"\"\n",
        "    Analytics Dashboard & Reporting System for Pipeline V1\n",
        "    Comprehensive monitoring and reporting capabilities\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ai_system, database, video_processor, config: Dict):\n",
        "        self.ai_system = ai_system\n",
        "        self.db = database\n",
        "        self.video_processor = video_processor\n",
        "        self.config = config\n",
        "\n",
        "        # Dashboard state\n",
        "        self.dashboard_data = {\n",
        "            'last_update': datetime.now(),\n",
        "            'system_metrics': [],\n",
        "            'performance_history': []\n",
        "        }\n",
        "\n",
        "        print(\"\\nüìä CELL 6: ANALYTICS DASHBOARD & REPORTING\")\n",
        "        print(\"=\" * 45)\n",
        "        print(\"‚îú‚îÄ Real-time Performance Monitoring\")\n",
        "        print(\"‚îú‚îÄ Attendance Analytics & Insights\")\n",
        "        print(\"‚îú‚îÄ Interactive Plotly Dashboards\")\n",
        "        print(\"‚îú‚îÄ Multi-format Export (CSV, Excel, JSON)\")\n",
        "        print(\"‚îî‚îÄ System Health & Optimization\")\n",
        "\n",
        "    def show_system_dashboard(self):\n",
        "        \"\"\"Display comprehensive real-time system dashboard\"\"\"\n",
        "        print(\"üöÄ SYSTEM PERFORMANCE DASHBOARD\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        # Collect current metrics\n",
        "        system_metrics = self._collect_system_metrics()\n",
        "\n",
        "        # Show system status\n",
        "        self._show_system_status(system_metrics)\n",
        "\n",
        "        # Show AI performance\n",
        "        self._show_ai_performance()\n",
        "\n",
        "        # Show database analytics\n",
        "        self._show_database_analytics()\n",
        "\n",
        "        # Interactive performance visualization\n",
        "        if COLAB_ENV:\n",
        "            self._create_performance_dashboard()\n",
        "\n",
        "    def _collect_system_metrics(self) -> Dict:\n",
        "        \"\"\"Collect comprehensive system metrics\"\"\"\n",
        "        metrics = {\n",
        "            'timestamp': datetime.now(),\n",
        "            'system_info': {},\n",
        "            'ai_performance': {},\n",
        "            'database_stats': {},\n",
        "            'processing_stats': {}\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # System information\n",
        "            import psutil\n",
        "            import torch\n",
        "\n",
        "            metrics['system_info'] = {\n",
        "                'cpu_usage': psutil.cpu_percent(),\n",
        "                'memory_usage': psutil.virtual_memory().percent,\n",
        "                'memory_available_gb': psutil.virtual_memory().available / (1024**3),\n",
        "                'gpu_available': torch.cuda.is_available()\n",
        "            }\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                metrics['system_info'].update({\n",
        "                    'gpu_memory_used_mb': torch.cuda.memory_allocated() / (1024**2),\n",
        "                    'gpu_memory_total_mb': torch.cuda.get_device_properties(0).total_memory / (1024**2),\n",
        "                    'gpu_name': torch.cuda.get_device_name(0)\n",
        "                })\n",
        "\n",
        "            # AI Performance\n",
        "            metrics['ai_performance'] = self.ai_system.get_performance_stats()\n",
        "\n",
        "            # Database statistics\n",
        "            metrics['database_stats'] = self.db.get_statistics()\n",
        "\n",
        "            # Video processing statistics\n",
        "            if hasattr(self.video_processor, 'get_processing_summary'):\n",
        "                metrics['processing_stats'] = self.video_processor.get_processing_summary()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error collecting system metrics: {e}\")\n",
        "\n",
        "        # Store metrics history\n",
        "        self.dashboard_data['system_metrics'].append(metrics)\n",
        "        if len(self.dashboard_data['system_metrics']) > 100:  # Keep last 100 metrics\n",
        "            self.dashboard_data['system_metrics'] = self.dashboard_data['system_metrics'][-100:]\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _show_system_status(self, metrics: Dict):\n",
        "        \"\"\"Display current system status\"\"\"\n",
        "        print(f\"\\nüñ•Ô∏è SYSTEM STATUS\")\n",
        "        print(\"-\" * 25)\n",
        "\n",
        "        sys_info = metrics['system_info']\n",
        "        print(f\"‚îú‚îÄ CPU Usage: {sys_info.get('cpu_usage', 0):.1f}%\")\n",
        "        print(f\"‚îú‚îÄ Memory Usage: {sys_info.get('memory_usage', 0):.1f}%\")\n",
        "        print(f\"‚îú‚îÄ Memory Available: {sys_info.get('memory_available_gb', 0):.2f} GB\")\n",
        "        print(f\"‚îú‚îÄ GPU Available: {sys_info.get('gpu_available', False)}\")\n",
        "\n",
        "        if sys_info.get('gpu_available'):\n",
        "            print(f\"‚îú‚îÄ GPU: {sys_info.get('gpu_name', 'Unknown')}\")\n",
        "            print(f\"‚îú‚îÄ GPU Memory Used: {sys_info.get('gpu_memory_used_mb', 0):.1f} MB\")\n",
        "            print(f\"‚îî‚îÄ GPU Memory Total: {sys_info.get('gpu_memory_total_mb', 0):.1f} MB\")\n",
        "        else:\n",
        "            print(f\"‚îî‚îÄ GPU: Not Available\")\n",
        "\n",
        "    def _show_ai_performance(self):\n",
        "        \"\"\"Display AI model performance metrics\"\"\"\n",
        "        ai_stats = self.ai_system.get_performance_stats()\n",
        "\n",
        "        print(f\"\\nü§ñ AI MODEL PERFORMANCE\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"‚îú‚îÄ Total Inferences: {ai_stats.get('total_inferences', 0):,}\")\n",
        "        print(f\"‚îú‚îÄ CPU Detections: {ai_stats.get('cpu_detections', 0):,}\")\n",
        "        print(f\"‚îú‚îÄ GPU Recognitions: {ai_stats.get('gpu_recognitions', 0):,}\")\n",
        "        print(f\"‚îú‚îÄ Avg CPU Latency: {ai_stats.get('avg_cpu_latency_ms', 0):.1f}ms\")\n",
        "        print(f\"‚îú‚îÄ Avg GPU Latency: {ai_stats.get('avg_gpu_latency_ms', 0):.1f}ms\")\n",
        "        print(f\"‚îú‚îÄ Dual Stream: {ai_stats.get('dual_stream_enabled', False)}\")\n",
        "        print(f\"‚îî‚îÄ Model Pack: {ai_stats.get('model_pack', 'Unknown')}\")\n",
        "\n",
        "        # Performance rating\n",
        "        total_latency = ai_stats.get('avg_cpu_latency_ms', 0) + ai_stats.get('avg_gpu_latency_ms', 0)\n",
        "        if total_latency < 100:\n",
        "            rating = \"üåü EXCELLENT\"\n",
        "        elif total_latency < 200:\n",
        "            rating = \"üëç GOOD\"\n",
        "        elif total_latency < 400:\n",
        "            rating = \"‚ö†Ô∏è MODERATE\"\n",
        "        else:\n",
        "            rating = \"üêå NEEDS OPTIMIZATION\"\n",
        "\n",
        "        print(f\"   ‚îî‚îÄ Performance Rating: {rating}\")\n",
        "\n",
        "    def _show_database_analytics(self):\n",
        "        \"\"\"Display database analytics and statistics\"\"\"\n",
        "        db_stats = self.db.get_statistics()\n",
        "\n",
        "        print(f\"\\nüóÑÔ∏è DATABASE ANALYTICS\")\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"‚îú‚îÄ Active Employees: {db_stats.get('total_active_employees', 0)}\")\n",
        "        print(f\"‚îú‚îÄ Employees with Faces: {db_stats.get('employees_with_faces', 0)}\")\n",
        "        print(f\"‚îú‚îÄ Total Attendance Logs: {db_stats.get('total_attendance_logs', 0)}\")\n",
        "        print(f\"‚îú‚îÄ Face Registrations: {db_stats.get('total_face_registrations', 0)}\")\n",
        "        print(f\"‚îú‚îÄ Today's Attendance: {db_stats.get('todays_attendance_count', 0)}\")\n",
        "        print(f\"‚îú‚îÄ Avg Registration Quality: {db_stats.get('avg_registration_quality', 0):.3f}\")\n",
        "        print(f\"‚îú‚îÄ CPU Detections: {db_stats.get('cpu_detections', 0)}\")\n",
        "        print(f\"‚îú‚îÄ GPU Recognitions: {db_stats.get('gpu_recognitions', 0)}\")\n",
        "        print(f\"‚îî‚îÄ Last 24h Activity: {db_stats.get('activity_last_24h', 0)}\")\n",
        "\n",
        "    def _create_performance_dashboard(self):\n",
        "        \"\"\"Create interactive performance dashboard with Plotly\"\"\"\n",
        "        if not COLAB_ENV:\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüìà INTERACTIVE PERFORMANCE DASHBOARD\")\n",
        "\n",
        "        # Create comprehensive dashboard\n",
        "        fig = make_subplots(\n",
        "            rows=3, cols=2,\n",
        "            subplot_titles=(\n",
        "                'System Resource Usage', 'AI Processing Performance',\n",
        "                'Database Statistics', 'Processing Timeline',\n",
        "                'Recognition Accuracy Distribution', 'System Health Score'\n",
        "            ),\n",
        "            specs=[[{\"secondary_y\": True}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"type\": \"indicator\"}]],\n",
        "            vertical_spacing=0.08\n",
        "        )\n",
        "\n",
        "        # 1. System Resource Usage\n",
        "        if len(self.dashboard_data['system_metrics']) > 1:\n",
        "            timestamps = [m['timestamp'] for m in self.dashboard_data['system_metrics'][-20:]]\n",
        "            cpu_usage = [m['system_info'].get('cpu_usage', 0) for m in self.dashboard_data['system_metrics'][-20:]]\n",
        "            memory_usage = [m['system_info'].get('memory_usage', 0) for m in self.dashboard_data['system_metrics'][-20:]]\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=timestamps, y=cpu_usage, name='CPU %', line=dict(color='blue')),\n",
        "                row=1, col=1\n",
        "            )\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=timestamps, y=memory_usage, name='Memory %', line=dict(color='red')),\n",
        "                row=1, col=1, secondary_y=True\n",
        "            )\n",
        "\n",
        "        # 2. AI Processing Performance\n",
        "        ai_stats = self.ai_system.get_performance_stats()\n",
        "        ai_metrics = ['CPU Detections', 'GPU Recognitions', 'Total Inferences']\n",
        "        ai_values = [\n",
        "            ai_stats.get('cpu_detections', 0),\n",
        "            ai_stats.get('gpu_recognitions', 0),\n",
        "            ai_stats.get('total_inferences', 0)\n",
        "        ]\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(x=ai_metrics, y=ai_values, name='AI Performance', marker_color='green'),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # 3. Database Statistics\n",
        "        db_stats = self.db.get_statistics()\n",
        "        db_labels = ['Employees', 'Attendance Logs', 'Face Registrations', 'Today\\'s Logs']\n",
        "        db_values = [\n",
        "            db_stats.get('total_active_employees', 0),\n",
        "            db_stats.get('total_attendance_logs', 0),\n",
        "            db_stats.get('total_face_registrations', 0),\n",
        "            db_stats.get('todays_attendance_count', 0)\n",
        "        ]\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(x=db_labels, y=db_values, name='Database Stats', marker_color='purple'),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # 4. Processing Timeline (if video processing occurred)\n",
        "        if hasattr(self.video_processor, 'processing_stats') and self.video_processor.processing_stats.get('processing_times'):\n",
        "            processing_times = list(self.video_processor.processing_stats['processing_times'])\n",
        "            frame_numbers = list(range(len(processing_times)))\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=frame_numbers, y=processing_times, name='Frame Processing Time',\n",
        "                          line=dict(color='orange')),\n",
        "                row=2, col=2\n",
        "            )\n",
        "\n",
        "        # 5. Recognition Accuracy Distribution\n",
        "        attendance_logs = self.db.get_attendance_logs(limit=100)\n",
        "        if attendance_logs:\n",
        "            confidences = [log['confidence'] for log in attendance_logs if log.get('confidence')]\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Histogram(x=confidences, nbinsx=15, name='Recognition Confidence',\n",
        "                           marker_color='lightblue'),\n",
        "                row=3, col=1\n",
        "            )\n",
        "\n",
        "        # 6. System Health Score (Indicator)\n",
        "        health_score = self._calculate_system_health_score()\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Indicator(\n",
        "                mode=\"gauge+number+delta\",\n",
        "                value=health_score,\n",
        "                domain={'x': [0, 1], 'y': [0, 1]},\n",
        "                title={'text': \"System Health\"},\n",
        "                delta={'reference': 80},\n",
        "                gauge={\n",
        "                    'axis': {'range': [None, 100]},\n",
        "                    'bar': {'color': \"darkgreen\"},\n",
        "                    'steps': [\n",
        "                        {'range': [0, 50], 'color': \"lightgray\"},\n",
        "                        {'range': [50, 80], 'color': \"yellow\"},\n",
        "                        {'range': [80, 100], 'color': \"green\"}\n",
        "                    ],\n",
        "                    'threshold': {\n",
        "                        'line': {'color': \"red\", 'width': 4},\n",
        "                        'thickness': 0.75,\n",
        "                        'value': 90\n",
        "                    }\n",
        "                }\n",
        "            ),\n",
        "            row=3, col=2\n",
        "        )\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            height=900,\n",
        "            title_text=\"üöÄ AI Attendance System - Real-time Dashboard\",\n",
        "            showlegend=True\n",
        "        )\n",
        "\n",
        "        # Update axes labels\n",
        "        fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"CPU Usage %\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Memory Usage %\", row=1, col=1, secondary_y=True)\n",
        "\n",
        "        fig.show()\n",
        "\n",
        "    def show_attendance_report(self, days_back: int = 7):\n",
        "        \"\"\"Display comprehensive attendance analytics\"\"\"\n",
        "        print(\"üìã ATTENDANCE ANALYTICS REPORT\")\n",
        "        print(\"=\" * 35)\n",
        "\n",
        "        # Get attendance data\n",
        "        cursor = self.db.conn.cursor()\n",
        "\n",
        "        # Date range for analysis\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=days_back)\n",
        "\n",
        "        cursor.execute(\"\"\"\n",
        "        SELECT al.*, e.name, e.employee_code, e.department\n",
        "        FROM attendance_logs al\n",
        "        JOIN employees e ON al.employee_id = e.id\n",
        "        WHERE al.timestamp >= ?\n",
        "        ORDER BY al.timestamp DESC\n",
        "        \"\"\", (start_date.strftime('%Y-%m-%d %H:%M:%S'),))\n",
        "\n",
        "        attendance_records = [dict(row) for row in cursor.fetchall()]\n",
        "\n",
        "        if not attendance_records:\n",
        "            print(\"üìù No attendance records found for the specified period\")\n",
        "            return\n",
        "\n",
        "        # Basic statistics\n",
        "        print(f\"üìä SUMMARY ({days_back} days):\")\n",
        "        print(f\"‚îú‚îÄ Total Records: {len(attendance_records)}\")\n",
        "        print(f\"‚îú‚îÄ Unique Employees: {len(set(r['employee_id'] for r in attendance_records))}\")\n",
        "        print(f\"‚îú‚îÄ Check-ins: {len([r for r in attendance_records if r['event_type'] == 'check_in'])}\")\n",
        "        print(f\"‚îú‚îÄ Check-outs: {len([r for r in attendance_records if r['event_type'] == 'check_out'])}\")\n",
        "        print(f\"‚îî‚îÄ Avg Confidence: {np.mean([r['confidence'] for r in attendance_records]):.3f}\")\n",
        "\n",
        "        # Department breakdown\n",
        "        dept_breakdown = {}\n",
        "        for record in attendance_records:\n",
        "            dept = record.get('department', 'Unknown')\n",
        "            dept_breakdown[dept] = dept_breakdown.get(dept, 0) + 1\n",
        "\n",
        "        print(f\"\\nüè¢ DEPARTMENT BREAKDOWN:\")\n",
        "        for dept, count in sorted(dept_breakdown.items()):\n",
        "            print(f\"‚îú‚îÄ {dept}: {count} records\")\n",
        "\n",
        "        # Daily attendance trends\n",
        "        daily_counts = {}\n",
        "        for record in attendance_records:\n",
        "            date = record['timestamp'][:10]  # YYYY-MM-DD\n",
        "            daily_counts[date] = daily_counts.get(date, 0) + 1\n",
        "\n",
        "        print(f\"\\nüìÖ DAILY ATTENDANCE TRENDS:\")\n",
        "        for date in sorted(daily_counts.keys()):\n",
        "            print(f\"‚îú‚îÄ {date}: {daily_counts[date]} records\")\n",
        "\n",
        "        # Create detailed DataFrame\n",
        "        df = pd.DataFrame(attendance_records)\n",
        "\n",
        "        if COLAB_ENV:\n",
        "            # Display interactive table\n",
        "            display(HTML(f\"<h3>üìã Detailed Attendance Records</h3>\"))\n",
        "            display_df = df[['name', 'employee_code', 'department', 'event_type',\n",
        "                           'timestamp', 'confidence']].head(20)\n",
        "            display(HTML(display_df.to_html(index=False)))\n",
        "\n",
        "            # Create attendance analytics charts\n",
        "            self._create_attendance_charts(df, days_back)\n",
        "\n",
        "    def _create_attendance_charts(self, df: pd.DataFrame, days_back: int):\n",
        "        \"\"\"Create interactive attendance analytics charts\"\"\"\n",
        "        if not COLAB_ENV or df.empty:\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüìä INTERACTIVE ATTENDANCE ANALYTICS\")\n",
        "\n",
        "        # Create comprehensive attendance dashboard\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                'Daily Attendance Trends',\n",
        "                'Department Distribution',\n",
        "                'Hourly Patterns',\n",
        "                'Event Type Distribution'\n",
        "            ),\n",
        "            specs=[[{\"secondary_y\": False}, {\"type\": \"pie\"}],\n",
        "                   [{\"secondary_y\": False}, {\"type\": \"pie\"}]]\n",
        "        )\n",
        "\n",
        "        # 1. Daily Attendance Trends\n",
        "        df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
        "        daily_counts = df.groupby('date').size().reset_index(name='count')\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=daily_counts['date'],\n",
        "                y=daily_counts['count'],\n",
        "                mode='lines+markers',\n",
        "                name='Daily Attendance',\n",
        "                line=dict(color='blue', width=3),\n",
        "                marker=dict(size=8)\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # 2. Department Distribution\n",
        "        dept_counts = df['department'].value_counts()\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Pie(\n",
        "                labels=dept_counts.index,\n",
        "                values=dept_counts.values,\n",
        "                name=\"Department Distribution\"\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # 3. Hourly Patterns\n",
        "        df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
        "        hourly_counts = df.groupby('hour').size().reset_index(name='count')\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=hourly_counts['hour'],\n",
        "                y=hourly_counts['count'],\n",
        "                name='Hourly Distribution',\n",
        "                marker_color='green'\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # 4. Event Type Distribution\n",
        "        event_counts = df['event_type'].value_counts()\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Pie(\n",
        "                labels=event_counts.index,\n",
        "                values=event_counts.values,\n",
        "                name=\"Event Type Distribution\"\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            height=800,\n",
        "            title_text=f\"üìä Attendance Analytics Dashboard ({days_back} days)\",\n",
        "            showlegend=True\n",
        "        )\n",
        "\n",
        "        # Update axes\n",
        "        fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Number of Records\", row=1, col=1)\n",
        "        fig.update_xaxes(title_text=\"Hour of Day\", row=2, col=1)\n",
        "        fig.update_yaxes(title_text=\"Number of Records\", row=2, col=1)\n",
        "\n",
        "        fig.show()\n",
        "\n",
        "    def export_comprehensive_reports(self):\n",
        "        \"\"\"Export comprehensive reports in multiple formats\"\"\"\n",
        "        print(\"üì§ COMPREHENSIVE REPORT EXPORT\")\n",
        "        print(\"=\" * 35)\n",
        "\n",
        "        try:\n",
        "            # Collect all data for export\n",
        "            export_data = self._prepare_export_data()\n",
        "\n",
        "            # Export to different formats\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "            # 1. JSON Export (Complete system state)\n",
        "            json_path = self.config['exports_dir'] / f'system_report_{timestamp}.json'\n",
        "            with open(json_path, 'w') as f:\n",
        "                json.dump(export_data, f, indent=2, default=str)\n",
        "            print(f\"‚úÖ JSON Report: {json_path}\")\n",
        "\n",
        "            # 2. CSV Export (Attendance logs)\n",
        "            if export_data['attendance_logs']:\n",
        "                csv_path = self.config['exports_dir'] / f'attendance_logs_{timestamp}.csv'\n",
        "                df_attendance = pd.DataFrame(export_data['attendance_logs'])\n",
        "                df_attendance.to_csv(csv_path, index=False)\n",
        "                print(f\"‚úÖ CSV Attendance: {csv_path}\")\n",
        "\n",
        "            # 3. Excel Export (Multiple sheets)\n",
        "            excel_path = self.config['exports_dir'] / f'attendance_report_{timestamp}.xlsx'\n",
        "            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
        "                # Attendance logs sheet\n",
        "                if export_data['attendance_logs']:\n",
        "                    df_attendance = pd.DataFrame(export_data['attendance_logs'])\n",
        "                    df_attendance.to_excel(writer, sheet_name='Attendance_Logs', index=False)\n",
        "\n",
        "                # Employee list sheet\n",
        "                if export_data['employees']:\n",
        "                    df_employees = pd.DataFrame(export_data['employees'])\n",
        "                    df_employees.to_excel(writer, sheet_name='Employees', index=False)\n",
        "\n",
        "                # System metrics sheet\n",
        "                if export_data['system_metrics']:\n",
        "                    df_metrics = pd.DataFrame([export_data['system_metrics']])\n",
        "                    df_metrics.to_excel(writer, sheet_name='System_Metrics', index=False)\n",
        "\n",
        "            print(f\"‚úÖ Excel Report: {excel_path}\")\n",
        "\n",
        "            # 4. Summary Report (TXT)\n",
        "            summary_path = self.config['exports_dir'] / f'summary_report_{timestamp}.txt'\n",
        "            self._create_summary_report(export_data, summary_path)\n",
        "            print(f\"‚úÖ Summary Report: {summary_path}\")\n",
        "\n",
        "            # Download files in Colab\n",
        "            if COLAB_ENV:\n",
        "                try:\n",
        "                    files.download(str(json_path))\n",
        "                    if export_data['attendance_logs']:\n",
        "                        files.download(str(csv_path))\n",
        "                    files.download(str(excel_path))\n",
        "                    files.download(str(summary_path))\n",
        "                    print(\"üì• Files downloaded to your computer\")\n",
        "                except Exception as e:\n",
        "                    print(f\"üí° Files saved to exports directory: {e}\")\n",
        "\n",
        "            print(f\"\\nüìä Export Summary:\")\n",
        "            print(f\"‚îú‚îÄ Attendance Records: {len(export_data['attendance_logs'])}\")\n",
        "            print(f\"‚îú‚îÄ Employees: {len(export_data['employees'])}\")\n",
        "            print(f\"‚îú‚îÄ Export Formats: JSON, CSV, Excel, Summary\")\n",
        "            print(f\"‚îî‚îÄ Export Location: {self.config['exports_dir']}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Export error: {e}\")\n",
        "\n",
        "    def _prepare_export_data(self) -> Dict:\n",
        "        \"\"\"Prepare comprehensive data for export\"\"\"\n",
        "        export_data = {\n",
        "            'export_metadata': {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'system_version': 'Pipeline V1',\n",
        "                'export_type': 'comprehensive_report'\n",
        "            },\n",
        "            'system_metrics': {},\n",
        "            'employees': [],\n",
        "            'attendance_logs': [],\n",
        "            'performance_data': {}\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # System metrics\n",
        "            current_metrics = self._collect_system_metrics()\n",
        "            export_data['system_metrics'] = current_metrics\n",
        "\n",
        "            # Employee data\n",
        "            cursor = self.db.conn.cursor()\n",
        "            cursor.execute(\"\"\"\n",
        "            SELECT id, employee_code, name, email, department, position,\n",
        "                   embedding_count, registration_quality, created_at\n",
        "            FROM employees WHERE is_active = 1\n",
        "            \"\"\")\n",
        "            export_data['employees'] = [dict(row) for row in cursor.fetchall()]\n",
        "\n",
        "            # Attendance logs (last 30 days)\n",
        "            thirty_days_ago = datetime.now() - timedelta(days=30)\n",
        "            cursor.execute(\"\"\"\n",
        "            SELECT al.*, e.name, e.employee_code, e.department\n",
        "            FROM attendance_logs al\n",
        "            JOIN employees e ON al.employee_id = e.id\n",
        "            WHERE al.timestamp >= ?\n",
        "            ORDER BY al.timestamp DESC\n",
        "            \"\"\", (thirty_days_ago.strftime('%Y-%m-%d %H:%M:%S'),))\n",
        "            export_data['attendance_logs'] = [dict(row) for row in cursor.fetchall()]\n",
        "\n",
        "            # Performance data\n",
        "            export_data['performance_data'] = {\n",
        "                'ai_performance': self.ai_system.get_performance_stats(),\n",
        "                'database_stats': self.db.get_statistics()\n",
        "            }\n",
        "\n",
        "            if hasattr(self.video_processor, 'get_processing_summary'):\n",
        "                export_data['performance_data']['video_processing'] = self.video_processor.get_processing_summary()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error preparing export data: {e}\")\n",
        "\n",
        "        return export_data\n",
        "\n",
        "    def _create_summary_report(self, export_data: Dict, file_path: Path):\n",
        "        \"\"\"Create human-readable summary report\"\"\"\n",
        "        with open(file_path, 'w') as f:\n",
        "            f.write(\"AI ATTENDANCE SYSTEM - COMPREHENSIVE REPORT\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "            # Report metadata\n",
        "            f.write(f\"Report Generated: {export_data['export_metadata']['timestamp']}\\n\")\n",
        "            f.write(f\"System Version: {export_data['export_metadata']['system_version']}\\n\\n\")\n",
        "\n",
        "            # System overview\n",
        "            f.write(\"SYSTEM OVERVIEW\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "\n",
        "            metrics = export_data['system_metrics']\n",
        "            if metrics:\n",
        "                sys_info = metrics.get('system_info', {})\n",
        "                f.write(f\"CPU Usage: {sys_info.get('cpu_usage', 0):.1f}%\\n\")\n",
        "                f.write(f\"Memory Usage: {sys_info.get('memory_usage', 0):.1f}%\\n\")\n",
        "                f.write(f\"GPU Available: {sys_info.get('gpu_available', False)}\\n\")\n",
        "\n",
        "                if sys_info.get('gpu_available'):\n",
        "                    f.write(f\"GPU: {sys_info.get('gpu_name', 'Unknown')}\\n\")\n",
        "\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Employee statistics\n",
        "            f.write(\"EMPLOYEE STATISTICS\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "            f.write(f\"Total Employees: {len(export_data['employees'])}\\n\")\n",
        "\n",
        "            if export_data['employees']:\n",
        "                dept_counts = {}\n",
        "                for emp in export_data['employees']:\n",
        "                    dept = emp.get('department', 'Unknown')\n",
        "                    dept_counts[dept] = dept_counts.get(dept, 0) + 1\n",
        "\n",
        "                f.write(\"Department Breakdown:\\n\")\n",
        "                for dept, count in dept_counts.items():\n",
        "                    f.write(f\"  - {dept}: {count}\\n\")\n",
        "\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Attendance statistics\n",
        "            f.write(\"ATTENDANCE STATISTICS\\n\")\n",
        "            f.write(\"-\" * 25 + \"\\n\")\n",
        "            f.write(f\"Total Records: {len(export_data['attendance_logs'])}\\n\")\n",
        "\n",
        "            if export_data['attendance_logs']:\n",
        "                check_ins = len([r for r in export_data['attendance_logs'] if r['event_type'] == 'check_in'])\n",
        "                check_outs = len([r for r in export_data['attendance_logs'] if r['event_type'] == 'check_out'])\n",
        "                avg_confidence = np.mean([r['confidence'] for r in export_data['attendance_logs']])\n",
        "\n",
        "                f.write(f\"Check-ins: {check_ins}\\n\")\n",
        "                f.write(f\"Check-outs: {check_outs}\\n\")\n",
        "                f.write(f\"Average Confidence: {avg_confidence:.3f}\\n\")\n",
        "\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Performance summary\n",
        "            f.write(\"PERFORMANCE SUMMARY\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "\n",
        "            perf_data = export_data['performance_data']\n",
        "            if 'ai_performance' in perf_data:\n",
        "                ai_perf = perf_data['ai_performance']\n",
        "                f.write(f\"Total AI Inferences: {ai_perf.get('total_inferences', 0)}\\n\")\n",
        "                f.write(f\"CPU Detections: {ai_perf.get('cpu_detections', 0)}\\n\")\n",
        "                f.write(f\"GPU Recognitions: {ai_perf.get('gpu_recognitions', 0)}\\n\")\n",
        "                f.write(f\"Average CPU Latency: {ai_perf.get('avg_cpu_latency_ms', 0):.1f}ms\\n\")\n",
        "                f.write(f\"Average GPU Latency: {ai_perf.get('avg_gpu_latency_ms', 0):.1f}ms\\n\")\n",
        "\n",
        "    def _calculate_system_health_score(self) -> float:\n",
        "        \"\"\"Calculate overall system health score (0-100)\"\"\"\n",
        "        try:\n",
        "            score = 100.0\n",
        "\n",
        "            # Get current metrics\n",
        "            current_metrics = self._collect_system_metrics()\n",
        "            sys_info = current_metrics['system_info']\n",
        "\n",
        "            # CPU usage penalty\n",
        "            cpu_usage = sys_info.get('cpu_usage', 0)\n",
        "            if cpu_usage > 80:\n",
        "                score -= (cpu_usage - 80) * 2\n",
        "\n",
        "            # Memory usage penalty\n",
        "            memory_usage = sys_info.get('memory_usage', 0)\n",
        "            if memory_usage > 85:\n",
        "                score -= (memory_usage - 85) * 3\n",
        "\n",
        "            # AI performance bonus\n",
        "            ai_stats = self.ai_system.get_performance_stats()\n",
        "            total_latency = ai_stats.get('avg_cpu_latency_ms', 0) + ai_stats.get('avg_gpu_latency_ms', 0)\n",
        "            if total_latency < 100:\n",
        "                score += 10\n",
        "            elif total_latency > 400:\n",
        "                score -= 20\n",
        "\n",
        "            # Database activity bonus\n",
        "            db_stats = self.db.get_statistics()\n",
        "            if db_stats.get('total_active_employees', 0) > 0:\n",
        "                score += 5\n",
        "\n",
        "            # GPU availability bonus\n",
        "            if sys_info.get('gpu_available', False):\n",
        "                score += 10\n",
        "\n",
        "            return max(0.0, min(100.0, score))\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating health score: {e}\")\n",
        "            return 50.0  # Default moderate health score\n",
        "\n",
        "    def create_interactive_widgets(self):\n",
        "        \"\"\"Create interactive widgets for dashboard control\"\"\"\n",
        "        if not COLAB_ENV:\n",
        "            print(\"‚ùå Interactive widgets require Google Colab environment\")\n",
        "            return\n",
        "\n",
        "        print(\"üéõÔ∏è INTERACTIVE DASHBOARD CONTROLS\")\n",
        "        print(\"=\" * 35)\n",
        "\n",
        "        # Create control widgets\n",
        "        refresh_button = widgets.Button(\n",
        "            description='üîÑ Refresh Dashboard',\n",
        "            button_style='info',\n",
        "            layout=widgets.Layout(width='200px', height='40px')\n",
        "        )\n",
        "\n",
        "        export_button = widgets.Button(\n",
        "            description='üì§ Export Reports',\n",
        "            button_style='success',\n",
        "            layout=widgets.Layout(width='200px', height='40px')\n",
        "        )\n",
        "\n",
        "        days_slider = widgets.IntSlider(\n",
        "            value=7,\n",
        "            min=1,\n",
        "            max=30,\n",
        "            step=1,\n",
        "            description='Report Days:',\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        output = widgets.Output()\n",
        "\n",
        "        # Button event handlers\n",
        "        def on_refresh_click(b):\n",
        "            with output:\n",
        "                clear_output()\n",
        "                print(\"üîÑ Refreshing dashboard...\")\n",
        "                self.show_system_dashboard()\n",
        "\n",
        "        def on_export_click(b):\n",
        "            with output:\n",
        "                clear_output()\n",
        "                print(\"üì§ Exporting reports...\")\n",
        "                self.export_comprehensive_reports()\n",
        "\n",
        "        def on_days_change(change):\n",
        "            with output:\n",
        "                clear_output()\n",
        "                print(f\"üìä Updating report for {change['new']} days...\")\n",
        "                self.show_attendance_report(change['new'])\n",
        "\n",
        "        refresh_button.on_click(on_refresh_click)\n",
        "        export_button.on_click(on_export_click)\n",
        "        days_slider.observe(on_days_change, names='value')\n",
        "\n",
        "        # Display widgets\n",
        "        controls = widgets.HBox([refresh_button, export_button])\n",
        "        display(widgets.VBox([controls, days_slider, output]))\n",
        "\n",
        "# Initialize when imported\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"‚úÖ Cell 6 completed. Analytics Dashboard & Reporting ready.\")\n",
        "    print(\"üí° Use with ai_system, database, video_processor, and config from previous cells.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qrQU5RLQiOK",
        "outputId": "0cd71dfb-19f2-4ccb-f7e3-c14fc21ef54f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéØ AI ATTENDANCE SYSTEM PIPELINE V1 - READY!\n",
            "==================================================\n",
            "üöÄ Quick Start Guide:\n",
            "\n",
            "1Ô∏è‚É£ Initialize System:\n",
            "   system = init()  # or init('buffalo_s') for faster model\n",
            "\n",
            "2Ô∏è‚É£ Run Interactive Demo:\n",
            "   demo()  # Interactive interface with all features\n",
            "\n",
            "3Ô∏è‚É£ Or Use Individual Functions:\n",
            "   upload_employees()  # Register employee photos\n",
            "   process_video()     # Process attendance videos\n",
            "   dashboard()         # View analytics\n",
            "   export()            # Export reports\n",
            "   status()            # System status\n",
            "   test()              # Test integration\n",
            "   cleanup()           # Clean resources\n",
            "\n",
            "üéÆ For full demo experience: init() ‚Üí demo()\n",
            "==================================================\n",
            "‚úÖ Cell 7 completed. Main execution system ready.\n",
            "üí° Run init() to start the AI Attendance System!\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CELL 7: MAIN EXECUTION & DEMO SYSTEM\n",
        "- Complete system initialization and integration\n",
        "- Sample data creation and demo scenarios\n",
        "- Convenience wrapper functions for easy usage\n",
        "- Interactive demo workflow\n",
        "- System testing and validation\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import Dict, Optional, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    import ipywidgets as widgets\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PipelineV1System:\n",
        "    \"\"\"\n",
        "    Complete AI Attendance System Pipeline V1\n",
        "    Integrates all components with convenience functions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # System components (will be initialized)\n",
        "        self.config = None\n",
        "        self.ai_system = None\n",
        "        self.database = None\n",
        "        self.employee_manager = None\n",
        "        self.video_processor = None\n",
        "        self.analytics_dashboard = None\n",
        "\n",
        "        # System state\n",
        "        self.is_initialized = False\n",
        "        self.demo_mode = False\n",
        "\n",
        "        print(\"\\nüöÄ CELL 7: AI ATTENDANCE SYSTEM PIPELINE V1\")\n",
        "        print(\"=\" * 50)\n",
        "        print(\"üéØ Complete system integration and demo capabilities\")\n",
        "        print(\"‚îú‚îÄ Dual-Stream AI Processing\")\n",
        "        print(\"‚îú‚îÄ SQLite Database with Vector Search\")\n",
        "        print(\"‚îú‚îÄ Employee Management & Video Processing\")\n",
        "        print(\"‚îú‚îÄ Analytics Dashboard & Reporting\")\n",
        "        print(\"‚îî‚îÄ Interactive Demo & Testing\")\n",
        "\n",
        "    def initialize_system(self, model_pack='buffalo_l', demo_mode=True) -> bool:\n",
        "        \"\"\"Initialize complete Pipeline V1 system\"\"\"\n",
        "        print(f\"\\nüîÑ INITIALIZING PIPELINE V1 SYSTEM\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"‚îú‚îÄ Model Pack: {model_pack}\")\n",
        "        print(f\"‚îú‚îÄ Demo Mode: {demo_mode}\")\n",
        "        print(f\"‚îî‚îÄ Environment: {'Google Colab' if COLAB_ENV else 'Local'}\")\n",
        "\n",
        "        try:\n",
        "            # Import all required modules (assuming previous cells are loaded)\n",
        "            global setup_system, DualStreamAISystem, AttendanceDatabaseSQLite\n",
        "            global EmployeeManager, VideoProcessor, AnalyticsDashboard\n",
        "\n",
        "            # Step 1: System setup\n",
        "            print(f\"\\nüì¶ Step 1: System Setup...\")\n",
        "            self.config = setup_system()\n",
        "\n",
        "            # Step 2: Initialize AI models\n",
        "            print(f\"\\nü§ñ Step 2: AI Models Initialization...\")\n",
        "            self.ai_system = DualStreamAISystem(model_pack=model_pack)\n",
        "\n",
        "            # Step 3: Initialize database\n",
        "            print(f\"\\nüóÑÔ∏è Step 3: Database Initialization...\")\n",
        "            db_path = self.config['db_dir'] / 'attendance_pipeline_v1.db'\n",
        "            self.database = AttendanceDatabaseSQLite(db_path=str(db_path))\n",
        "\n",
        "            # Step 4: Initialize employee manager\n",
        "            print(f\"\\nüë• Step 4: Employee Manager...\")\n",
        "            self.employee_manager = EmployeeManager(\n",
        "                self.ai_system, self.database, self.config\n",
        "            )\n",
        "\n",
        "            # Step 5: Initialize video processor\n",
        "            print(f\"\\nüé• Step 5: Video Processor...\")\n",
        "            self.video_processor = VideoProcessor(\n",
        "                self.ai_system, self.database, self.config\n",
        "            )\n",
        "\n",
        "            # Step 6: Initialize analytics dashboard\n",
        "            print(f\"\\nüìä Step 6: Analytics Dashboard...\")\n",
        "            self.analytics_dashboard = AnalyticsDashboard(\n",
        "                self.ai_system, self.database, self.video_processor, self.config\n",
        "            )\n",
        "\n",
        "            self.is_initialized = True\n",
        "            self.demo_mode = demo_mode\n",
        "\n",
        "            print(f\"\\nüéâ SYSTEM INITIALIZATION COMPLETED!\")\n",
        "            print(f\"‚úÖ All components ready for Pipeline V1 operations\")\n",
        "\n",
        "            # Create sample data in demo mode\n",
        "            if demo_mode:\n",
        "                print(f\"\\nüß™ Creating sample data for demo...\")\n",
        "                self.create_sample_data()\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå System initialization failed: {e}\")\n",
        "            print(f\"üí° Make sure all previous cells (1-6) are executed\")\n",
        "            return False\n",
        "\n",
        "    def create_sample_data(self):\n",
        "        \"\"\"Create comprehensive sample data for demo\"\"\"\n",
        "        print(f\"\\nüß™ CREATING SAMPLE DATA FOR DEMO\")\n",
        "        print(\"-\" * 35)\n",
        "\n",
        "        try:\n",
        "            # Create sample employees\n",
        "            self.employee_manager.create_sample_employee_data()\n",
        "\n",
        "            # Add some sample attendance records\n",
        "            self._create_sample_attendance_records()\n",
        "\n",
        "            print(f\"‚úÖ Sample data creation completed!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Sample data creation error: {e}\")\n",
        "\n",
        "    def _create_sample_attendance_records(self):\n",
        "        \"\"\"Create sample attendance records for demo\"\"\"\n",
        "        import random\n",
        "        from datetime import timedelta\n",
        "\n",
        "        # Get all employees\n",
        "        cursor = self.database.conn.cursor()\n",
        "        cursor.execute(\"SELECT id, name FROM employees WHERE is_active = 1\")\n",
        "        employees = cursor.fetchall()\n",
        "\n",
        "        if not employees:\n",
        "            return\n",
        "\n",
        "        # Create sample attendance for last few days\n",
        "        base_time = datetime.now()\n",
        "\n",
        "        for days_back in range(5):  # Last 5 days\n",
        "            date = base_time - timedelta(days=days_back)\n",
        "\n",
        "            for emp in employees:\n",
        "                if random.random() > 0.3:  # 70% attendance rate\n",
        "                    # Morning check-in\n",
        "                    checkin_time = date.replace(\n",
        "                        hour=random.randint(7, 9),\n",
        "                        minute=random.randint(0, 59),\n",
        "                        second=0, microsecond=0\n",
        "                    )\n",
        "\n",
        "                    # Create sample dual-stream metadata\n",
        "                    dual_stream_metadata = {\n",
        "                        'detection_stream': 'cpu',\n",
        "                        'recognition_stream': 'gpu',\n",
        "                        'cpu_processing_time_ms': random.uniform(30, 80),\n",
        "                        'gpu_processing_time_ms': random.uniform(50, 150)\n",
        "                    }\n",
        "\n",
        "                    # Record check-in\n",
        "                    self.database.record_attendance(\n",
        "                        employee_id=emp['id'],\n",
        "                        event_type='check_in',\n",
        "                        confidence=random.uniform(0.75, 0.95),\n",
        "                        timestamp=checkin_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                        dual_stream_metadata=dual_stream_metadata,\n",
        "                        video_source='demo_data',\n",
        "                        face_quality=random.uniform(0.7, 0.9),\n",
        "                        zone_detected='main_entrance'\n",
        "                    )\n",
        "\n",
        "                    # Evening check-out (80% chance)\n",
        "                    if random.random() > 0.2:\n",
        "                        checkout_time = checkin_time.replace(\n",
        "                            hour=random.randint(17, 19),\n",
        "                            minute=random.randint(0, 59)\n",
        "                        )\n",
        "\n",
        "                        self.database.record_attendance(\n",
        "                            employee_id=emp['id'],\n",
        "                            event_type='check_out',\n",
        "                            confidence=random.uniform(0.75, 0.95),\n",
        "                            timestamp=checkout_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                            dual_stream_metadata=dual_stream_metadata,\n",
        "                            video_source='demo_data',\n",
        "                            face_quality=random.uniform(0.7, 0.9),\n",
        "                            zone_detected='main_entrance'\n",
        "                        )\n",
        "\n",
        "        print(f\"üìã Sample attendance records created for {len(employees)} employees\")\n",
        "\n",
        "    def run_interactive_demo(self):\n",
        "        \"\"\"Run interactive demo with step-by-step guidance\"\"\"\n",
        "        if not self.is_initialized:\n",
        "            print(\"‚ùå System not initialized. Run init() first.\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüéÆ INTERACTIVE DEMO MODE\")\n",
        "        print(\"=\" * 30)\n",
        "\n",
        "        if COLAB_ENV:\n",
        "            self._create_demo_interface()\n",
        "        else:\n",
        "            self._run_console_demo()\n",
        "\n",
        "    def _create_demo_interface(self):\n",
        "        \"\"\"Create interactive demo interface for Colab\"\"\"\n",
        "        print(\"üéõÔ∏è Interactive Demo Interface\")\n",
        "\n",
        "        # Demo action buttons\n",
        "        demo_buttons = {\n",
        "            'System Status': self.show_system_status,\n",
        "            'Employee Management': self.demo_employee_management,\n",
        "            'Video Processing': self.demo_video_processing,\n",
        "            'Analytics Dashboard': self.demo_analytics,\n",
        "            'Export Reports': self.demo_export\n",
        "        }\n",
        "\n",
        "        # Create buttons\n",
        "        button_widgets = []\n",
        "        for name, func in demo_buttons.items():\n",
        "            button = widgets.Button(\n",
        "                description=name,\n",
        "                button_style='info',\n",
        "                layout=widgets.Layout(width='200px', height='40px', margin='5px')\n",
        "            )\n",
        "            button.on_click(lambda b, f=func: self._execute_demo_action(f))\n",
        "            button_widgets.append(button)\n",
        "\n",
        "        # Output area\n",
        "        output = widgets.Output()\n",
        "\n",
        "        # Display interface\n",
        "        button_box = widgets.VBox(button_widgets)\n",
        "        demo_interface = widgets.HBox([button_box, output])\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            widgets.HTML(\"<h3>üéÆ AI Attendance System Demo Interface</h3>\"),\n",
        "            demo_interface\n",
        "        ]))\n",
        "\n",
        "        # Store output widget for demo actions\n",
        "        self._demo_output = output\n",
        "\n",
        "    def _execute_demo_action(self, action_func):\n",
        "        \"\"\"Execute demo action with output capture\"\"\"\n",
        "        if hasattr(self, '_demo_output'):\n",
        "            with self._demo_output:\n",
        "                clear_output()\n",
        "                action_func()\n",
        "\n",
        "    def _run_console_demo(self):\n",
        "        \"\"\"Run console-based demo for non-Colab environments\"\"\"\n",
        "        demo_options = {\n",
        "            '1': ('System Status', self.show_system_status),\n",
        "            '2': ('Employee Management', self.demo_employee_management),\n",
        "            '3': ('Video Processing', self.demo_video_processing),\n",
        "            '4': ('Analytics Dashboard', self.demo_analytics),\n",
        "            '5': ('Export Reports', self.demo_export),\n",
        "            '6': ('Exit Demo', None)\n",
        "        }\n",
        "\n",
        "        while True:\n",
        "            print(f\"\\nüéÆ DEMO OPTIONS:\")\n",
        "            for key, (name, _) in demo_options.items():\n",
        "                print(f\"{key}. {name}\")\n",
        "\n",
        "            choice = input(\"\\nSelect option: \").strip()\n",
        "\n",
        "            if choice == '6':\n",
        "                print(\"üëã Demo completed!\")\n",
        "                break\n",
        "            elif choice in demo_options:\n",
        "                _, action = demo_options[choice]\n",
        "                if action:\n",
        "                    action()\n",
        "            else:\n",
        "                print(\"‚ùå Invalid option. Try again.\")\n",
        "\n",
        "    def show_system_status(self):\n",
        "        \"\"\"Show comprehensive system status\"\"\"\n",
        "        print(f\"\\nüöÄ SYSTEM STATUS OVERVIEW\")\n",
        "        print(\"=\" * 30)\n",
        "\n",
        "        if not self.is_initialized:\n",
        "            print(\"‚ùå System not initialized\")\n",
        "            return\n",
        "\n",
        "        # Component status\n",
        "        print(f\"üìä Component Status:\")\n",
        "        print(f\"‚îú‚îÄ AI System: {'‚úÖ Active' if self.ai_system else '‚ùå Not loaded'}\")\n",
        "        print(f\"‚îú‚îÄ Database: {'‚úÖ Connected' if self.database else '‚ùå Not connected'}\")\n",
        "        print(f\"‚îú‚îÄ Employee Manager: {'‚úÖ Ready' if self.employee_manager else '‚ùå Not ready'}\")\n",
        "        print(f\"‚îú‚îÄ Video Processor: {'‚úÖ Ready' if self.video_processor else '‚ùå Not ready'}\")\n",
        "        print(f\"‚îî‚îÄ Analytics: {'‚úÖ Ready' if self.analytics_dashboard else '‚ùå Not ready'}\")\n",
        "\n",
        "        # Quick statistics\n",
        "        if self.database:\n",
        "            stats = self.database.get_statistics()\n",
        "            print(f\"\\nüìà Quick Statistics:\")\n",
        "            print(f\"‚îú‚îÄ Employees: {stats.get('total_active_employees', 0)}\")\n",
        "            print(f\"‚îú‚îÄ Attendance Records: {stats.get('total_attendance_logs', 0)}\")\n",
        "            print(f\"‚îú‚îÄ Today's Activity: {stats.get('todays_attendance_count', 0)}\")\n",
        "            print(f\"‚îî‚îÄ Face Registrations: {stats.get('total_face_registrations', 0)}\")\n",
        "\n",
        "        # Performance overview\n",
        "        if self.ai_system:\n",
        "            ai_stats = self.ai_system.get_performance_stats()\n",
        "            total_latency = ai_stats.get('avg_cpu_latency_ms', 0) + ai_stats.get('avg_gpu_latency_ms', 0)\n",
        "            print(f\"\\n‚ö° Performance Overview:\")\n",
        "            print(f\"‚îú‚îÄ Total Inferences: {ai_stats.get('total_inferences', 0)}\")\n",
        "            print(f\"‚îú‚îÄ Dual Stream: {'‚úÖ Enabled' if ai_stats.get('dual_stream_enabled') else '‚ùå Disabled'}\")\n",
        "            print(f\"‚îî‚îÄ Avg Latency: {total_latency:.1f}ms\")\n",
        "\n",
        "    def demo_employee_management(self):\n",
        "        \"\"\"Demo employee management features\"\"\"\n",
        "        print(f\"\\nüë• EMPLOYEE MANAGEMENT DEMO\")\n",
        "        print(\"=\" * 35)\n",
        "\n",
        "        if not self.employee_manager:\n",
        "            print(\"‚ùå Employee manager not initialized\")\n",
        "            return\n",
        "\n",
        "        # Show current employees\n",
        "        self.employee_manager.show_registered_employees()\n",
        "\n",
        "        if COLAB_ENV:\n",
        "            print(f\"\\nüì§ To register new employees:\")\n",
        "            print(f\"employee_manager.upload_employee_photos()\")\n",
        "        else:\n",
        "            print(f\"\\nüí° Employee management features:\")\n",
        "            print(f\"‚îú‚îÄ Upload photos: employee_manager.upload_employee_photos()\")\n",
        "            print(f\"‚îú‚îÄ Show employees: employee_manager.show_registered_employees()\")\n",
        "            print(f\"‚îî‚îÄ Get statistics: employee_manager.get_employee_statistics()\")\n",
        "\n",
        "    def demo_video_processing(self):\n",
        "        \"\"\"Demo video processing capabilities\"\"\"\n",
        "        print(f\"\\nüé• VIDEO PROCESSING DEMO\")\n",
        "        print(\"=\" * 30)\n",
        "\n",
        "        if not self.video_processor:\n",
        "            print(\"‚ùå Video processor not initialized\")\n",
        "            return\n",
        "\n",
        "        print(f\"üé¨ Video Processing Features:\")\n",
        "        print(f\"‚îú‚îÄ Dual-Stream Processing: CPU detection + GPU recognition\")\n",
        "        print(f\"‚îú‚îÄ Business Logic: 30min cooldown, work hours validation\")\n",
        "        print(f\"‚îú‚îÄ Real-time Analytics: Frame-by-frame tracking\")\n",
        "        print(f\"‚îî‚îÄ Attendance Recording: Automatic event logging\")\n",
        "\n",
        "        # Show processing stats if available\n",
        "        if hasattr(self.video_processor, 'processing_stats'):\n",
        "            stats = self.video_processor.processing_stats\n",
        "            print(f\"\\nüìä Current Processing Stats:\")\n",
        "            print(f\"‚îú‚îÄ Frames Processed: {stats.get('frames_processed', 0)}\")\n",
        "            print(f\"‚îú‚îÄ Persons Detected: {stats.get('persons_detected', 0)}\")\n",
        "            print(f\"‚îú‚îÄ Faces Recognized: {stats.get('faces_recognized', 0)}\")\n",
        "            print(f\"‚îî‚îÄ Attendance Recorded: {stats.get('attendance_recorded', 0)}\")\n",
        "\n",
        "        if COLAB_ENV:\n",
        "            print(f\"\\nüì§ To process videos:\")\n",
        "            print(f\"video_processor.upload_and_process_video()\")\n",
        "\n",
        "    def demo_analytics(self):\n",
        "        \"\"\"Demo analytics dashboard features\"\"\"\n",
        "        print(f\"\\nüìä ANALYTICS DASHBOARD DEMO\")\n",
        "        print(\"=\" * 35)\n",
        "\n",
        "        if not self.analytics_dashboard:\n",
        "            print(\"‚ùå Analytics dashboard not initialized\")\n",
        "            return\n",
        "\n",
        "        # Show system dashboard\n",
        "        self.analytics_dashboard.show_system_dashboard()\n",
        "\n",
        "        # Show attendance report\n",
        "        print(f\"\\nüìã Attendance Report (last 7 days):\")\n",
        "        self.analytics_dashboard.show_attendance_report(days_back=7)\n",
        "\n",
        "        if COLAB_ENV:\n",
        "            print(f\"\\nüéõÔ∏è Interactive controls available:\")\n",
        "            print(f\"analytics_dashboard.create_interactive_widgets()\")\n",
        "\n",
        "    def demo_export(self):\n",
        "        \"\"\"Demo export functionality\"\"\"\n",
        "        print(f\"\\nüì§ EXPORT FUNCTIONALITY DEMO\")\n",
        "        print(\"=\" * 35)\n",
        "\n",
        "        if not self.analytics_dashboard:\n",
        "            print(\"‚ùå Analytics dashboard not initialized\")\n",
        "            return\n",
        "\n",
        "        # Export comprehensive reports\n",
        "        self.analytics_dashboard.export_comprehensive_reports()\n",
        "\n",
        "    def test_system_integration(self) -> bool:\n",
        "        \"\"\"Test complete system integration\"\"\"\n",
        "        print(f\"\\nüß™ SYSTEM INTEGRATION TEST\")\n",
        "        print(\"=\" * 30)\n",
        "\n",
        "        if not self.is_initialized:\n",
        "            print(\"‚ùå System not initialized\")\n",
        "            return False\n",
        "\n",
        "        test_results = {\n",
        "            'ai_system': False,\n",
        "            'database': False,\n",
        "            'employee_manager': False,\n",
        "            'video_processor': False,\n",
        "            'analytics': False\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Test AI system\n",
        "            print(f\"ü§ñ Testing AI system...\")\n",
        "            test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
        "            ai_result = self.ai_system.process_dual_stream(test_image, test_image)\n",
        "            test_results['ai_system'] = isinstance(ai_result, dict)\n",
        "            print(f\"   {'‚úÖ PASS' if test_results['ai_system'] else '‚ùå FAIL'}\")\n",
        "\n",
        "            # Test database\n",
        "            print(f\"üóÑÔ∏è Testing database...\")\n",
        "            db_stats = self.database.get_statistics()\n",
        "            test_results['database'] = isinstance(db_stats, dict)\n",
        "            print(f\"   {'‚úÖ PASS' if test_results['database'] else '‚ùå FAIL'}\")\n",
        "\n",
        "            # Test employee manager\n",
        "            print(f\"üë• Testing employee manager...\")\n",
        "            emp_stats = self.employee_manager.get_employee_statistics()\n",
        "            test_results['employee_manager'] = isinstance(emp_stats, dict)\n",
        "            print(f\"   {'‚úÖ PASS' if test_results['employee_manager'] else '‚ùå FAIL'}\")\n",
        "\n",
        "            # Test video processor\n",
        "            print(f\"üé• Testing video processor...\")\n",
        "            proc_summary = self.video_processor.get_processing_summary()\n",
        "            test_results['video_processor'] = isinstance(proc_summary, dict)\n",
        "            print(f\"   {'‚úÖ PASS' if test_results['video_processor'] else '‚ùå FAIL'}\")\n",
        "\n",
        "            # Test analytics\n",
        "            print(f\"üìä Testing analytics...\")\n",
        "            health_score = self.analytics_dashboard._calculate_system_health_score()\n",
        "            test_results['analytics'] = isinstance(health_score, (int, float))\n",
        "            print(f\"   {'‚úÖ PASS' if test_results['analytics'] else '‚ùå FAIL'}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Integration test error: {e}\")\n",
        "\n",
        "        # Test summary\n",
        "        passed_tests = sum(test_results.values())\n",
        "        total_tests = len(test_results)\n",
        "\n",
        "        print(f\"\\nüìã TEST SUMMARY:\")\n",
        "        print(f\"‚îú‚îÄ Passed: {passed_tests}/{total_tests}\")\n",
        "        print(f\"‚îú‚îÄ Success Rate: {passed_tests/total_tests*100:.1f}%\")\n",
        "        print(f\"‚îî‚îÄ Status: {'‚úÖ ALL SYSTEMS OPERATIONAL' if passed_tests == total_tests else '‚ö†Ô∏è SOME ISSUES DETECTED'}\")\n",
        "\n",
        "        return passed_tests == total_tests\n",
        "\n",
        "    def cleanup_system(self):\n",
        "        \"\"\"Cleanup system resources\"\"\"\n",
        "        print(f\"\\nüßπ CLEANING UP SYSTEM RESOURCES\")\n",
        "        print(\"-\" * 35)\n",
        "\n",
        "        try:\n",
        "            if self.ai_system:\n",
        "                self.ai_system.cleanup()\n",
        "                print(\"‚úÖ AI system cleaned up\")\n",
        "\n",
        "            if self.database:\n",
        "                self.database.close()\n",
        "                print(\"‚úÖ Database connection closed\")\n",
        "\n",
        "            print(\"‚úÖ System cleanup completed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Cleanup error: {e}\")\n",
        "\n",
        "# Global convenience functions for easy usage\n",
        "def init(model_pack='buffalo_l', demo_mode=True) -> PipelineV1System:\n",
        "    \"\"\"Initialize complete Pipeline V1 system\"\"\"\n",
        "    system = PipelineV1System()\n",
        "    success = system.initialize_system(model_pack=model_pack, demo_mode=demo_mode)\n",
        "\n",
        "    if success:\n",
        "        print(f\"\\nüéâ SYSTEM READY!\")\n",
        "        print(f\"üí° Next steps:\")\n",
        "        print(f\"   ‚îú‚îÄ demo() - Run interactive demo\")\n",
        "        print(f\"   ‚îú‚îÄ upload_employees() - Register employees\")\n",
        "        print(f\"   ‚îú‚îÄ process_video() - Process attendance videos\")\n",
        "        print(f\"   ‚îú‚îÄ dashboard() - View analytics dashboard\")\n",
        "        print(f\"   ‚îî‚îÄ export() - Export reports\")\n",
        "\n",
        "        # Store system globally for convenience functions\n",
        "        globals()['_pipeline_system'] = system\n",
        "        return system\n",
        "    else:\n",
        "        print(f\"‚ùå System initialization failed\")\n",
        "        return None\n",
        "\n",
        "def demo():\n",
        "    \"\"\"Run interactive demo\"\"\"\n",
        "    if '_pipeline_system' in globals():\n",
        "        globals()['_pipeline_system'].run_interactive_demo()\n",
        "    else:\n",
        "        print(\"‚ùå System not initialized. Run init() first.\")\n",
        "\n",
        "def upload_employees():\n",
        "    \"\"\"Upload employee photos\"\"\"\n",
        "    if '_pipeline_system' in globals():\n",
        "        system = globals()['_pipeline_system']\n",
        "        if system.employee_manager:\n",
        "            system.employee_manager.upload_employee_photos()\n",
        "        else:\n",
        "            print(\"‚ùå Employee manager not available\")\n",
        "    else:\n",
        "        print(\"‚ùå System not initialized. Run init() first.\")\n",
        "\n",
        "def process_video():\n",
        "    \"\"\"Process attendance video\"\"\"\n",
        "    if '_pipeline_system' in globals():\n",
        "        system = globals()['_pipeline_system']\n",
        "        if system.video_processor:\n",
        "            system.video_processor.upload_and_process_video()\n",
        "        else:\n",
        "            print(\"‚ùå Video processor not available\")\n",
        "    else:\n",
        "        print(\"‚ùå System not initialized. Run init() first.\")\n",
        "\n",
        "def dashboard():\n",
        "    \"\"\"Show analytics dashboard\"\"\"\n",
        "    if '_pipeline_system' in globals():\n",
        "        system = globals()['_pipeline_system']\n",
        "        if system.analytics_dashboard:\n",
        "            system.analytics_dashboard.show_system_dashboard()\n",
        "        else:\n",
        "            print(\"‚ùå Analytics dashboard not available\")\n",
        "    else:\n",
        "        print(\"‚ùå System not initialized. Run init() first.\")\n",
        "\n",
        "def export():\n",
        "    \"\"\"Export comprehensive reports\"\"\"\n",
        "    if '_pipeline_system' in globals():\n",
        "        system = globals()['_pipeline_system']\n",
        "        if system.analytics_dashboard:\n",
        "            system.analytics_dashboard.export_comprehensive_reports()\n",
        "        else:\n",
        "            print(\"‚ùå Analytics dashboard not available\")\n",
        "    else:\n",
        "        print(\"‚ùå System not initialized. Run init() first.\")\n",
        "\n",
        "def status():\n",
        "    \"\"\"Show system status\"\"\"\n",
        "    if '_pipeline_system' in globals():\n",
        "        globals()['_pipeline_system'].show_system_status()\n",
        "    else:\n",
        "        print(\"‚ùå System not initialized. Run init() first.\")\n",
        "\n",
        "def test():\n",
        "    \"\"\"Test system integration\"\"\"\n",
        "    if '_pipeline_system' in globals():\n",
        "        return globals()['_pipeline_system'].test_system_integration()\n",
        "    else:\n",
        "        print(\"‚ùå System not initialized. Run init() first.\")\n",
        "        return False\n",
        "\n",
        "def cleanup():\n",
        "    \"\"\"Cleanup system resources\"\"\"\n",
        "    if '_pipeline_system' in globals():\n",
        "        globals()['_pipeline_system'].cleanup_system()\n",
        "        del globals()['_pipeline_system']\n",
        "    else:\n",
        "        print(\"‚ùå System not initialized.\")\n",
        "\n",
        "# Auto-run welcome message\n",
        "def show_welcome():\n",
        "    \"\"\"Show welcome message and quick start guide\"\"\"\n",
        "    print(f\"\\nüéØ AI ATTENDANCE SYSTEM PIPELINE V1 - READY!\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üöÄ Quick Start Guide:\")\n",
        "    print(f\"\")\n",
        "    print(f\"1Ô∏è‚É£ Initialize System:\")\n",
        "    print(f\"   system = init()  # or init('buffalo_s') for faster model\")\n",
        "    print(f\"\")\n",
        "    print(f\"2Ô∏è‚É£ Run Interactive Demo:\")\n",
        "    print(f\"   demo()  # Interactive interface with all features\")\n",
        "    print(f\"\")\n",
        "    print(f\"3Ô∏è‚É£ Or Use Individual Functions:\")\n",
        "    print(f\"   upload_employees()  # Register employee photos\")\n",
        "    print(f\"   process_video()     # Process attendance videos\")\n",
        "    print(f\"   dashboard()         # View analytics\")\n",
        "    print(f\"   export()            # Export reports\")\n",
        "    print(f\"   status()            # System status\")\n",
        "    print(f\"   test()              # Test integration\")\n",
        "    print(f\"   cleanup()           # Clean resources\")\n",
        "    print(f\"\")\n",
        "    print(f\"üéÆ For full demo experience: init() ‚Üí demo()\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# Initialize when imported\n",
        "if __name__ == \"__main__\":\n",
        "    show_welcome()\n",
        "    print(\"‚úÖ Cell 7 completed. Main execution system ready.\")\n",
        "    print(\"üí° Run init() to start the AI Attendance System!\")\n",
        "else:\n",
        "    show_welcome()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kLJGHhxQoWH"
      },
      "source": [
        "# **Quick start**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAKLHQB-Qmct",
        "outputId": "dadbc22e-6d9a-4bbd-a253-640ca112263c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:‚ùå System initialization failed: name 'setup_system' is not defined\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ CELL 7: AI ATTENDANCE SYSTEM PIPELINE V1\n",
            "==================================================\n",
            "üéØ Complete system integration and demo capabilities\n",
            "‚îú‚îÄ Dual-Stream AI Processing\n",
            "‚îú‚îÄ SQLite Database with Vector Search\n",
            "‚îú‚îÄ Employee Management & Video Processing\n",
            "‚îú‚îÄ Analytics Dashboard & Reporting\n",
            "‚îî‚îÄ Interactive Demo & Testing\n",
            "\n",
            "üîÑ INITIALIZING PIPELINE V1 SYSTEM\n",
            "========================================\n",
            "‚îú‚îÄ Model Pack: buffalo_l\n",
            "‚îú‚îÄ Demo Mode: True\n",
            "‚îî‚îÄ Environment: Google Colab\n",
            "\n",
            "üì¶ Step 1: System Setup...\n",
            "üí° Make sure all previous cells (1-6) are executed\n",
            "‚ùå System initialization failed\n",
            "‚ùå System not initialized. Run init() first.\n",
            "‚ùå System not initialized. Run init() first.\n",
            "‚ùå System not initialized. Run init() first.\n",
            "‚ùå System not initialized. Run init() first.\n",
            "‚ùå System not initialized. Run init() first.\n"
          ]
        }
      ],
      "source": [
        "# Copy-paste t·∫•t c·∫£ 7 cells v√†o Colab, sau ƒë√≥:\n",
        "\n",
        "# 1. Initialize complete system\n",
        "system = init()\n",
        "\n",
        "# 2. Run interactive demo\n",
        "demo()\n",
        "\n",
        "# 3. Or use individual functions\n",
        "upload_employees()    # Upload employee photos\n",
        "process_video()       # Process attendance videos\n",
        "dashboard()           # View analytics\n",
        "export()             # Export reports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBlqk0ecQrOL"
      },
      "source": [
        "# **Advanced Usage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "un6ElT9VQnjM",
        "outputId": "d7fcf2bb-fcbd-41a5-b6d8-a2a282dec1d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:‚ùå System initialization failed: name 'setup_system' is not defined\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ CELL 7: AI ATTENDANCE SYSTEM PIPELINE V1\n",
            "==================================================\n",
            "üéØ Complete system integration and demo capabilities\n",
            "‚îú‚îÄ Dual-Stream AI Processing\n",
            "‚îú‚îÄ SQLite Database with Vector Search\n",
            "‚îú‚îÄ Employee Management & Video Processing\n",
            "‚îú‚îÄ Analytics Dashboard & Reporting\n",
            "‚îî‚îÄ Interactive Demo & Testing\n",
            "\n",
            "üîÑ INITIALIZING PIPELINE V1 SYSTEM\n",
            "========================================\n",
            "‚îú‚îÄ Model Pack: buffalo_s\n",
            "‚îú‚îÄ Demo Mode: False\n",
            "‚îî‚îÄ Environment: Google Colab\n",
            "\n",
            "üì¶ Step 1: System Setup...\n",
            "üí° Make sure all previous cells (1-6) are executed\n",
            "‚ùå System initialization failed\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'employee_manager'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-298414902.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Direct component access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memployee_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_employee_photos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_and_process_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalytics_dashboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_system_dashboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'employee_manager'"
          ]
        }
      ],
      "source": [
        "# Custom initialization\n",
        "system = init(model_pack='buffalo_s', demo_mode=False)\n",
        "\n",
        "# Direct component access\n",
        "system.employee_manager.upload_employee_photos()\n",
        "system.video_processor.upload_and_process_video()\n",
        "system.analytics_dashboard.show_system_dashboard()\n",
        "\n",
        "# System testing\n",
        "test()  # Run integration tests\n",
        "status()  # Show system status\n",
        "cleanup()  # Clean resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOBNbaVPR3rr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
